{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a55b0eb-8654-4cf7-8d8c-8f70dc8e3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "#from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import adios2 as ad2\n",
    "import xgc4py\n",
    "import nanopq\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c5e4f9-01b2-4fe5-9edd-decf9166aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(frame):\n",
    "    x = np.linspace(0, 38, 39)\n",
    "    y = np.linspace(0, 38, 39)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    plt.imshow(frame, origin='lower')\n",
    "    #plt.imshow(frame, origin='upper')\n",
    "    plt.colorbar()\n",
    "    plt.contour(X, Y, frame, 5, origin='image', colors='white', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac48c3c-b4ae-4238-b19d-8791da92deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16395\n"
     ]
    }
   ],
   "source": [
    "with ad2.open('./dataset/xgc.mesh.bp', 'r') as f:\n",
    "    nnodes = int(f.read('n_n', ))\n",
    "    ncells = int(f.read('n_t', ))\n",
    "    rz = f.read('rz')\n",
    "    conn = f.read('nd_connect_list')\n",
    "    psi = f.read('psi')\n",
    "    nextnode = f.read('nextnode')\n",
    "    epsilon = f.read('epsilon')\n",
    "    node_vol = f.read('node_vol')\n",
    "    node_vol_nearest = f.read('node_vol_nearest')\n",
    "    psi_surf = f.read('psi_surf')\n",
    "    surf_idx = f.read('surf_idx')\n",
    "    surf_len = f.read('surf_len')\n",
    "\n",
    "r = rz[:,0]\n",
    "z = rz[:,1]\n",
    "print (nnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b876f2ea-97fe-4171-b857-beeb072d6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 16395, 39, 39)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep = 100\n",
    "with ad2.open('dataset/d3d_coarse_v2_{}.bp'.format(timestep), 'r') as f:\n",
    "    i_f = f.read('i_f')\n",
    "i_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30b8079b-d5db-40df-84c3-69e05230eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "dirname = './results/AET3d/v2_{}/model/'.format(timestep)\n",
    "os.makedirs(os.path.dirname(dirname), exist_ok=True)\n",
    "dirname = './results/AET3d/v2_{}/output/evaluations/'.format(timestep)\n",
    "os.makedirs(os.path.dirname(dirname), exist_ok=True)\n",
    "dirname = './results/AET3d/v2_{}/output/residuals/'.format(timestep)\n",
    "os.makedirs(os.path.dirname(dirname), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02505d8c-b8df-4a67-b6da-ac1626c37613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.mesh.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.f0.mesh.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.fluxavg.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.mesh.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.grad_rz.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.bfield.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.f0.mesh.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.f0analysis.static.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.equil.bp\n",
      "Reading: /gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/xgc.units.bp\n",
      "sml_00_npsi, sml_inpsi, sml_outpsi= 101 0.0 0.29281518594779\n",
      "sml_nphi_total= 8\n",
      "sml_grid_nrho= 4\n",
      "sml_exclude_private= True\n",
      "sml_rhomax= 0.008\n"
     ]
    }
   ],
   "source": [
    "exdir = '/gpfs/alpine/proj-shared/csc143/jyc/summit/xgc-deeplearning/d3d_coarse_v2/'\n",
    "xgcexp = xgc4py.XGC(exdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b7bdda-6bcd-4df5-af5c-8b0434f75773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ./dataset/xgc.mesh.bp\n",
      "Reading: ./dataset/xgc.f0.mesh.bp\n",
      "Reading: ./dataset/xgc.fluxavg.bp\n",
      "Reading: ./dataset/xgc.mesh.bp\n",
      "Reading: ./dataset/xgc.grad_rz.bp\n",
      "Reading: ./dataset/xgc.bfield.bp\n",
      "Reading: ./dataset/xgc.f0.mesh.bp\n",
      "Reading: ./dataset/xgc.f0analysis.static.bp\n",
      "Reading: ./dataset/xgc.equil.bp\n",
      "Reading: ./dataset/xgc.units.bp\n",
      "sml_00_npsi, sml_inpsi, sml_outpsi= 101 0.0 0.29281518594779\n",
      "sml_nphi_total= 8\n",
      "sml_grid_nrho= 4\n",
      "sml_exclude_private= True\n",
      "sml_rhomax= 0.008\n"
     ]
    }
   ],
   "source": [
    "xgcexp = xgc4py.XGC('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7afb0c7e-85e9-47f8-b51c-4916b0a05815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 16395, 39, 39) (8, 16395) (8, 16395)\n"
     ]
    }
   ],
   "source": [
    "all_planes = np.copy(i_f)\n",
    "\n",
    "# changed for separate tucker\n",
    "\n",
    "mu = np.mean(all_planes, axis=(2,3))\n",
    "sig = np.std(all_planes, axis=(2,3))\n",
    "all_planes = (all_planes - mu[:,:,np.newaxis, np.newaxis])/sig[:,:,np.newaxis, np.newaxis]\n",
    "print(all_planes.shape, mu.shape, sig.shape)\n",
    "#del all_planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63139f1b-80dd-40e7-9216-c3bc2b4ee248",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 500\n",
    "Lr_Rate = 1e-3\n",
    "Batch_Size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31128e95-498b-4ddc-9070-42addd7a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.enc1 = nn.Linear(in_features=input_dim, out_features=latent)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.enc1(x)        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):        \n",
    "        x = F.linear(x, weight=self.enc1.weight.transpose(0,1))        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        latent = self.encode(x)\n",
    "        recon = self.decode(latent)\n",
    "\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d926c9-726d-4695-ae54-36f982434e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189a446b-5391-4e2b-9da6-965d1555d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laent space:  300\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 300]         456,600\n",
      "================================================================\n",
      "Total params: 456,600\n",
      "Trainable params: 456,600\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 1.74\n",
      "Estimated Total Size (MB): 1.75\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "#latent_dims = [5, 10, 20, 30]\n",
    "#latent_dims = [45, 60, 90, 120]\n",
    "latent_dims = [300]\n",
    "#latent_dims = [150, 180, 210, 240, 300]\n",
    "for latent in latent_dims:\n",
    "    model = Autoencoder(1521, latent)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Lr_Rate)\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    print('laent space: ', latent)\n",
    "    print(summary(model, (1, 1521)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84f02639-5093-495d-b632-ca75f124693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, Epochs, latent_dim, timestep, plane_idx):\n",
    "    train_loss = []\n",
    "    best_loss = 100\n",
    "    for epoch in range(Epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            data = data[0].to(device)            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        loss = running_loss / len(train_loader)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.6f}'.format(\n",
    "            epoch+1, Epochs, loss))\n",
    "        if (loss < best_loss):\n",
    "            torch.save(model.state_dict(), './results/AET3d/v2_{}/model/AET3d_plane{}_latent{}_best_parameters.pt'.format(timestep,plane_idx,latent_dim))\n",
    "            best_loss = loss\n",
    "            print('best loss: ', best_loss)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12ed77e-08ab-423c-8615-181d2171f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstruct(model, test_loader, num_instance, input_dim):\n",
    "    #reconstructions = np.zeros((78*16,num_coeffs))\n",
    "    reconstructions = []\n",
    "    \n",
    "    temp = 0\n",
    "    for batch in test_loader:\n",
    "        data = batch[0]\n",
    "        data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        if (temp == 0):\n",
    "            print('Tensor type:', outputs[0][0].dtype)\n",
    "            temp += 1\n",
    "        reconstructions.append(outputs.cpu().data.numpy())\n",
    "        #reconstructions[count] = outputs.cpu().numpy()\n",
    "        #count = count + 1\n",
    "        \n",
    "    decoded_vectors = np.zeros((num_instance,input_dim), dtype=np.float32)\n",
    "    count = 0\n",
    "    for i in range(len(reconstructions)):\n",
    "        for j in range(len(reconstructions[i])):\n",
    "            decoded_vectors[count] = np.float32(reconstructions[i][j])\n",
    "            count = count + 1\n",
    "    \n",
    "    return decoded_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa874061-b3f1-4849-bc98-3076ecdbb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(model, test_loader, num_instance, latent_dim):\n",
    "    #reconstructions = np.zeros((78*16,num_coeffs))\n",
    "    latent_vectors = []\n",
    "    \n",
    "    #count = 0\n",
    "    for batch in test_loader:\n",
    "        data = batch[0]\n",
    "        data = data.to(device)\n",
    "        outputs = model.encode(data)\n",
    "        latent_vectors.append(outputs.cpu().data.numpy())\n",
    "        #reconstructions[count] = outputs.cpu().numpy()\n",
    "        #count = count + 1\n",
    "    \n",
    "    encoded_vectors = np.zeros((num_instance,latent_dim), dtype=np.float32)\n",
    "    count = 0\n",
    "    for i in range(len(latent_vectors)):\n",
    "        for j in range(len(latent_vectors[i])):\n",
    "            encoded_vectors[count] = np.float32(latent_vectors[i][j])\n",
    "            count = count + 1  \n",
    "    \n",
    "    return encoded_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c6cbb9-90b5-4e34-98d8-34bb87007e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(model, test_loader, num_instance, input_dim):\n",
    "    #reconstructions = np.zeros((78*16,num_coeffs))\n",
    "    reconstructions = []    \n",
    "\n",
    "    for batch in test_loader:\n",
    "        data = batch[0]\n",
    "        data = data.to(device)\n",
    "        outputs = model.decode(data)\n",
    "        reconstructions.append(outputs.cpu().data.numpy())\n",
    "        #reconstructions[count] = outputs.cpu().numpy()\n",
    "        #count = count + 1\n",
    "    \n",
    "    decoded_vectors = np.zeros((num_instance,input_dim), dtype=np.float32)\n",
    "    count = 0\n",
    "    for i in range(len(reconstructions)):\n",
    "        for j in range(len(reconstructions[i])):\n",
    "            decoded_vectors[count] = np.float32(reconstructions[i][j])\n",
    "            count = count + 1\n",
    "    \n",
    "    return decoded_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de7c7eee-a775-45d6-824c-3ef372743700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tucker(data):\n",
    "    shape = [data.shape[-2], data.shape[-1]]\n",
    "    \n",
    "    X_1 = np.transpose(data, (1, 2, 0))\n",
    "    X_1 = X_1.reshape(X_1.shape[0], X_1.shape[1]* X_1.shape[2])\n",
    "    #U_1, _, _ = LA.svd(X_1)\n",
    "        \n",
    "    X_2 = np.transpose(data, (2, 1, 0))\n",
    "    X_2 = X_2.reshape(X_2.shape[0], X_2.shape[1]* X_2.shape[2])\n",
    "    #U_2, _, _ = LA.svd(X_2)\n",
    "    \n",
    "    C_1 = np.zeros(shape)\n",
    "    for i in tqdm(range(X_1.shape[1]), desc=\"finding C_1\"):\n",
    "        column = X_1[:,i]\n",
    "        C_1 += np.outer(column, column.T)\n",
    "    \n",
    "    C_2 = np.zeros(shape)\n",
    "    for i in tqdm(range(X_2.shape[1]), desc=\"finding C_2\"):\n",
    "        row = X_2[:,i]\n",
    "        C_2 += np.outer(row, row.T)\n",
    "        \n",
    "    U_1 = LA.eigh(C_1)[1]\n",
    "    U_2 = LA.eigh(C_2)[1]\n",
    "    \n",
    "    coeff_tucker = np.zeros(data.shape)\n",
    "    for i, img in enumerate(tqdm(data)):\n",
    "        for j in range(U_1.shape[1]) :\n",
    "            for k in range(U_2.shape[1]):\n",
    "                coeff_tucker[i, j, k] = np.dot(U_1.T[j], np.dot(img, U_2.T[k]))\n",
    "                \n",
    "    return coeff_tucker, [U_1, U_2]\n",
    "\n",
    "def tucker_top_coeffs(coeff_tucker, num_coeffs):\n",
    "    sum_coeff_tucker= np.zeros((39, 39))\n",
    "    for j in range(39):\n",
    "        for k in range(39):\n",
    "            sum_coeff_tucker[j, k] = sum(np.abs(coeff_tucker[:, j, k]))\n",
    "            \n",
    "    max_index_pair_tucker = np.dstack(np.unravel_index(np.argsort(sum_coeff_tucker.ravel())[::-1], (39, 39)))\n",
    "    max_index_pair_tucker = np.dstack(np.unravel_index(np.argsort(sum_coeff_tucker.ravel())[::-1], (39, 39)))\n",
    "    \n",
    "    max_index_pair_tucker = max_index_pair_tucker.reshape((1521, 2))\n",
    "    top_index_pair_tucker = max_index_pair_tucker[:num_coeffs]\n",
    "    \n",
    "    return top_index_pair_tucker\n",
    "\n",
    "def compute_basis(coeff_tucker, top_index_pair_tucker, factors, train_num):\n",
    "    U_1 = factors[0]\n",
    "    U_2 = factors[1]\n",
    "    basis_product_reduced = np.array([np.outer(U_1.T[pair[0]], U_2.T[pair[1]]) for pair in top_index_pair_tucker])\n",
    "    basis_product_reduced = basis_product_reduced.reshape((basis_product_reduced.shape[0], basis_product_reduced.shape[1]*basis_product_reduced.shape[2]))\n",
    "    basis_product_reduced = basis_product_reduced.transpose()    \n",
    "    \n",
    "    return basis_product_reduced\n",
    "\n",
    "def to_tucker_basis(coeff_tucker, top_index_pair_tucker, train_num, num_coeffs):    \n",
    "    coeff_tucker_reduced = np.zeros((train_num, num_coeffs))\n",
    "    for i in range(train_num):\n",
    "        for j, pair in enumerate(top_index_pair_tucker):\n",
    "            coeff_tucker_reduced[i,j]= coeff_tucker[i, pair[0], pair[1]]\n",
    "            \n",
    "    return coeff_tucker_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dca174a-47fe-4d16-8a96-1d3fd717ad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a13eea5a-bfa1-49e3-9e51-1c003f9becc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data timestep:  1000\n",
      "Plane idx:  0\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:04<00:00, 146436.59it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 149476.35it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 253.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  1\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:04<00:00, 143629.06it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 136457.23it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 254.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  2\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:04<00:00, 150418.03it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 147130.27it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 252.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  3\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:04<00:00, 148774.18it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 140189.45it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 252.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  4\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:04<00:00, 146271.75it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 139749.98it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 252.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  5\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:04<00:00, 130340.69it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 131491.76it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 253.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  6\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:05<00:00, 125186.69it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:05<00:00, 122128.11it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 254.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "Plane idx:  7\n",
      "X_train shape:  (16395, 39, 39)\n",
      "converting data to tucker basis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding C_1: 100%|██████████| 639405/639405 [00:05<00:00, 126918.28it/s]\n",
      "finding C_2: 100%|██████████| 639405/639405 [00:04<00:00, 129063.65it/s]\n",
      "100%|██████████| 16395/16395 [01:04<00:00, 253.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n"
     ]
    }
   ],
   "source": [
    "print('data timestep: ', timestep)\n",
    "for p in range(8):    \n",
    "    X_train = all_planes[p]\n",
    "    #X_train = i_f[p]\n",
    "    plane_idx = p\n",
    "    print('Plane idx: ', plane_idx)\n",
    "    print('X_train shape: ', X_train.shape)\n",
    "    \n",
    "    train_num = X_train.shape[0]\n",
    "    \n",
    "    tqdm.write('converting data to tucker basis...')\n",
    "    coeff_tucker, factors = tucker(X_train)\n",
    "    \n",
    "    Num_coeffs = [1521]\n",
    "    \n",
    "    for num_coeffs in Num_coeffs:\n",
    "        \n",
    "        tqdm.write('tucker_coeffs: '+str(num_coeffs))\n",
    "        \n",
    "        top_index_pair_tucker = tucker_top_coeffs(coeff_tucker, num_coeffs)\n",
    "        basis_product = compute_basis(coeff_tucker, top_index_pair_tucker, factors, train_num)\n",
    "        print(\"basis product shape: \", basis_product.shape)\n",
    "        np.save('./results/AET3d/v2_{}/basis_product_{}_plane{}.npy'.format(timestep, num_coeffs, plane_idx), basis_product)\n",
    "        X_train_tucker = to_tucker_basis(coeff_tucker, top_index_pair_tucker, train_num, num_coeffs)\n",
    "        print('X_train_tucker shape: ', X_train_tucker.shape)\n",
    "        np.save('./results/AET3d/v2_{}/X_train_tucker_{}_plane{}.npy'.format(timestep, num_coeffs, plane_idx), X_train_tucker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b87719ba-4876-4996-86e7-bacbbf928428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39, 39), (1521, 2), (16395, 39, 39))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors[0].shape, top_index_pair_tucker.shape, coeff_tucker.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db32398-a916-419b-b15b-0e89d76d0d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 39)\n",
      "(1, 1521, 2)\n",
      "(1521, 2)\n"
     ]
    }
   ],
   "source": [
    "sum_coeff_tucker= np.zeros((39, 39))\n",
    "for j in range(39):\n",
    "    for k in range(39):\n",
    "        sum_coeff_tucker[j, k] = sum(np.abs(coeff_tucker[:, j, k]))\n",
    "\n",
    "print(sum_coeff_tucker.shape)\n",
    "max_index_pair_tucker = np.dstack(np.unravel_index(np.argsort(sum_coeff_tucker.ravel())[::-1], (39, 39)))\n",
    "print(max_index_pair_tucker.shape)\n",
    "max_index_pair_tucker = max_index_pair_tucker.reshape((1521, 2))\n",
    "print(max_index_pair_tucker.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4339a621-7158-4087-a51d-035eca4a2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n"
     ]
    }
   ],
   "source": [
    "print(\"basis product shape: \", basis_product.shape)\n",
    "print('X_train_tucker shape: ', X_train_tucker.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f58f500-2016-41b5-ab4d-a8397196ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16395, 1521)\n",
      "(16395, 39, 39)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_tucker@basis_product.T\n",
    "print(X_train.shape)\n",
    "X_train = X_train.reshape(16395,39,39)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e294db1d-69e7-4720-9de1-d7a28b45d94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(all_planes[7][12050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec38fb3a-b148-46ce-8ddb-ef20b96f93c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(X_train[12050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ece41-fe5d-4075-969c-fe936c34204e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pqbit:  4\n",
      "data timestep:  1000\n",
      "plane idx:  0\n",
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "X_train max avg:  36.71770648409709 0.013919332462742124\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=5, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.853509\n",
      "best loss:  0.8535092620886573\n",
      "Epoch 2 of 500, Train Loss: 0.308331\n",
      "best loss:  0.30833093307970105\n",
      "Epoch 3 of 500, Train Loss: 0.090595\n",
      "best loss:  0.09059534790788511\n",
      "Epoch 4 of 500, Train Loss: 0.058426\n",
      "best loss:  0.05842640948149758\n",
      "Epoch 5 of 500, Train Loss: 0.056510\n",
      "best loss:  0.05650998438823997\n",
      "Epoch 6 of 500, Train Loss: 0.056404\n",
      "best loss:  0.056403991581272885\n",
      "Epoch 7 of 500, Train Loss: 0.056334\n",
      "best loss:  0.056333760633458234\n",
      "Epoch 8 of 500, Train Loss: 0.056245\n",
      "best loss:  0.05624486095734056\n",
      "Epoch 9 of 500, Train Loss: 0.056126\n",
      "best loss:  0.05612583149426667\n",
      "Epoch 10 of 500, Train Loss: 0.055965\n",
      "best loss:  0.05596453255690389\n",
      "Epoch 11 of 500, Train Loss: 0.055746\n",
      "best loss:  0.055745858060152725\n",
      "Epoch 12 of 500, Train Loss: 0.055451\n",
      "best loss:  0.05545130124191082\n",
      "Epoch 13 of 500, Train Loss: 0.055060\n",
      "best loss:  0.05505958681496568\n",
      "Epoch 14 of 500, Train Loss: 0.054548\n",
      "best loss:  0.054547655694421326\n",
      "Epoch 15 of 500, Train Loss: 0.053893\n",
      "best loss:  0.0538929037527766\n",
      "Epoch 16 of 500, Train Loss: 0.053077\n",
      "best loss:  0.05307683330426325\n",
      "Epoch 17 of 500, Train Loss: 0.052090\n",
      "best loss:  0.052089770037550914\n",
      "Epoch 18 of 500, Train Loss: 0.050936\n",
      "best loss:  0.05093597193734085\n",
      "Epoch 19 of 500, Train Loss: 0.049637\n",
      "best loss:  0.049637208411618054\n",
      "Epoch 20 of 500, Train Loss: 0.048234\n",
      "best loss:  0.048233850274772866\n",
      "Epoch 21 of 500, Train Loss: 0.046780\n",
      "best loss:  0.04678040560986486\n",
      "Epoch 22 of 500, Train Loss: 0.045336\n",
      "best loss:  0.045335833084329154\n",
      "Epoch 23 of 500, Train Loss: 0.043952\n",
      "best loss:  0.04395239369884177\n",
      "Epoch 24 of 500, Train Loss: 0.042666\n",
      "best loss:  0.04266581814799299\n",
      "Epoch 25 of 500, Train Loss: 0.041495\n",
      "best loss:  0.041494925142276685\n",
      "Epoch 26 of 500, Train Loss: 0.040444\n",
      "best loss:  0.04044389892811706\n",
      "Epoch 27 of 500, Train Loss: 0.039507\n",
      "best loss:  0.039507482440008734\n",
      "Epoch 28 of 500, Train Loss: 0.038675\n",
      "best loss:  0.03867501877302434\n",
      "Epoch 29 of 500, Train Loss: 0.037933\n",
      "best loss:  0.037933162913040455\n",
      "Epoch 30 of 500, Train Loss: 0.037267\n",
      "best loss:  0.03726693072497065\n",
      "Epoch 31 of 500, Train Loss: 0.036661\n",
      "best loss:  0.03666139755834818\n",
      "Epoch 32 of 500, Train Loss: 0.036102\n",
      "best loss:  0.03610238030709889\n",
      "Epoch 33 of 500, Train Loss: 0.035577\n",
      "best loss:  0.035577281607430455\n",
      "Epoch 34 of 500, Train Loss: 0.035074\n",
      "best loss:  0.03507443538518594\n",
      "Epoch 35 of 500, Train Loss: 0.034584\n",
      "best loss:  0.03458372945716792\n",
      "Epoch 36 of 500, Train Loss: 0.034097\n",
      "best loss:  0.03409666276100099\n",
      "Epoch 37 of 500, Train Loss: 0.033606\n",
      "best loss:  0.03360587602856663\n",
      "Epoch 38 of 500, Train Loss: 0.033106\n",
      "best loss:  0.03310609558938331\n",
      "Epoch 39 of 500, Train Loss: 0.032594\n",
      "best loss:  0.03259366284875387\n",
      "Epoch 40 of 500, Train Loss: 0.032068\n",
      "best loss:  0.03206796995625839\n",
      "Epoch 41 of 500, Train Loss: 0.031531\n",
      "best loss:  0.0315309789649016\n",
      "Epoch 42 of 500, Train Loss: 0.030988\n",
      "best loss:  0.03098832831046609\n",
      "Epoch 43 of 500, Train Loss: 0.030449\n",
      "best loss:  0.030448946240448298\n",
      "Epoch 44 of 500, Train Loss: 0.029925\n",
      "best loss:  0.029924994913045344\n",
      "Epoch 45 of 500, Train Loss: 0.029429\n",
      "best loss:  0.029429439191757015\n",
      "Epoch 46 of 500, Train Loss: 0.028975\n",
      "best loss:  0.028974876850987822\n",
      "Epoch 47 of 500, Train Loss: 0.028570\n",
      "best loss:  0.02857026992575085\n",
      "Epoch 48 of 500, Train Loss: 0.028221\n",
      "best loss:  0.028220953640000997\n",
      "Epoch 49 of 500, Train Loss: 0.027927\n",
      "best loss:  0.027926530467902215\n",
      "Epoch 50 of 500, Train Loss: 0.027683\n",
      "best loss:  0.027682543856763216\n",
      "Epoch 51 of 500, Train Loss: 0.027483\n",
      "best loss:  0.02748266666678192\n",
      "Epoch 52 of 500, Train Loss: 0.027319\n",
      "best loss:  0.02731896179517038\n",
      "Epoch 53 of 500, Train Loss: 0.027185\n",
      "best loss:  0.027184662366260652\n",
      "Epoch 54 of 500, Train Loss: 0.027074\n",
      "best loss:  0.027073821395837753\n",
      "Epoch 55 of 500, Train Loss: 0.026982\n",
      "best loss:  0.026981753107185598\n",
      "Epoch 56 of 500, Train Loss: 0.026905\n",
      "best loss:  0.026904742696409134\n",
      "Epoch 57 of 500, Train Loss: 0.026840\n",
      "best loss:  0.026839676062673935\n",
      "Epoch 58 of 500, Train Loss: 0.026785\n",
      "best loss:  0.02678463479247695\n",
      "Epoch 59 of 500, Train Loss: 0.026738\n",
      "best loss:  0.02673784076180237\n",
      "Epoch 60 of 500, Train Loss: 0.026698\n",
      "best loss:  0.02669776797289202\n",
      "Epoch 61 of 500, Train Loss: 0.026663\n",
      "best loss:  0.026663255926976052\n",
      "Epoch 62 of 500, Train Loss: 0.026633\n",
      "best loss:  0.026633483318471197\n",
      "Epoch 63 of 500, Train Loss: 0.026608\n",
      "best loss:  0.026607588934351133\n",
      "Epoch 64 of 500, Train Loss: 0.026585\n",
      "best loss:  0.026584863851330597\n",
      "Epoch 65 of 500, Train Loss: 0.026565\n",
      "best loss:  0.026564772586453733\n",
      "Epoch 66 of 500, Train Loss: 0.026547\n",
      "best loss:  0.026546545769358703\n",
      "Epoch 67 of 500, Train Loss: 0.026530\n",
      "best loss:  0.026530287947321613\n",
      "Epoch 68 of 500, Train Loss: 0.026516\n",
      "best loss:  0.02651559955678707\n",
      "Epoch 69 of 500, Train Loss: 0.026502\n",
      "best loss:  0.026502213405640147\n",
      "Epoch 70 of 500, Train Loss: 0.026490\n",
      "best loss:  0.02648992292888015\n",
      "Epoch 71 of 500, Train Loss: 0.026478\n",
      "best loss:  0.026478401101428958\n",
      "Epoch 72 of 500, Train Loss: 0.026468\n",
      "best loss:  0.026467853705416787\n",
      "Epoch 73 of 500, Train Loss: 0.026458\n",
      "best loss:  0.026457759357227172\n",
      "Epoch 74 of 500, Train Loss: 0.026448\n",
      "best loss:  0.026448216894370786\n",
      "Epoch 75 of 500, Train Loss: 0.026439\n",
      "best loss:  0.026439158282921354\n",
      "Epoch 76 of 500, Train Loss: 0.026431\n",
      "best loss:  0.02643063761945453\n",
      "Epoch 77 of 500, Train Loss: 0.026423\n",
      "best loss:  0.02642255967037117\n",
      "Epoch 78 of 500, Train Loss: 0.026415\n",
      "best loss:  0.02641464563812194\n",
      "Epoch 79 of 500, Train Loss: 0.026407\n",
      "best loss:  0.026407244776343104\n",
      "Epoch 80 of 500, Train Loss: 0.026400\n",
      "best loss:  0.026399914654018718\n",
      "Epoch 81 of 500, Train Loss: 0.026393\n",
      "best loss:  0.026392824279122748\n",
      "Epoch 82 of 500, Train Loss: 0.026386\n",
      "best loss:  0.0263859257882627\n",
      "Epoch 83 of 500, Train Loss: 0.026379\n",
      "best loss:  0.02637917471935886\n",
      "Epoch 84 of 500, Train Loss: 0.026373\n",
      "best loss:  0.026372659840899006\n",
      "Epoch 85 of 500, Train Loss: 0.026366\n",
      "best loss:  0.026366350822179585\n",
      "Epoch 86 of 500, Train Loss: 0.026360\n",
      "best loss:  0.026360173011649862\n",
      "Epoch 87 of 500, Train Loss: 0.026354\n",
      "best loss:  0.026354245761185596\n",
      "Epoch 88 of 500, Train Loss: 0.026348\n",
      "best loss:  0.02634835468926431\n",
      "Epoch 89 of 500, Train Loss: 0.026343\n",
      "best loss:  0.02634260114715543\n",
      "Epoch 90 of 500, Train Loss: 0.026337\n",
      "best loss:  0.026336858095059736\n",
      "Epoch 91 of 500, Train Loss: 0.026331\n",
      "best loss:  0.026331342339818906\n",
      "Epoch 92 of 500, Train Loss: 0.026326\n",
      "best loss:  0.026326074732861\n",
      "Epoch 93 of 500, Train Loss: 0.026321\n",
      "best loss:  0.026321025293179635\n",
      "Epoch 94 of 500, Train Loss: 0.026316\n",
      "best loss:  0.026315843545680128\n",
      "Epoch 95 of 500, Train Loss: 0.026311\n",
      "best loss:  0.026311078710326303\n",
      "Epoch 96 of 500, Train Loss: 0.026306\n",
      "best loss:  0.026306343314963457\n",
      "Epoch 97 of 500, Train Loss: 0.026302\n",
      "best loss:  0.02630169013137893\n",
      "Epoch 98 of 500, Train Loss: 0.026297\n",
      "best loss:  0.026297025588276082\n",
      "Epoch 99 of 500, Train Loss: 0.026293\n",
      "best loss:  0.02629263791916242\n",
      "Epoch 100 of 500, Train Loss: 0.026288\n",
      "best loss:  0.026288414772874278\n",
      "Epoch 101 of 500, Train Loss: 0.026284\n",
      "best loss:  0.0262842839349119\n",
      "Epoch 102 of 500, Train Loss: 0.026280\n",
      "best loss:  0.026280045664107587\n",
      "Epoch 103 of 500, Train Loss: 0.026276\n",
      "best loss:  0.026276079755521906\n",
      "Epoch 104 of 500, Train Loss: 0.026272\n",
      "best loss:  0.026272276762057123\n",
      "Epoch 105 of 500, Train Loss: 0.026268\n",
      "best loss:  0.026268466476841952\n",
      "Epoch 106 of 500, Train Loss: 0.026265\n",
      "best loss:  0.026264826310960967\n",
      "Epoch 107 of 500, Train Loss: 0.026261\n",
      "best loss:  0.026261242790609714\n",
      "Epoch 108 of 500, Train Loss: 0.026258\n",
      "best loss:  0.026257761281665935\n",
      "Epoch 109 of 500, Train Loss: 0.026254\n",
      "best loss:  0.026254350437273727\n",
      "Epoch 110 of 500, Train Loss: 0.026251\n",
      "best loss:  0.026251187192478268\n",
      "Epoch 111 of 500, Train Loss: 0.026248\n",
      "best loss:  0.026247944718031862\n",
      "Epoch 112 of 500, Train Loss: 0.026245\n",
      "best loss:  0.026244777872032204\n",
      "Epoch 113 of 500, Train Loss: 0.026242\n",
      "best loss:  0.026241830026265234\n",
      "Epoch 114 of 500, Train Loss: 0.026239\n",
      "best loss:  0.026238880443744103\n",
      "Epoch 115 of 500, Train Loss: 0.026236\n",
      "best loss:  0.026236006953074893\n",
      "Epoch 116 of 500, Train Loss: 0.026233\n",
      "best loss:  0.026233206945290976\n",
      "Epoch 117 of 500, Train Loss: 0.026231\n",
      "best loss:  0.02623057122930682\n",
      "Epoch 118 of 500, Train Loss: 0.026228\n",
      "best loss:  0.026227949320721304\n",
      "Epoch 119 of 500, Train Loss: 0.026225\n",
      "best loss:  0.02622535990419357\n",
      "Epoch 120 of 500, Train Loss: 0.026223\n",
      "best loss:  0.02622290791459959\n",
      "Epoch 121 of 500, Train Loss: 0.026220\n",
      "best loss:  0.02622041198453873\n",
      "Epoch 122 of 500, Train Loss: 0.026218\n",
      "best loss:  0.026218045231265325\n",
      "Epoch 123 of 500, Train Loss: 0.026216\n",
      "best loss:  0.026215765257943564\n",
      "Epoch 124 of 500, Train Loss: 0.026214\n",
      "best loss:  0.026213519272932334\n",
      "Epoch 125 of 500, Train Loss: 0.026211\n",
      "best loss:  0.02621125424499154\n",
      "Epoch 126 of 500, Train Loss: 0.026209\n",
      "best loss:  0.026209038745904297\n",
      "Epoch 127 of 500, Train Loss: 0.026207\n",
      "best loss:  0.026206858153364716\n",
      "Epoch 128 of 500, Train Loss: 0.026205\n",
      "best loss:  0.02620477809177264\n",
      "Epoch 129 of 500, Train Loss: 0.026203\n",
      "best loss:  0.02620271695759725\n",
      "Epoch 130 of 500, Train Loss: 0.026201\n",
      "best loss:  0.026200775448287035\n",
      "Epoch 131 of 500, Train Loss: 0.026199\n",
      "best loss:  0.02619862386030903\n",
      "Epoch 132 of 500, Train Loss: 0.026197\n",
      "best loss:  0.026196626476156563\n",
      "Epoch 133 of 500, Train Loss: 0.026195\n",
      "best loss:  0.026194645716834004\n",
      "Epoch 134 of 500, Train Loss: 0.026193\n",
      "best loss:  0.026192770157254828\n",
      "Epoch 135 of 500, Train Loss: 0.026191\n",
      "best loss:  0.026190745302696185\n",
      "Epoch 136 of 500, Train Loss: 0.026189\n",
      "best loss:  0.02618896032416428\n",
      "Epoch 137 of 500, Train Loss: 0.026187\n",
      "best loss:  0.026186943181895093\n",
      "Epoch 138 of 500, Train Loss: 0.026185\n",
      "best loss:  0.026184996832324583\n",
      "Epoch 139 of 500, Train Loss: 0.026183\n",
      "best loss:  0.026183047750152563\n",
      "Epoch 140 of 500, Train Loss: 0.026181\n",
      "best loss:  0.026181275735233815\n",
      "Epoch 141 of 500, Train Loss: 0.026179\n",
      "best loss:  0.026179428706884153\n",
      "Epoch 142 of 500, Train Loss: 0.026178\n",
      "best loss:  0.026177558759606423\n",
      "Epoch 143 of 500, Train Loss: 0.026176\n",
      "best loss:  0.02617577882348235\n",
      "Epoch 144 of 500, Train Loss: 0.026174\n",
      "best loss:  0.02617391066033711\n",
      "Epoch 145 of 500, Train Loss: 0.026172\n",
      "best loss:  0.026172056147565185\n",
      "Epoch 146 of 500, Train Loss: 0.026170\n",
      "best loss:  0.02617044602213933\n",
      "Epoch 147 of 500, Train Loss: 0.026169\n",
      "best loss:  0.02616867944354538\n",
      "Epoch 148 of 500, Train Loss: 0.026167\n",
      "best loss:  0.026166795057609188\n",
      "Epoch 149 of 500, Train Loss: 0.026165\n",
      "best loss:  0.026165082025551057\n",
      "Epoch 150 of 500, Train Loss: 0.026163\n",
      "best loss:  0.02616347321769396\n",
      "Epoch 151 of 500, Train Loss: 0.026162\n",
      "best loss:  0.026161518708672806\n",
      "Epoch 152 of 500, Train Loss: 0.026160\n",
      "best loss:  0.02615975808801649\n",
      "Epoch 153 of 500, Train Loss: 0.026158\n",
      "best loss:  0.026157797995120892\n",
      "Epoch 154 of 500, Train Loss: 0.026156\n",
      "best loss:  0.026156032859986692\n",
      "Epoch 155 of 500, Train Loss: 0.026155\n",
      "best loss:  0.02615472824536553\n",
      "Epoch 156 of 500, Train Loss: 0.026153\n",
      "best loss:  0.026152717462170495\n",
      "Epoch 157 of 500, Train Loss: 0.026151\n",
      "best loss:  0.02615095959757924\n",
      "Epoch 158 of 500, Train Loss: 0.026149\n",
      "best loss:  0.026149236735194748\n",
      "Epoch 159 of 500, Train Loss: 0.026148\n",
      "best loss:  0.026147594855910585\n",
      "Epoch 160 of 500, Train Loss: 0.026146\n",
      "best loss:  0.0261456508161464\n",
      "Epoch 161 of 500, Train Loss: 0.026144\n",
      "best loss:  0.026144013986489627\n",
      "Epoch 162 of 500, Train Loss: 0.026142\n",
      "best loss:  0.026142305518543824\n",
      "Epoch 163 of 500, Train Loss: 0.026141\n",
      "best loss:  0.02614082410514658\n",
      "Epoch 164 of 500, Train Loss: 0.026139\n",
      "best loss:  0.02613895906084903\n",
      "Epoch 165 of 500, Train Loss: 0.026137\n",
      "best loss:  0.026137253995569668\n",
      "Epoch 166 of 500, Train Loss: 0.026136\n",
      "best loss:  0.026135849917141445\n",
      "Epoch 167 of 500, Train Loss: 0.026134\n",
      "best loss:  0.02613436183332675\n",
      "Epoch 168 of 500, Train Loss: 0.026133\n",
      "best loss:  0.026132916847998348\n",
      "Epoch 169 of 500, Train Loss: 0.026131\n",
      "best loss:  0.0261312989994537\n",
      "Epoch 170 of 500, Train Loss: 0.026130\n",
      "best loss:  0.02612999709758027\n",
      "Epoch 171 of 500, Train Loss: 0.026129\n",
      "best loss:  0.026128674669609976\n",
      "Epoch 172 of 500, Train Loss: 0.026128\n",
      "best loss:  0.026127597256184576\n",
      "Epoch 173 of 500, Train Loss: 0.026127\n",
      "best loss:  0.026126509919029827\n",
      "Epoch 174 of 500, Train Loss: 0.026125\n",
      "best loss:  0.0261252143631451\n",
      "Epoch 175 of 500, Train Loss: 0.026124\n",
      "best loss:  0.026123969396937265\n",
      "Epoch 176 of 500, Train Loss: 0.026123\n",
      "best loss:  0.02612300297333823\n",
      "Epoch 177 of 500, Train Loss: 0.026122\n",
      "best loss:  0.0261222135199797\n",
      "Epoch 178 of 500, Train Loss: 0.026121\n",
      "best loss:  0.026121406012770625\n",
      "Epoch 179 of 500, Train Loss: 0.026121\n",
      "best loss:  0.026120649266702867\n",
      "Epoch 180 of 500, Train Loss: 0.026120\n",
      "best loss:  0.026119969542455938\n",
      "Epoch 181 of 500, Train Loss: 0.026120\n",
      "best loss:  0.02611980547310871\n",
      "Epoch 182 of 500, Train Loss: 0.026120\n",
      "best loss:  0.026119739402901354\n",
      "Epoch 183 of 500, Train Loss: 0.026120\n",
      "best loss:  0.026119679302814627\n",
      "Epoch 184 of 500, Train Loss: 0.026120\n",
      "best loss:  0.026119535317366525\n",
      "Epoch 185 of 500, Train Loss: 0.026119\n",
      "best loss:  0.026119070619889787\n",
      "Epoch 186 of 500, Train Loss: 0.026119\n",
      "best loss:  0.026118922169147015\n",
      "Epoch 187 of 500, Train Loss: 0.026119\n",
      "Epoch 188 of 500, Train Loss: 0.026119\n",
      "Epoch 189 of 500, Train Loss: 0.026120\n",
      "Epoch 190 of 500, Train Loss: 0.026121\n",
      "Epoch 191 of 500, Train Loss: 0.026121\n",
      "Epoch 192 of 500, Train Loss: 0.026122\n",
      "Epoch 193 of 500, Train Loss: 0.026122\n",
      "Epoch 194 of 500, Train Loss: 0.026123\n",
      "Epoch 195 of 500, Train Loss: 0.026124\n",
      "Epoch 196 of 500, Train Loss: 0.026125\n",
      "Epoch 197 of 500, Train Loss: 0.026127\n",
      "Epoch 198 of 500, Train Loss: 0.026128\n",
      "Epoch 199 of 500, Train Loss: 0.026129\n",
      "Epoch 200 of 500, Train Loss: 0.026130\n",
      "Epoch 201 of 500, Train Loss: 0.026132\n",
      "Epoch 202 of 500, Train Loss: 0.026134\n",
      "Epoch 203 of 500, Train Loss: 0.026135\n",
      "Epoch 204 of 500, Train Loss: 0.026137\n",
      "Epoch 205 of 500, Train Loss: 0.026139\n",
      "Epoch 206 of 500, Train Loss: 0.026140\n",
      "Epoch 207 of 500, Train Loss: 0.026142\n",
      "Epoch 208 of 500, Train Loss: 0.026143\n",
      "Epoch 209 of 500, Train Loss: 0.026144\n",
      "Epoch 210 of 500, Train Loss: 0.026146\n",
      "Epoch 211 of 500, Train Loss: 0.026149\n",
      "Epoch 212 of 500, Train Loss: 0.026150\n",
      "Epoch 213 of 500, Train Loss: 0.026150\n",
      "Epoch 214 of 500, Train Loss: 0.026153\n",
      "Epoch 215 of 500, Train Loss: 0.026154\n",
      "Epoch 216 of 500, Train Loss: 0.026155\n",
      "Epoch 217 of 500, Train Loss: 0.026157\n",
      "Epoch 218 of 500, Train Loss: 0.026158\n",
      "Epoch 219 of 500, Train Loss: 0.026159\n",
      "Epoch 220 of 500, Train Loss: 0.026160\n",
      "Epoch 221 of 500, Train Loss: 0.026161\n",
      "Epoch 222 of 500, Train Loss: 0.026162\n",
      "Epoch 223 of 500, Train Loss: 0.026164\n",
      "Epoch 224 of 500, Train Loss: 0.026163\n",
      "Epoch 225 of 500, Train Loss: 0.026164\n",
      "Epoch 226 of 500, Train Loss: 0.026166\n",
      "Epoch 227 of 500, Train Loss: 0.026167\n",
      "Epoch 228 of 500, Train Loss: 0.026168\n",
      "Epoch 229 of 500, Train Loss: 0.026169\n",
      "Epoch 230 of 500, Train Loss: 0.026170\n",
      "Epoch 231 of 500, Train Loss: 0.026171\n",
      "Epoch 232 of 500, Train Loss: 0.026172\n",
      "Epoch 233 of 500, Train Loss: 0.026172\n",
      "Epoch 234 of 500, Train Loss: 0.026173\n",
      "Epoch 235 of 500, Train Loss: 0.026174\n",
      "Epoch 236 of 500, Train Loss: 0.026176\n",
      "Epoch 237 of 500, Train Loss: 0.026176\n",
      "Epoch 238 of 500, Train Loss: 0.026177\n",
      "Epoch 239 of 500, Train Loss: 0.026177\n",
      "Epoch 240 of 500, Train Loss: 0.026178\n",
      "Epoch 241 of 500, Train Loss: 0.026180\n",
      "Epoch 242 of 500, Train Loss: 0.026180\n",
      "Epoch 243 of 500, Train Loss: 0.026180\n",
      "Epoch 244 of 500, Train Loss: 0.026181\n",
      "Epoch 245 of 500, Train Loss: 0.026181\n",
      "Epoch 246 of 500, Train Loss: 0.026183\n",
      "Epoch 247 of 500, Train Loss: 0.026183\n",
      "Epoch 248 of 500, Train Loss: 0.026183\n",
      "Epoch 249 of 500, Train Loss: 0.026184\n",
      "Epoch 250 of 500, Train Loss: 0.026185\n",
      "Epoch 251 of 500, Train Loss: 0.026186\n",
      "Epoch 252 of 500, Train Loss: 0.026186\n",
      "Epoch 253 of 500, Train Loss: 0.026186\n",
      "Epoch 254 of 500, Train Loss: 0.026186\n",
      "Epoch 255 of 500, Train Loss: 0.026187\n",
      "Epoch 256 of 500, Train Loss: 0.026188\n",
      "Epoch 257 of 500, Train Loss: 0.026188\n",
      "Epoch 258 of 500, Train Loss: 0.026189\n",
      "Epoch 259 of 500, Train Loss: 0.026189\n",
      "Epoch 260 of 500, Train Loss: 0.026189\n",
      "Epoch 261 of 500, Train Loss: 0.026189\n",
      "Epoch 262 of 500, Train Loss: 0.026190\n",
      "Epoch 263 of 500, Train Loss: 0.026191\n",
      "Epoch 264 of 500, Train Loss: 0.026191\n",
      "Epoch 265 of 500, Train Loss: 0.026192\n",
      "Epoch 266 of 500, Train Loss: 0.026192\n",
      "Epoch 267 of 500, Train Loss: 0.026192\n",
      "Epoch 268 of 500, Train Loss: 0.026193\n",
      "Epoch 269 of 500, Train Loss: 0.026193\n",
      "Epoch 270 of 500, Train Loss: 0.026193\n",
      "Epoch 271 of 500, Train Loss: 0.026194\n",
      "Epoch 272 of 500, Train Loss: 0.026195\n",
      "Epoch 273 of 500, Train Loss: 0.026195\n",
      "Epoch 274 of 500, Train Loss: 0.026195\n",
      "Epoch 275 of 500, Train Loss: 0.026195\n",
      "Epoch 276 of 500, Train Loss: 0.026196\n",
      "Epoch 277 of 500, Train Loss: 0.026197\n",
      "Epoch 278 of 500, Train Loss: 0.026196\n",
      "Epoch 279 of 500, Train Loss: 0.026197\n",
      "Epoch 280 of 500, Train Loss: 0.026197\n",
      "Epoch 281 of 500, Train Loss: 0.026197\n",
      "Epoch 282 of 500, Train Loss: 0.026198\n",
      "Epoch 283 of 500, Train Loss: 0.026199\n",
      "Epoch 284 of 500, Train Loss: 0.026198\n",
      "Epoch 285 of 500, Train Loss: 0.026199\n",
      "Epoch 286 of 500, Train Loss: 0.026199\n",
      "Epoch 287 of 500, Train Loss: 0.026200\n",
      "Epoch 288 of 500, Train Loss: 0.026200\n",
      "Epoch 289 of 500, Train Loss: 0.026200\n",
      "Epoch 290 of 500, Train Loss: 0.026201\n",
      "Epoch 291 of 500, Train Loss: 0.026201\n",
      "Epoch 292 of 500, Train Loss: 0.026201\n",
      "Epoch 293 of 500, Train Loss: 0.026202\n",
      "Epoch 294 of 500, Train Loss: 0.026202\n",
      "Epoch 295 of 500, Train Loss: 0.026203\n",
      "Epoch 296 of 500, Train Loss: 0.026203\n",
      "Epoch 297 of 500, Train Loss: 0.026203\n",
      "Epoch 298 of 500, Train Loss: 0.026203\n",
      "Epoch 299 of 500, Train Loss: 0.026204\n",
      "Epoch 300 of 500, Train Loss: 0.026205\n",
      "Epoch 301 of 500, Train Loss: 0.026206\n",
      "Epoch 302 of 500, Train Loss: 0.026205\n",
      "Epoch 303 of 500, Train Loss: 0.026206\n",
      "Epoch 304 of 500, Train Loss: 0.026206\n",
      "Epoch 305 of 500, Train Loss: 0.026206\n",
      "Epoch 306 of 500, Train Loss: 0.026206\n",
      "Epoch 307 of 500, Train Loss: 0.026207\n",
      "Epoch 308 of 500, Train Loss: 0.026207\n",
      "Epoch 309 of 500, Train Loss: 0.026208\n",
      "Epoch 310 of 500, Train Loss: 0.026208\n",
      "Epoch 311 of 500, Train Loss: 0.026208\n",
      "Epoch 312 of 500, Train Loss: 0.026209\n",
      "Epoch 313 of 500, Train Loss: 0.026209\n",
      "Epoch 314 of 500, Train Loss: 0.026210\n",
      "Epoch 315 of 500, Train Loss: 0.026210\n",
      "Epoch 316 of 500, Train Loss: 0.026211\n",
      "Epoch 317 of 500, Train Loss: 0.026211\n",
      "Epoch 318 of 500, Train Loss: 0.026211\n",
      "Epoch 319 of 500, Train Loss: 0.026211\n",
      "Epoch 320 of 500, Train Loss: 0.026212\n",
      "Epoch 321 of 500, Train Loss: 0.026213\n",
      "Epoch 322 of 500, Train Loss: 0.026212\n",
      "Epoch 323 of 500, Train Loss: 0.026212\n",
      "Epoch 324 of 500, Train Loss: 0.026213\n",
      "Epoch 325 of 500, Train Loss: 0.026213\n",
      "Epoch 326 of 500, Train Loss: 0.026213\n",
      "Epoch 327 of 500, Train Loss: 0.026214\n",
      "Epoch 328 of 500, Train Loss: 0.026214\n",
      "Epoch 329 of 500, Train Loss: 0.026215\n",
      "Epoch 330 of 500, Train Loss: 0.026215\n",
      "Epoch 331 of 500, Train Loss: 0.026215\n",
      "Epoch 332 of 500, Train Loss: 0.026216\n",
      "Epoch 333 of 500, Train Loss: 0.026216\n",
      "Epoch 334 of 500, Train Loss: 0.026216\n",
      "Epoch 335 of 500, Train Loss: 0.026216\n",
      "Epoch 336 of 500, Train Loss: 0.026217\n",
      "Epoch 337 of 500, Train Loss: 0.026217\n",
      "Epoch 338 of 500, Train Loss: 0.026217\n",
      "Epoch 339 of 500, Train Loss: 0.026218\n",
      "Epoch 340 of 500, Train Loss: 0.026218\n",
      "Epoch 341 of 500, Train Loss: 0.026219\n",
      "Epoch 342 of 500, Train Loss: 0.026219\n",
      "Epoch 343 of 500, Train Loss: 0.026219\n",
      "Epoch 344 of 500, Train Loss: 0.026219\n",
      "Epoch 345 of 500, Train Loss: 0.026220\n",
      "Epoch 346 of 500, Train Loss: 0.026220\n",
      "Epoch 347 of 500, Train Loss: 0.026221\n",
      "Epoch 348 of 500, Train Loss: 0.026221\n",
      "Epoch 349 of 500, Train Loss: 0.026222\n",
      "Epoch 350 of 500, Train Loss: 0.026222\n",
      "Epoch 351 of 500, Train Loss: 0.026223\n",
      "Epoch 352 of 500, Train Loss: 0.026222\n",
      "Epoch 353 of 500, Train Loss: 0.026222\n",
      "Epoch 354 of 500, Train Loss: 0.026222\n",
      "Epoch 355 of 500, Train Loss: 0.026223\n",
      "Epoch 356 of 500, Train Loss: 0.026223\n",
      "Epoch 357 of 500, Train Loss: 0.026224\n",
      "Epoch 358 of 500, Train Loss: 0.026224\n",
      "Epoch 359 of 500, Train Loss: 0.026224\n",
      "Epoch 360 of 500, Train Loss: 0.026224\n",
      "Epoch 361 of 500, Train Loss: 0.026225\n",
      "Epoch 362 of 500, Train Loss: 0.026226\n",
      "Epoch 363 of 500, Train Loss: 0.026226\n",
      "Epoch 364 of 500, Train Loss: 0.026226\n",
      "Epoch 365 of 500, Train Loss: 0.026227\n",
      "Epoch 366 of 500, Train Loss: 0.026227\n",
      "Epoch 367 of 500, Train Loss: 0.026227\n",
      "Epoch 368 of 500, Train Loss: 0.026227\n",
      "Epoch 369 of 500, Train Loss: 0.026227\n",
      "Epoch 370 of 500, Train Loss: 0.026227\n",
      "Epoch 371 of 500, Train Loss: 0.026228\n",
      "Epoch 372 of 500, Train Loss: 0.026229\n",
      "Epoch 373 of 500, Train Loss: 0.026229\n",
      "Epoch 374 of 500, Train Loss: 0.026229\n",
      "Epoch 375 of 500, Train Loss: 0.026229\n",
      "Epoch 376 of 500, Train Loss: 0.026229\n",
      "Epoch 377 of 500, Train Loss: 0.026230\n",
      "Epoch 378 of 500, Train Loss: 0.026230\n",
      "Epoch 379 of 500, Train Loss: 0.026230\n",
      "Epoch 380 of 500, Train Loss: 0.026230\n",
      "Epoch 381 of 500, Train Loss: 0.026231\n",
      "Epoch 382 of 500, Train Loss: 0.026231\n",
      "Epoch 383 of 500, Train Loss: 0.026231\n",
      "Epoch 384 of 500, Train Loss: 0.026231\n",
      "Epoch 385 of 500, Train Loss: 0.026232\n",
      "Epoch 386 of 500, Train Loss: 0.026232\n",
      "Epoch 387 of 500, Train Loss: 0.026233\n",
      "Epoch 388 of 500, Train Loss: 0.026233\n",
      "Epoch 389 of 500, Train Loss: 0.026232\n",
      "Epoch 390 of 500, Train Loss: 0.026233\n",
      "Epoch 391 of 500, Train Loss: 0.026233\n",
      "Epoch 392 of 500, Train Loss: 0.026234\n",
      "Epoch 393 of 500, Train Loss: 0.026234\n",
      "Epoch 394 of 500, Train Loss: 0.026234\n",
      "Epoch 395 of 500, Train Loss: 0.026234\n",
      "Epoch 396 of 500, Train Loss: 0.026235\n",
      "Epoch 397 of 500, Train Loss: 0.026235\n",
      "Epoch 398 of 500, Train Loss: 0.026235\n",
      "Epoch 399 of 500, Train Loss: 0.026235\n",
      "Epoch 400 of 500, Train Loss: 0.026236\n",
      "Epoch 401 of 500, Train Loss: 0.026236\n",
      "Epoch 402 of 500, Train Loss: 0.026237\n",
      "Epoch 403 of 500, Train Loss: 0.026237\n",
      "Epoch 404 of 500, Train Loss: 0.026237\n",
      "Epoch 405 of 500, Train Loss: 0.026237\n",
      "Epoch 406 of 500, Train Loss: 0.026237\n",
      "Epoch 407 of 500, Train Loss: 0.026237\n",
      "Epoch 408 of 500, Train Loss: 0.026237\n",
      "Epoch 409 of 500, Train Loss: 0.026237\n",
      "Epoch 410 of 500, Train Loss: 0.026238\n",
      "Epoch 411 of 500, Train Loss: 0.026238\n",
      "Epoch 412 of 500, Train Loss: 0.026238\n",
      "Epoch 413 of 500, Train Loss: 0.026239\n",
      "Epoch 414 of 500, Train Loss: 0.026239\n",
      "Epoch 415 of 500, Train Loss: 0.026239\n",
      "Epoch 416 of 500, Train Loss: 0.026240\n",
      "Epoch 417 of 500, Train Loss: 0.026240\n",
      "Epoch 418 of 500, Train Loss: 0.026240\n",
      "Epoch 419 of 500, Train Loss: 0.026240\n",
      "Epoch 420 of 500, Train Loss: 0.026240\n",
      "Epoch 421 of 500, Train Loss: 0.026240\n",
      "Epoch 422 of 500, Train Loss: 0.026241\n",
      "Epoch 423 of 500, Train Loss: 0.026241\n",
      "Epoch 424 of 500, Train Loss: 0.026241\n",
      "Epoch 425 of 500, Train Loss: 0.026242\n",
      "Epoch 426 of 500, Train Loss: 0.026242\n",
      "Epoch 427 of 500, Train Loss: 0.026241\n",
      "Epoch 428 of 500, Train Loss: 0.026241\n",
      "Epoch 429 of 500, Train Loss: 0.026242\n",
      "Epoch 430 of 500, Train Loss: 0.026242\n",
      "Epoch 431 of 500, Train Loss: 0.026242\n",
      "Epoch 432 of 500, Train Loss: 0.026242\n",
      "Epoch 433 of 500, Train Loss: 0.026242\n",
      "Epoch 434 of 500, Train Loss: 0.026243\n",
      "Epoch 435 of 500, Train Loss: 0.026243\n",
      "Epoch 436 of 500, Train Loss: 0.026243\n",
      "Epoch 437 of 500, Train Loss: 0.026245\n",
      "Epoch 438 of 500, Train Loss: 0.026244\n",
      "Epoch 439 of 500, Train Loss: 0.026244\n",
      "Epoch 440 of 500, Train Loss: 0.026244\n",
      "Epoch 441 of 500, Train Loss: 0.026244\n",
      "Epoch 442 of 500, Train Loss: 0.026244\n",
      "Epoch 443 of 500, Train Loss: 0.026244\n",
      "Epoch 444 of 500, Train Loss: 0.026245\n",
      "Epoch 445 of 500, Train Loss: 0.026245\n",
      "Epoch 446 of 500, Train Loss: 0.026244\n",
      "Epoch 447 of 500, Train Loss: 0.026245\n",
      "Epoch 448 of 500, Train Loss: 0.026245\n",
      "Epoch 449 of 500, Train Loss: 0.026245\n",
      "Epoch 450 of 500, Train Loss: 0.026246\n",
      "Epoch 451 of 500, Train Loss: 0.026245\n",
      "Epoch 452 of 500, Train Loss: 0.026246\n",
      "Epoch 453 of 500, Train Loss: 0.026246\n",
      "Epoch 454 of 500, Train Loss: 0.026246\n",
      "Epoch 455 of 500, Train Loss: 0.026246\n",
      "Epoch 456 of 500, Train Loss: 0.026246\n",
      "Epoch 457 of 500, Train Loss: 0.026246\n",
      "Epoch 458 of 500, Train Loss: 0.026246\n",
      "Epoch 459 of 500, Train Loss: 0.026246\n",
      "Epoch 460 of 500, Train Loss: 0.026247\n",
      "Epoch 461 of 500, Train Loss: 0.026246\n",
      "Epoch 462 of 500, Train Loss: 0.026247\n",
      "Epoch 463 of 500, Train Loss: 0.026247\n",
      "Epoch 464 of 500, Train Loss: 0.026247\n",
      "Epoch 465 of 500, Train Loss: 0.026247\n",
      "Epoch 466 of 500, Train Loss: 0.026247\n",
      "Epoch 467 of 500, Train Loss: 0.026247\n",
      "Epoch 468 of 500, Train Loss: 0.026248\n",
      "Epoch 469 of 500, Train Loss: 0.026247\n",
      "Epoch 470 of 500, Train Loss: 0.026248\n",
      "Epoch 471 of 500, Train Loss: 0.026247\n",
      "Epoch 472 of 500, Train Loss: 0.026248\n",
      "Epoch 473 of 500, Train Loss: 0.026248\n",
      "Epoch 474 of 500, Train Loss: 0.026248\n",
      "Epoch 475 of 500, Train Loss: 0.026248\n",
      "Epoch 476 of 500, Train Loss: 0.026248\n",
      "Epoch 477 of 500, Train Loss: 0.026248\n",
      "Epoch 478 of 500, Train Loss: 0.026248\n",
      "Epoch 479 of 500, Train Loss: 0.026248\n",
      "Epoch 480 of 500, Train Loss: 0.026249\n",
      "Epoch 481 of 500, Train Loss: 0.026249\n",
      "Epoch 482 of 500, Train Loss: 0.026249\n",
      "Epoch 483 of 500, Train Loss: 0.026248\n",
      "Epoch 484 of 500, Train Loss: 0.026249\n",
      "Epoch 485 of 500, Train Loss: 0.026249\n",
      "Epoch 486 of 500, Train Loss: 0.026249\n",
      "Epoch 487 of 500, Train Loss: 0.026248\n",
      "Epoch 488 of 500, Train Loss: 0.026248\n",
      "Epoch 489 of 500, Train Loss: 0.026249\n",
      "Epoch 490 of 500, Train Loss: 0.026249\n",
      "Epoch 491 of 500, Train Loss: 0.026250\n",
      "Epoch 492 of 500, Train Loss: 0.026249\n",
      "Epoch 493 of 500, Train Loss: 0.026249\n",
      "Epoch 494 of 500, Train Loss: 0.026249\n",
      "Epoch 495 of 500, Train Loss: 0.026249\n",
      "Epoch 496 of 500, Train Loss: 0.026249\n",
      "Epoch 497 of 500, Train Loss: 0.026249\n",
      "Epoch 498 of 500, Train Loss: 0.026249\n",
      "Epoch 499 of 500, Train Loss: 0.026250\n",
      "Epoch 500 of 500, Train Loss: 0.026249\n",
      "latent train shape:  (16395, 5)\n",
      "M: 5, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 48\n",
      "Training the subspace: 0 / 5\n",
      "Training the subspace: 1 / 5\n",
      "Training the subspace: 2 / 5\n",
      "Training the subspace: 3 / 5\n",
      "Training the subspace: 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/cluster/vq.py:607: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the subspace: 0 / 5\n",
      "Encoding the subspace: 1 / 5\n",
      "Encoding the subspace: 2 / 5\n",
      "Encoding the subspace: 3 / 5\n",
      "Encoding the subspace: 4 / 5\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.747627\n",
      "best loss:  0.74762716150099\n",
      "Epoch 2 of 500, Train Loss: 0.154802\n",
      "best loss:  0.15480197339441426\n",
      "Epoch 3 of 500, Train Loss: 0.059064\n",
      "best loss:  0.05906448495401026\n",
      "Epoch 4 of 500, Train Loss: 0.056303\n",
      "best loss:  0.05630339061355854\n",
      "Epoch 5 of 500, Train Loss: 0.056043\n",
      "best loss:  0.05604284141680168\n",
      "Epoch 6 of 500, Train Loss: 0.055683\n",
      "best loss:  0.0556830455262084\n",
      "Epoch 7 of 500, Train Loss: 0.055146\n",
      "best loss:  0.055145966901638824\n",
      "Epoch 8 of 500, Train Loss: 0.054357\n",
      "best loss:  0.05435706710654073\n",
      "Epoch 9 of 500, Train Loss: 0.053233\n",
      "best loss:  0.05323346218413695\n",
      "Epoch 10 of 500, Train Loss: 0.051699\n",
      "best loss:  0.05169920421154924\n",
      "Epoch 11 of 500, Train Loss: 0.049717\n",
      "best loss:  0.049716895977269804\n",
      "Epoch 12 of 500, Train Loss: 0.047325\n",
      "best loss:  0.04732520410848796\n",
      "Epoch 13 of 500, Train Loss: 0.044662\n",
      "best loss:  0.04466184796115687\n",
      "Epoch 14 of 500, Train Loss: 0.041933\n",
      "best loss:  0.04193292229568095\n",
      "Epoch 15 of 500, Train Loss: 0.039340\n",
      "best loss:  0.03933969252226629\n",
      "Epoch 16 of 500, Train Loss: 0.037007\n",
      "best loss:  0.037007361241659625\n",
      "Epoch 17 of 500, Train Loss: 0.034984\n",
      "best loss:  0.034984103951318\n",
      "Epoch 18 of 500, Train Loss: 0.033269\n",
      "best loss:  0.03326893631231704\n",
      "Epoch 19 of 500, Train Loss: 0.031837\n",
      "best loss:  0.03183684546803666\n",
      "Epoch 20 of 500, Train Loss: 0.030656\n",
      "best loss:  0.03065599130453946\n",
      "Epoch 21 of 500, Train Loss: 0.029690\n",
      "best loss:  0.02969047619217588\n",
      "Epoch 22 of 500, Train Loss: 0.028902\n",
      "best loss:  0.02890211406349775\n",
      "Epoch 23 of 500, Train Loss: 0.028251\n",
      "best loss:  0.02825127955112794\n",
      "Epoch 24 of 500, Train Loss: 0.027701\n",
      "best loss:  0.027700711639558305\n",
      "Epoch 25 of 500, Train Loss: 0.027219\n",
      "best loss:  0.027218775437917387\n",
      "Epoch 26 of 500, Train Loss: 0.026782\n",
      "best loss:  0.02678237518597062\n",
      "Epoch 27 of 500, Train Loss: 0.026375\n",
      "best loss:  0.02637483822244992\n",
      "Epoch 28 of 500, Train Loss: 0.025987\n",
      "best loss:  0.025986877421202325\n",
      "Epoch 29 of 500, Train Loss: 0.025614\n",
      "best loss:  0.025613919857080825\n",
      "Epoch 30 of 500, Train Loss: 0.025254\n",
      "best loss:  0.025254183414107\n",
      "Epoch 31 of 500, Train Loss: 0.024908\n",
      "best loss:  0.024908365467436928\n",
      "Epoch 32 of 500, Train Loss: 0.024578\n",
      "best loss:  0.024577576339483088\n",
      "Epoch 33 of 500, Train Loss: 0.024263\n",
      "best loss:  0.024263119735653954\n",
      "Epoch 34 of 500, Train Loss: 0.023967\n",
      "best loss:  0.023967056342512512\n",
      "Epoch 35 of 500, Train Loss: 0.023690\n",
      "best loss:  0.023690213163224474\n",
      "Epoch 36 of 500, Train Loss: 0.023434\n",
      "best loss:  0.023434008282154452\n",
      "Epoch 37 of 500, Train Loss: 0.023199\n",
      "best loss:  0.023198893017590393\n",
      "Epoch 38 of 500, Train Loss: 0.022985\n",
      "best loss:  0.022985476430130096\n",
      "Epoch 39 of 500, Train Loss: 0.022792\n",
      "best loss:  0.022792473422993308\n",
      "Epoch 40 of 500, Train Loss: 0.022619\n",
      "best loss:  0.022619319978159268\n",
      "Epoch 41 of 500, Train Loss: 0.022465\n",
      "best loss:  0.022464965070803585\n",
      "Epoch 42 of 500, Train Loss: 0.022327\n",
      "best loss:  0.022326806940539026\n",
      "Epoch 43 of 500, Train Loss: 0.022203\n",
      "best loss:  0.022202950398944852\n",
      "Epoch 44 of 500, Train Loss: 0.022092\n",
      "best loss:  0.02209157088943829\n",
      "Epoch 45 of 500, Train Loss: 0.021991\n",
      "best loss:  0.02199073355936431\n",
      "Epoch 46 of 500, Train Loss: 0.021898\n",
      "best loss:  0.021898048037352662\n",
      "Epoch 47 of 500, Train Loss: 0.021812\n",
      "best loss:  0.02181247438939447\n",
      "Epoch 48 of 500, Train Loss: 0.021732\n",
      "best loss:  0.021731954867059956\n",
      "Epoch 49 of 500, Train Loss: 0.021656\n",
      "best loss:  0.021656235793015247\n",
      "Epoch 50 of 500, Train Loss: 0.021584\n",
      "best loss:  0.0215841699788679\n",
      "Epoch 51 of 500, Train Loss: 0.021514\n",
      "best loss:  0.02151443972793776\n",
      "Epoch 52 of 500, Train Loss: 0.021447\n",
      "best loss:  0.021447234249866907\n",
      "Epoch 53 of 500, Train Loss: 0.021382\n",
      "best loss:  0.021381992389639306\n",
      "Epoch 54 of 500, Train Loss: 0.021318\n",
      "best loss:  0.021318421468757773\n",
      "Epoch 55 of 500, Train Loss: 0.021256\n",
      "best loss:  0.021256211250090914\n",
      "Epoch 56 of 500, Train Loss: 0.021195\n",
      "best loss:  0.021195301734400162\n",
      "Epoch 57 of 500, Train Loss: 0.021136\n",
      "best loss:  0.021135561725001983\n",
      "Epoch 58 of 500, Train Loss: 0.021077\n",
      "best loss:  0.0210769193852952\n",
      "Epoch 59 of 500, Train Loss: 0.021020\n",
      "best loss:  0.021019576799193064\n",
      "Epoch 60 of 500, Train Loss: 0.020963\n",
      "best loss:  0.020962923311478594\n",
      "Epoch 61 of 500, Train Loss: 0.020908\n",
      "best loss:  0.02090784911605053\n",
      "Epoch 62 of 500, Train Loss: 0.020853\n",
      "best loss:  0.020853448080940373\n",
      "Epoch 63 of 500, Train Loss: 0.020800\n",
      "best loss:  0.020800017160877888\n",
      "Epoch 64 of 500, Train Loss: 0.020747\n",
      "best loss:  0.020747243870742792\n",
      "Epoch 65 of 500, Train Loss: 0.020695\n",
      "best loss:  0.02069523957735758\n",
      "Epoch 66 of 500, Train Loss: 0.020644\n",
      "best loss:  0.020644254835373035\n",
      "Epoch 67 of 500, Train Loss: 0.020593\n",
      "best loss:  0.02059342172366834\n",
      "Epoch 68 of 500, Train Loss: 0.020543\n",
      "best loss:  0.020543003387346704\n",
      "Epoch 69 of 500, Train Loss: 0.020492\n",
      "best loss:  0.020492086022760172\n",
      "Epoch 70 of 500, Train Loss: 0.020442\n",
      "best loss:  0.020441644963792114\n",
      "Epoch 71 of 500, Train Loss: 0.020392\n",
      "best loss:  0.02039153982574741\n",
      "Epoch 72 of 500, Train Loss: 0.020341\n",
      "best loss:  0.020341139553654432\n",
      "Epoch 73 of 500, Train Loss: 0.020290\n",
      "best loss:  0.02029003713721258\n",
      "Epoch 74 of 500, Train Loss: 0.020239\n",
      "best loss:  0.020239236182984427\n",
      "Epoch 75 of 500, Train Loss: 0.020189\n",
      "best loss:  0.020188692836460926\n",
      "Epoch 76 of 500, Train Loss: 0.020139\n",
      "best loss:  0.02013906564355048\n",
      "Epoch 77 of 500, Train Loss: 0.020089\n",
      "best loss:  0.02008890157363327\n",
      "Epoch 78 of 500, Train Loss: 0.020040\n",
      "best loss:  0.020040194396346505\n",
      "Epoch 79 of 500, Train Loss: 0.019993\n",
      "best loss:  0.019992589665614718\n",
      "Epoch 80 of 500, Train Loss: 0.019947\n",
      "best loss:  0.019946937699704757\n",
      "Epoch 81 of 500, Train Loss: 0.019903\n",
      "best loss:  0.019902623253793262\n",
      "Epoch 82 of 500, Train Loss: 0.019861\n",
      "best loss:  0.019860608725674998\n",
      "Epoch 83 of 500, Train Loss: 0.019821\n",
      "best loss:  0.01982141535999001\n",
      "Epoch 84 of 500, Train Loss: 0.019785\n",
      "best loss:  0.019784526080939946\n",
      "Epoch 85 of 500, Train Loss: 0.019750\n",
      "best loss:  0.019750425303733887\n",
      "Epoch 86 of 500, Train Loss: 0.019719\n",
      "best loss:  0.0197189090120996\n",
      "Epoch 87 of 500, Train Loss: 0.019691\n",
      "best loss:  0.019690567130979164\n",
      "Epoch 88 of 500, Train Loss: 0.019665\n",
      "best loss:  0.01966451004020285\n",
      "Epoch 89 of 500, Train Loss: 0.019641\n",
      "best loss:  0.019640705650203046\n",
      "Epoch 90 of 500, Train Loss: 0.019620\n",
      "best loss:  0.01961950573401504\n",
      "Epoch 91 of 500, Train Loss: 0.019601\n",
      "best loss:  0.01960061779972202\n",
      "Epoch 92 of 500, Train Loss: 0.019584\n",
      "best loss:  0.019583623537517608\n",
      "Epoch 93 of 500, Train Loss: 0.019568\n",
      "best loss:  0.019568340241224974\n",
      "Epoch 94 of 500, Train Loss: 0.019555\n",
      "best loss:  0.019554966278942734\n",
      "Epoch 95 of 500, Train Loss: 0.019543\n",
      "best loss:  0.019542736871720312\n",
      "Epoch 96 of 500, Train Loss: 0.019532\n",
      "best loss:  0.01953212769617247\n",
      "Epoch 97 of 500, Train Loss: 0.019523\n",
      "best loss:  0.01952254938078113\n",
      "Epoch 98 of 500, Train Loss: 0.019514\n",
      "best loss:  0.019513880095558326\n",
      "Epoch 99 of 500, Train Loss: 0.019506\n",
      "best loss:  0.019506262498682134\n",
      "Epoch 100 of 500, Train Loss: 0.019499\n",
      "best loss:  0.019499066940512393\n",
      "Epoch 101 of 500, Train Loss: 0.019493\n",
      "best loss:  0.01949313420115595\n",
      "Epoch 102 of 500, Train Loss: 0.019488\n",
      "best loss:  0.01948767703012094\n",
      "Epoch 103 of 500, Train Loss: 0.019482\n",
      "best loss:  0.019481935944451495\n",
      "Epoch 104 of 500, Train Loss: 0.019478\n",
      "best loss:  0.019477616117043465\n",
      "Epoch 105 of 500, Train Loss: 0.019474\n",
      "best loss:  0.01947356198369273\n",
      "Epoch 106 of 500, Train Loss: 0.019469\n",
      "best loss:  0.019468658076023482\n",
      "Epoch 107 of 500, Train Loss: 0.019465\n",
      "best loss:  0.01946495049393938\n",
      "Epoch 108 of 500, Train Loss: 0.019461\n",
      "best loss:  0.019460917237961967\n",
      "Epoch 109 of 500, Train Loss: 0.019457\n",
      "best loss:  0.01945675045293359\n",
      "Epoch 110 of 500, Train Loss: 0.019453\n",
      "best loss:  0.019452863175227025\n",
      "Epoch 111 of 500, Train Loss: 0.019449\n",
      "best loss:  0.01944876570063246\n",
      "Epoch 112 of 500, Train Loss: 0.019445\n",
      "best loss:  0.019444699085122624\n",
      "Epoch 113 of 500, Train Loss: 0.019442\n",
      "best loss:  0.01944150739798842\n",
      "Epoch 114 of 500, Train Loss: 0.019437\n",
      "best loss:  0.019437189439317038\n",
      "Epoch 115 of 500, Train Loss: 0.019433\n",
      "best loss:  0.019432787689815326\n",
      "Epoch 116 of 500, Train Loss: 0.019429\n",
      "best loss:  0.019429109081609695\n",
      "Epoch 117 of 500, Train Loss: 0.019424\n",
      "best loss:  0.019424417182485495\n",
      "Epoch 118 of 500, Train Loss: 0.019420\n",
      "best loss:  0.01941978194603747\n",
      "Epoch 119 of 500, Train Loss: 0.019415\n",
      "best loss:  0.019414588112100714\n",
      "Epoch 120 of 500, Train Loss: 0.019410\n",
      "best loss:  0.019410036600830193\n",
      "Epoch 121 of 500, Train Loss: 0.019405\n",
      "best loss:  0.01940532341507415\n",
      "Epoch 122 of 500, Train Loss: 0.019400\n",
      "best loss:  0.01940011445233461\n",
      "Epoch 123 of 500, Train Loss: 0.019396\n",
      "best loss:  0.019395733586072113\n",
      "Epoch 124 of 500, Train Loss: 0.019390\n",
      "best loss:  0.019390364181134823\n",
      "Epoch 125 of 500, Train Loss: 0.019386\n",
      "best loss:  0.019385563942663327\n",
      "Epoch 126 of 500, Train Loss: 0.019380\n",
      "best loss:  0.01938031876618548\n",
      "Epoch 127 of 500, Train Loss: 0.019375\n",
      "best loss:  0.019374771718912245\n",
      "Epoch 128 of 500, Train Loss: 0.019370\n",
      "best loss:  0.019369840720089068\n",
      "Epoch 129 of 500, Train Loss: 0.019364\n",
      "best loss:  0.019364253919690762\n",
      "Epoch 130 of 500, Train Loss: 0.019359\n",
      "best loss:  0.019359125009631422\n",
      "Epoch 131 of 500, Train Loss: 0.019354\n",
      "best loss:  0.019353934637696822\n",
      "Epoch 132 of 500, Train Loss: 0.019349\n",
      "best loss:  0.019348563721165234\n",
      "Epoch 133 of 500, Train Loss: 0.019344\n",
      "best loss:  0.019343833610742142\n",
      "Epoch 134 of 500, Train Loss: 0.019339\n",
      "best loss:  0.01933906627268451\n",
      "Epoch 135 of 500, Train Loss: 0.019335\n",
      "best loss:  0.019334937621570417\n",
      "Epoch 136 of 500, Train Loss: 0.019330\n",
      "best loss:  0.019330147783093284\n",
      "Epoch 137 of 500, Train Loss: 0.019326\n",
      "best loss:  0.01932552207848815\n",
      "Epoch 138 of 500, Train Loss: 0.019322\n",
      "best loss:  0.01932173412380733\n",
      "Epoch 139 of 500, Train Loss: 0.019318\n",
      "best loss:  0.019317556355579923\n",
      "Epoch 140 of 500, Train Loss: 0.019314\n",
      "best loss:  0.01931364905556606\n",
      "Epoch 141 of 500, Train Loss: 0.019310\n",
      "best loss:  0.019309935930217507\n",
      "Epoch 142 of 500, Train Loss: 0.019306\n",
      "best loss:  0.019306233122289657\n",
      "Epoch 143 of 500, Train Loss: 0.019303\n",
      "best loss:  0.019302991891862797\n",
      "Epoch 144 of 500, Train Loss: 0.019300\n",
      "best loss:  0.019299659794765025\n",
      "Epoch 145 of 500, Train Loss: 0.019296\n",
      "best loss:  0.019296417497197713\n",
      "Epoch 146 of 500, Train Loss: 0.019294\n",
      "best loss:  0.019293721281569594\n",
      "Epoch 147 of 500, Train Loss: 0.019291\n",
      "best loss:  0.019291196716723732\n",
      "Epoch 148 of 500, Train Loss: 0.019288\n",
      "best loss:  0.019288063718478896\n",
      "Epoch 149 of 500, Train Loss: 0.019285\n",
      "best loss:  0.01928490567086048\n",
      "Epoch 150 of 500, Train Loss: 0.019282\n",
      "best loss:  0.019282410541267763\n",
      "Epoch 151 of 500, Train Loss: 0.019280\n",
      "best loss:  0.019279940711476935\n",
      "Epoch 152 of 500, Train Loss: 0.019278\n",
      "best loss:  0.019277809694943616\n",
      "Epoch 153 of 500, Train Loss: 0.019276\n",
      "best loss:  0.019275675090742767\n",
      "Epoch 154 of 500, Train Loss: 0.019274\n",
      "best loss:  0.019273655379214504\n",
      "Epoch 155 of 500, Train Loss: 0.019272\n",
      "best loss:  0.019271646187028868\n",
      "Epoch 156 of 500, Train Loss: 0.019269\n",
      "best loss:  0.019269360085658715\n",
      "Epoch 157 of 500, Train Loss: 0.019268\n",
      "best loss:  0.019268118494592767\n",
      "Epoch 158 of 500, Train Loss: 0.019266\n",
      "best loss:  0.019266200255938306\n",
      "Epoch 159 of 500, Train Loss: 0.019265\n",
      "best loss:  0.019264997613017063\n",
      "Epoch 160 of 500, Train Loss: 0.019264\n",
      "best loss:  0.01926397824120282\n",
      "Epoch 161 of 500, Train Loss: 0.019263\n",
      "best loss:  0.019262545722691964\n",
      "Epoch 162 of 500, Train Loss: 0.019262\n",
      "best loss:  0.01926187973094079\n",
      "Epoch 163 of 500, Train Loss: 0.019261\n",
      "best loss:  0.019260973878278822\n",
      "Epoch 164 of 500, Train Loss: 0.019260\n",
      "best loss:  0.019260187431413667\n",
      "Epoch 165 of 500, Train Loss: 0.019259\n",
      "best loss:  0.019259446605141166\n",
      "Epoch 166 of 500, Train Loss: 0.019259\n",
      "best loss:  0.019258838192949636\n",
      "Epoch 167 of 500, Train Loss: 0.019258\n",
      "best loss:  0.01925833441669937\n",
      "Epoch 168 of 500, Train Loss: 0.019258\n",
      "best loss:  0.019258149033646973\n",
      "Epoch 169 of 500, Train Loss: 0.019258\n",
      "best loss:  0.019257968126267355\n",
      "Epoch 170 of 500, Train Loss: 0.019259\n",
      "Epoch 171 of 500, Train Loss: 0.019259\n",
      "Epoch 172 of 500, Train Loss: 0.019259\n",
      "Epoch 173 of 500, Train Loss: 0.019259\n",
      "Epoch 174 of 500, Train Loss: 0.019259\n",
      "Epoch 175 of 500, Train Loss: 0.019260\n",
      "Epoch 176 of 500, Train Loss: 0.019260\n",
      "Epoch 177 of 500, Train Loss: 0.019261\n",
      "Epoch 178 of 500, Train Loss: 0.019261\n",
      "Epoch 179 of 500, Train Loss: 0.019261\n",
      "Epoch 180 of 500, Train Loss: 0.019260\n",
      "Epoch 181 of 500, Train Loss: 0.019260\n",
      "Epoch 182 of 500, Train Loss: 0.019259\n",
      "Epoch 183 of 500, Train Loss: 0.019260\n",
      "Epoch 184 of 500, Train Loss: 0.019259\n",
      "Epoch 185 of 500, Train Loss: 0.019259\n",
      "Epoch 186 of 500, Train Loss: 0.019259\n",
      "Epoch 187 of 500, Train Loss: 0.019259\n",
      "Epoch 188 of 500, Train Loss: 0.019259\n",
      "Epoch 189 of 500, Train Loss: 0.019258\n",
      "Epoch 190 of 500, Train Loss: 0.019257\n",
      "best loss:  0.019257459175401156\n",
      "Epoch 191 of 500, Train Loss: 0.019256\n",
      "best loss:  0.019256362675503257\n",
      "Epoch 192 of 500, Train Loss: 0.019255\n",
      "best loss:  0.019255385309992025\n",
      "Epoch 193 of 500, Train Loss: 0.019256\n",
      "Epoch 194 of 500, Train Loss: 0.019255\n",
      "Epoch 195 of 500, Train Loss: 0.019254\n",
      "best loss:  0.01925400711321669\n",
      "Epoch 196 of 500, Train Loss: 0.019253\n",
      "best loss:  0.01925297064978319\n",
      "Epoch 197 of 500, Train Loss: 0.019252\n",
      "best loss:  0.019252480111263563\n",
      "Epoch 198 of 500, Train Loss: 0.019251\n",
      "best loss:  0.01925097969735164\n",
      "Epoch 199 of 500, Train Loss: 0.019249\n",
      "best loss:  0.019249275839994046\n",
      "Epoch 200 of 500, Train Loss: 0.019247\n",
      "best loss:  0.01924736787228441\n",
      "Epoch 201 of 500, Train Loss: 0.019246\n",
      "best loss:  0.019246132578926392\n",
      "Epoch 202 of 500, Train Loss: 0.019245\n",
      "best loss:  0.019244795943097782\n",
      "Epoch 203 of 500, Train Loss: 0.019242\n",
      "best loss:  0.019242295052977136\n",
      "Epoch 204 of 500, Train Loss: 0.019240\n",
      "best loss:  0.019240203970283372\n",
      "Epoch 205 of 500, Train Loss: 0.019239\n",
      "best loss:  0.019238966271459927\n",
      "Epoch 206 of 500, Train Loss: 0.019237\n",
      "best loss:  0.019237172909671182\n",
      "Epoch 207 of 500, Train Loss: 0.019235\n",
      "best loss:  0.01923504482179541\n",
      "Epoch 208 of 500, Train Loss: 0.019233\n",
      "best loss:  0.019233295448528705\n",
      "Epoch 209 of 500, Train Loss: 0.019232\n",
      "best loss:  0.01923165760565253\n",
      "Epoch 210 of 500, Train Loss: 0.019230\n",
      "best loss:  0.01922975069921745\n",
      "Epoch 211 of 500, Train Loss: 0.019227\n",
      "best loss:  0.019227173747476768\n",
      "Epoch 212 of 500, Train Loss: 0.019225\n",
      "best loss:  0.019224905990544244\n",
      "Epoch 213 of 500, Train Loss: 0.019223\n",
      "best loss:  0.019223376078884906\n",
      "Epoch 214 of 500, Train Loss: 0.019222\n",
      "best loss:  0.019221889574122217\n",
      "Epoch 215 of 500, Train Loss: 0.019220\n",
      "best loss:  0.01922047881467282\n",
      "Epoch 216 of 500, Train Loss: 0.019219\n",
      "best loss:  0.019219358142126036\n",
      "Epoch 217 of 500, Train Loss: 0.019218\n",
      "best loss:  0.019218016835922273\n",
      "Epoch 218 of 500, Train Loss: 0.019215\n",
      "best loss:  0.019215269598572032\n",
      "Epoch 219 of 500, Train Loss: 0.019213\n",
      "best loss:  0.0192128103770146\n",
      "Epoch 220 of 500, Train Loss: 0.019211\n",
      "best loss:  0.01921109202572383\n",
      "Epoch 221 of 500, Train Loss: 0.019210\n",
      "best loss:  0.01920958793886338\n",
      "Epoch 222 of 500, Train Loss: 0.019208\n",
      "best loss:  0.019207638077205047\n",
      "Epoch 223 of 500, Train Loss: 0.019206\n",
      "best loss:  0.019206053351978013\n",
      "Epoch 224 of 500, Train Loss: 0.019206\n",
      "best loss:  0.01920570991752857\n",
      "Epoch 225 of 500, Train Loss: 0.019205\n",
      "best loss:  0.01920489833961862\n",
      "Epoch 226 of 500, Train Loss: 0.019203\n",
      "best loss:  0.01920313872338287\n",
      "Epoch 227 of 500, Train Loss: 0.019201\n",
      "best loss:  0.01920127465462767\n",
      "Epoch 228 of 500, Train Loss: 0.019200\n",
      "best loss:  0.019199831038984188\n",
      "Epoch 229 of 500, Train Loss: 0.019199\n",
      "best loss:  0.019199257128142803\n",
      "Epoch 230 of 500, Train Loss: 0.019199\n",
      "best loss:  0.01919859320614776\n",
      "Epoch 231 of 500, Train Loss: 0.019199\n",
      "best loss:  0.019198542629781924\n",
      "Epoch 232 of 500, Train Loss: 0.019197\n",
      "best loss:  0.019197251418264324\n",
      "Epoch 233 of 500, Train Loss: 0.019196\n",
      "best loss:  0.019195670797354215\n",
      "Epoch 234 of 500, Train Loss: 0.019195\n",
      "best loss:  0.019195234664271824\n",
      "Epoch 235 of 500, Train Loss: 0.019194\n",
      "best loss:  0.019194488962994916\n",
      "Epoch 236 of 500, Train Loss: 0.019194\n",
      "best loss:  0.01919363102448794\n",
      "Epoch 237 of 500, Train Loss: 0.019193\n",
      "best loss:  0.019192593890989502\n",
      "Epoch 238 of 500, Train Loss: 0.019192\n",
      "best loss:  0.019192020091148773\n",
      "Epoch 239 of 500, Train Loss: 0.019191\n",
      "best loss:  0.01919078095247536\n",
      "Epoch 240 of 500, Train Loss: 0.019191\n",
      "best loss:  0.01919070467314062\n",
      "Epoch 241 of 500, Train Loss: 0.019190\n",
      "best loss:  0.019190447926593496\n",
      "Epoch 242 of 500, Train Loss: 0.019191\n",
      "Epoch 243 of 500, Train Loss: 0.019190\n",
      "best loss:  0.01919024187937025\n",
      "Epoch 244 of 500, Train Loss: 0.019191\n",
      "Epoch 245 of 500, Train Loss: 0.019190\n",
      "best loss:  0.019190218872228246\n",
      "Epoch 246 of 500, Train Loss: 0.019189\n",
      "best loss:  0.019189106744701447\n",
      "Epoch 247 of 500, Train Loss: 0.019189\n",
      "best loss:  0.019188584162413265\n",
      "Epoch 248 of 500, Train Loss: 0.019189\n",
      "best loss:  0.019188506398783174\n",
      "Epoch 249 of 500, Train Loss: 0.019188\n",
      "best loss:  0.019187749354683165\n",
      "Epoch 250 of 500, Train Loss: 0.019187\n",
      "best loss:  0.019187039565728155\n",
      "Epoch 251 of 500, Train Loss: 0.019187\n",
      "Epoch 252 of 500, Train Loss: 0.019187\n",
      "best loss:  0.019186787399988577\n",
      "Epoch 253 of 500, Train Loss: 0.019187\n",
      "Epoch 254 of 500, Train Loss: 0.019188\n",
      "Epoch 255 of 500, Train Loss: 0.019187\n",
      "Epoch 256 of 500, Train Loss: 0.019188\n",
      "Epoch 257 of 500, Train Loss: 0.019188\n",
      "Epoch 258 of 500, Train Loss: 0.019189\n",
      "Epoch 259 of 500, Train Loss: 0.019188\n",
      "Epoch 260 of 500, Train Loss: 0.019190\n",
      "Epoch 261 of 500, Train Loss: 0.019189\n",
      "Epoch 262 of 500, Train Loss: 0.019190\n",
      "Epoch 263 of 500, Train Loss: 0.019191\n",
      "Epoch 264 of 500, Train Loss: 0.019191\n",
      "Epoch 265 of 500, Train Loss: 0.019193\n",
      "Epoch 266 of 500, Train Loss: 0.019193\n",
      "Epoch 267 of 500, Train Loss: 0.019194\n",
      "Epoch 268 of 500, Train Loss: 0.019195\n",
      "Epoch 269 of 500, Train Loss: 0.019196\n",
      "Epoch 270 of 500, Train Loss: 0.019197\n",
      "Epoch 271 of 500, Train Loss: 0.019197\n",
      "Epoch 272 of 500, Train Loss: 0.019198\n",
      "Epoch 273 of 500, Train Loss: 0.019200\n",
      "Epoch 274 of 500, Train Loss: 0.019200\n",
      "Epoch 275 of 500, Train Loss: 0.019200\n",
      "Epoch 276 of 500, Train Loss: 0.019202\n",
      "Epoch 277 of 500, Train Loss: 0.019203\n",
      "Epoch 278 of 500, Train Loss: 0.019203\n",
      "Epoch 279 of 500, Train Loss: 0.019204\n",
      "Epoch 280 of 500, Train Loss: 0.019204\n",
      "Epoch 281 of 500, Train Loss: 0.019206\n",
      "Epoch 282 of 500, Train Loss: 0.019206\n",
      "Epoch 283 of 500, Train Loss: 0.019205\n",
      "Epoch 284 of 500, Train Loss: 0.019206\n",
      "Epoch 285 of 500, Train Loss: 0.019207\n",
      "Epoch 286 of 500, Train Loss: 0.019207\n",
      "Epoch 287 of 500, Train Loss: 0.019207\n",
      "Epoch 288 of 500, Train Loss: 0.019208\n",
      "Epoch 289 of 500, Train Loss: 0.019208\n",
      "Epoch 290 of 500, Train Loss: 0.019208\n",
      "Epoch 291 of 500, Train Loss: 0.019208\n",
      "Epoch 292 of 500, Train Loss: 0.019209\n",
      "Epoch 293 of 500, Train Loss: 0.019208\n",
      "Epoch 294 of 500, Train Loss: 0.019209\n",
      "Epoch 295 of 500, Train Loss: 0.019209\n",
      "Epoch 296 of 500, Train Loss: 0.019210\n",
      "Epoch 297 of 500, Train Loss: 0.019209\n",
      "Epoch 298 of 500, Train Loss: 0.019210\n",
      "Epoch 299 of 500, Train Loss: 0.019210\n",
      "Epoch 300 of 500, Train Loss: 0.019210\n",
      "Epoch 301 of 500, Train Loss: 0.019211\n",
      "Epoch 302 of 500, Train Loss: 0.019210\n",
      "Epoch 303 of 500, Train Loss: 0.019211\n",
      "Epoch 304 of 500, Train Loss: 0.019210\n",
      "Epoch 305 of 500, Train Loss: 0.019210\n",
      "Epoch 306 of 500, Train Loss: 0.019211\n",
      "Epoch 307 of 500, Train Loss: 0.019211\n",
      "Epoch 308 of 500, Train Loss: 0.019210\n",
      "Epoch 309 of 500, Train Loss: 0.019210\n",
      "Epoch 310 of 500, Train Loss: 0.019210\n",
      "Epoch 311 of 500, Train Loss: 0.019211\n",
      "Epoch 312 of 500, Train Loss: 0.019210\n",
      "Epoch 313 of 500, Train Loss: 0.019211\n",
      "Epoch 314 of 500, Train Loss: 0.019210\n",
      "Epoch 315 of 500, Train Loss: 0.019211\n",
      "Epoch 316 of 500, Train Loss: 0.019211\n",
      "Epoch 317 of 500, Train Loss: 0.019212\n",
      "Epoch 318 of 500, Train Loss: 0.019211\n",
      "Epoch 319 of 500, Train Loss: 0.019211\n",
      "Epoch 320 of 500, Train Loss: 0.019211\n",
      "Epoch 321 of 500, Train Loss: 0.019211\n",
      "Epoch 322 of 500, Train Loss: 0.019211\n",
      "Epoch 323 of 500, Train Loss: 0.019211\n",
      "Epoch 324 of 500, Train Loss: 0.019211\n",
      "Epoch 325 of 500, Train Loss: 0.019212\n",
      "Epoch 326 of 500, Train Loss: 0.019211\n",
      "Epoch 327 of 500, Train Loss: 0.019211\n",
      "Epoch 328 of 500, Train Loss: 0.019210\n",
      "Epoch 329 of 500, Train Loss: 0.019211\n",
      "Epoch 330 of 500, Train Loss: 0.019211\n",
      "Epoch 331 of 500, Train Loss: 0.019211\n",
      "Epoch 332 of 500, Train Loss: 0.019210\n",
      "Epoch 333 of 500, Train Loss: 0.019211\n",
      "Epoch 334 of 500, Train Loss: 0.019210\n",
      "Epoch 335 of 500, Train Loss: 0.019210\n",
      "Epoch 336 of 500, Train Loss: 0.019210\n",
      "Epoch 337 of 500, Train Loss: 0.019211\n",
      "Epoch 338 of 500, Train Loss: 0.019210\n",
      "Epoch 339 of 500, Train Loss: 0.019210\n",
      "Epoch 340 of 500, Train Loss: 0.019210\n",
      "Epoch 341 of 500, Train Loss: 0.019211\n",
      "Epoch 342 of 500, Train Loss: 0.019210\n",
      "Epoch 343 of 500, Train Loss: 0.019210\n",
      "Epoch 344 of 500, Train Loss: 0.019210\n",
      "Epoch 345 of 500, Train Loss: 0.019209\n",
      "Epoch 346 of 500, Train Loss: 0.019210\n",
      "Epoch 347 of 500, Train Loss: 0.019211\n",
      "Epoch 348 of 500, Train Loss: 0.019210\n",
      "Epoch 349 of 500, Train Loss: 0.019210\n",
      "Epoch 350 of 500, Train Loss: 0.019210\n",
      "Epoch 351 of 500, Train Loss: 0.019210\n",
      "Epoch 352 of 500, Train Loss: 0.019209\n",
      "Epoch 353 of 500, Train Loss: 0.019209\n",
      "Epoch 354 of 500, Train Loss: 0.019210\n",
      "Epoch 355 of 500, Train Loss: 0.019210\n",
      "Epoch 356 of 500, Train Loss: 0.019209\n",
      "Epoch 357 of 500, Train Loss: 0.019210\n",
      "Epoch 358 of 500, Train Loss: 0.019210\n",
      "Epoch 359 of 500, Train Loss: 0.019209\n",
      "Epoch 360 of 500, Train Loss: 0.019209\n",
      "Epoch 361 of 500, Train Loss: 0.019209\n",
      "Epoch 362 of 500, Train Loss: 0.019209\n",
      "Epoch 363 of 500, Train Loss: 0.019209\n",
      "Epoch 364 of 500, Train Loss: 0.019208\n",
      "Epoch 365 of 500, Train Loss: 0.019209\n",
      "Epoch 366 of 500, Train Loss: 0.019208\n",
      "Epoch 367 of 500, Train Loss: 0.019208\n",
      "Epoch 368 of 500, Train Loss: 0.019207\n",
      "Epoch 369 of 500, Train Loss: 0.019208\n",
      "Epoch 370 of 500, Train Loss: 0.019209\n",
      "Epoch 371 of 500, Train Loss: 0.019209\n",
      "Epoch 372 of 500, Train Loss: 0.019208\n",
      "Epoch 373 of 500, Train Loss: 0.019208\n",
      "Epoch 374 of 500, Train Loss: 0.019208\n",
      "Epoch 375 of 500, Train Loss: 0.019208\n",
      "Epoch 376 of 500, Train Loss: 0.019208\n",
      "Epoch 377 of 500, Train Loss: 0.019207\n",
      "Epoch 378 of 500, Train Loss: 0.019207\n",
      "Epoch 379 of 500, Train Loss: 0.019208\n",
      "Epoch 380 of 500, Train Loss: 0.019207\n",
      "Epoch 381 of 500, Train Loss: 0.019207\n",
      "Epoch 382 of 500, Train Loss: 0.019207\n",
      "Epoch 383 of 500, Train Loss: 0.019207\n",
      "Epoch 384 of 500, Train Loss: 0.019207\n",
      "Epoch 385 of 500, Train Loss: 0.019207\n",
      "Epoch 386 of 500, Train Loss: 0.019208\n",
      "Epoch 387 of 500, Train Loss: 0.019208\n",
      "Epoch 388 of 500, Train Loss: 0.019206\n",
      "Epoch 389 of 500, Train Loss: 0.019207\n",
      "Epoch 390 of 500, Train Loss: 0.019207\n",
      "Epoch 391 of 500, Train Loss: 0.019207\n",
      "Epoch 392 of 500, Train Loss: 0.019206\n",
      "Epoch 393 of 500, Train Loss: 0.019207\n",
      "Epoch 394 of 500, Train Loss: 0.019206\n",
      "Epoch 395 of 500, Train Loss: 0.019207\n",
      "Epoch 396 of 500, Train Loss: 0.019206\n",
      "Epoch 397 of 500, Train Loss: 0.019206\n",
      "Epoch 398 of 500, Train Loss: 0.019205\n",
      "Epoch 399 of 500, Train Loss: 0.019205\n",
      "Epoch 400 of 500, Train Loss: 0.019205\n",
      "Epoch 401 of 500, Train Loss: 0.019206\n",
      "Epoch 402 of 500, Train Loss: 0.019206\n",
      "Epoch 403 of 500, Train Loss: 0.019206\n",
      "Epoch 404 of 500, Train Loss: 0.019205\n",
      "Epoch 405 of 500, Train Loss: 0.019206\n",
      "Epoch 406 of 500, Train Loss: 0.019206\n",
      "Epoch 407 of 500, Train Loss: 0.019207\n",
      "Epoch 408 of 500, Train Loss: 0.019205\n",
      "Epoch 409 of 500, Train Loss: 0.019205\n",
      "Epoch 410 of 500, Train Loss: 0.019205\n",
      "Epoch 411 of 500, Train Loss: 0.019206\n",
      "Epoch 412 of 500, Train Loss: 0.019205\n",
      "Epoch 413 of 500, Train Loss: 0.019205\n",
      "Epoch 414 of 500, Train Loss: 0.019205\n",
      "Epoch 415 of 500, Train Loss: 0.019205\n",
      "Epoch 416 of 500, Train Loss: 0.019204\n",
      "Epoch 417 of 500, Train Loss: 0.019205\n",
      "Epoch 418 of 500, Train Loss: 0.019206\n",
      "Epoch 419 of 500, Train Loss: 0.019205\n",
      "Epoch 420 of 500, Train Loss: 0.019205\n",
      "Epoch 421 of 500, Train Loss: 0.019205\n",
      "Epoch 422 of 500, Train Loss: 0.019204\n",
      "Epoch 423 of 500, Train Loss: 0.019205\n",
      "Epoch 424 of 500, Train Loss: 0.019204\n",
      "Epoch 425 of 500, Train Loss: 0.019205\n",
      "Epoch 426 of 500, Train Loss: 0.019205\n",
      "Epoch 427 of 500, Train Loss: 0.019205\n",
      "Epoch 428 of 500, Train Loss: 0.019205\n",
      "Epoch 429 of 500, Train Loss: 0.019205\n",
      "Epoch 430 of 500, Train Loss: 0.019205\n",
      "Epoch 431 of 500, Train Loss: 0.019204\n",
      "Epoch 432 of 500, Train Loss: 0.019204\n",
      "Epoch 433 of 500, Train Loss: 0.019205\n",
      "Epoch 434 of 500, Train Loss: 0.019204\n",
      "Epoch 435 of 500, Train Loss: 0.019204\n",
      "Epoch 436 of 500, Train Loss: 0.019204\n",
      "Epoch 437 of 500, Train Loss: 0.019204\n",
      "Epoch 438 of 500, Train Loss: 0.019205\n",
      "Epoch 439 of 500, Train Loss: 0.019205\n",
      "Epoch 440 of 500, Train Loss: 0.019204\n",
      "Epoch 441 of 500, Train Loss: 0.019205\n",
      "Epoch 442 of 500, Train Loss: 0.019205\n",
      "Epoch 443 of 500, Train Loss: 0.019205\n",
      "Epoch 444 of 500, Train Loss: 0.019205\n",
      "Epoch 445 of 500, Train Loss: 0.019204\n",
      "Epoch 446 of 500, Train Loss: 0.019204\n",
      "Epoch 447 of 500, Train Loss: 0.019205\n",
      "Epoch 448 of 500, Train Loss: 0.019204\n",
      "Epoch 449 of 500, Train Loss: 0.019204\n",
      "Epoch 450 of 500, Train Loss: 0.019204\n",
      "Epoch 451 of 500, Train Loss: 0.019204\n",
      "Epoch 452 of 500, Train Loss: 0.019204\n",
      "Epoch 453 of 500, Train Loss: 0.019204\n",
      "Epoch 454 of 500, Train Loss: 0.019204\n",
      "Epoch 455 of 500, Train Loss: 0.019204\n",
      "Epoch 456 of 500, Train Loss: 0.019205\n",
      "Epoch 457 of 500, Train Loss: 0.019205\n",
      "Epoch 458 of 500, Train Loss: 0.019204\n",
      "Epoch 459 of 500, Train Loss: 0.019204\n",
      "Epoch 460 of 500, Train Loss: 0.019204\n",
      "Epoch 461 of 500, Train Loss: 0.019204\n",
      "Epoch 462 of 500, Train Loss: 0.019205\n",
      "Epoch 463 of 500, Train Loss: 0.019204\n",
      "Epoch 464 of 500, Train Loss: 0.019204\n",
      "Epoch 465 of 500, Train Loss: 0.019204\n",
      "Epoch 466 of 500, Train Loss: 0.019204\n",
      "Epoch 467 of 500, Train Loss: 0.019205\n",
      "Epoch 468 of 500, Train Loss: 0.019204\n",
      "Epoch 469 of 500, Train Loss: 0.019204\n",
      "Epoch 470 of 500, Train Loss: 0.019204\n",
      "Epoch 471 of 500, Train Loss: 0.019204\n",
      "Epoch 472 of 500, Train Loss: 0.019204\n",
      "Epoch 473 of 500, Train Loss: 0.019204\n",
      "Epoch 474 of 500, Train Loss: 0.019204\n",
      "Epoch 475 of 500, Train Loss: 0.019204\n",
      "Epoch 476 of 500, Train Loss: 0.019204\n",
      "Epoch 477 of 500, Train Loss: 0.019204\n",
      "Epoch 478 of 500, Train Loss: 0.019204\n",
      "Epoch 479 of 500, Train Loss: 0.019205\n",
      "Epoch 480 of 500, Train Loss: 0.019204\n",
      "Epoch 481 of 500, Train Loss: 0.019204\n",
      "Epoch 482 of 500, Train Loss: 0.019204\n",
      "Epoch 483 of 500, Train Loss: 0.019204\n",
      "Epoch 484 of 500, Train Loss: 0.019204\n",
      "Epoch 485 of 500, Train Loss: 0.019204\n",
      "Epoch 486 of 500, Train Loss: 0.019204\n",
      "Epoch 487 of 500, Train Loss: 0.019204\n",
      "Epoch 488 of 500, Train Loss: 0.019204\n",
      "Epoch 489 of 500, Train Loss: 0.019204\n",
      "Epoch 490 of 500, Train Loss: 0.019204\n",
      "Epoch 491 of 500, Train Loss: 0.019204\n",
      "Epoch 492 of 500, Train Loss: 0.019204\n",
      "Epoch 493 of 500, Train Loss: 0.019204\n",
      "Epoch 494 of 500, Train Loss: 0.019204\n",
      "Epoch 495 of 500, Train Loss: 0.019204\n",
      "Epoch 496 of 500, Train Loss: 0.019204\n",
      "Epoch 497 of 500, Train Loss: 0.019204\n",
      "Epoch 498 of 500, Train Loss: 0.019204\n",
      "Epoch 499 of 500, Train Loss: 0.019204\n",
      "Epoch 500 of 500, Train Loss: 0.019204\n",
      "latent train shape:  (16395, 10)\n",
      "M: 10, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 23\n",
      "Training the subspace: 0 / 10\n",
      "Training the subspace: 1 / 10\n",
      "Training the subspace: 2 / 10\n",
      "Training the subspace: 3 / 10\n",
      "Training the subspace: 4 / 10\n",
      "Training the subspace: 5 / 10\n",
      "Training the subspace: 6 / 10\n",
      "Training the subspace: 7 / 10\n",
      "Training the subspace: 8 / 10\n",
      "Training the subspace: 9 / 10\n",
      "Encoding the subspace: 0 / 10\n",
      "Encoding the subspace: 1 / 10\n",
      "Encoding the subspace: 2 / 10\n",
      "Encoding the subspace: 3 / 10\n",
      "Encoding the subspace: 4 / 10\n",
      "Encoding the subspace: 5 / 10\n",
      "Encoding the subspace: 6 / 10\n",
      "Encoding the subspace: 7 / 10\n",
      "Encoding the subspace: 8 / 10\n",
      "Encoding the subspace: 9 / 10\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=20, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.600074\n",
      "best loss:  0.6000737631621287\n",
      "Epoch 2 of 500, Train Loss: 0.078592\n",
      "best loss:  0.0785920194949406\n",
      "Epoch 3 of 500, Train Loss: 0.056359\n",
      "best loss:  0.05635947297379562\n",
      "Epoch 4 of 500, Train Loss: 0.055815\n",
      "best loss:  0.055814551916460745\n",
      "Epoch 5 of 500, Train Loss: 0.054943\n",
      "best loss:  0.054942889245995864\n",
      "Epoch 6 of 500, Train Loss: 0.053507\n",
      "best loss:  0.05350672575562124\n",
      "Epoch 7 of 500, Train Loss: 0.051317\n",
      "best loss:  0.05131700125346032\n",
      "Epoch 8 of 500, Train Loss: 0.048315\n",
      "best loss:  0.04831494850353807\n",
      "Epoch 9 of 500, Train Loss: 0.044732\n",
      "best loss:  0.04473176401906264\n",
      "Epoch 10 of 500, Train Loss: 0.041072\n",
      "best loss:  0.041071741712683914\n",
      "Epoch 11 of 500, Train Loss: 0.037788\n",
      "best loss:  0.037787702809044615\n",
      "Epoch 12 of 500, Train Loss: 0.035025\n",
      "best loss:  0.0350253227547663\n",
      "Epoch 13 of 500, Train Loss: 0.032741\n",
      "best loss:  0.0327405744465068\n",
      "Epoch 14 of 500, Train Loss: 0.030862\n",
      "best loss:  0.030861775503158138\n",
      "Epoch 15 of 500, Train Loss: 0.029337\n",
      "best loss:  0.029337277347601\n",
      "Epoch 16 of 500, Train Loss: 0.028115\n",
      "best loss:  0.02811450905449934\n",
      "Epoch 17 of 500, Train Loss: 0.027127\n",
      "best loss:  0.027126914592795506\n",
      "Epoch 18 of 500, Train Loss: 0.026301\n",
      "best loss:  0.026301227747318297\n",
      "Epoch 19 of 500, Train Loss: 0.025575\n",
      "best loss:  0.025574814594556513\n",
      "Epoch 20 of 500, Train Loss: 0.024906\n",
      "best loss:  0.024906228169214065\n",
      "Epoch 21 of 500, Train Loss: 0.024277\n",
      "best loss:  0.024277036964023384\n",
      "Epoch 22 of 500, Train Loss: 0.023682\n",
      "best loss:  0.02368202966334688\n",
      "Epoch 23 of 500, Train Loss: 0.023124\n",
      "best loss:  0.023123990313886383\n",
      "Epoch 24 of 500, Train Loss: 0.022607\n",
      "best loss:  0.022606919265565167\n",
      "Epoch 25 of 500, Train Loss: 0.022133\n",
      "best loss:  0.022133083943216232\n",
      "Epoch 26 of 500, Train Loss: 0.021703\n",
      "best loss:  0.02170300473505992\n",
      "Epoch 27 of 500, Train Loss: 0.021314\n",
      "best loss:  0.021314405192045294\n",
      "Epoch 28 of 500, Train Loss: 0.020964\n",
      "best loss:  0.02096418695715653\n",
      "Epoch 29 of 500, Train Loss: 0.020649\n",
      "best loss:  0.020648522930609627\n",
      "Epoch 30 of 500, Train Loss: 0.020363\n",
      "best loss:  0.02036269395833279\n",
      "Epoch 31 of 500, Train Loss: 0.020102\n",
      "best loss:  0.020101994518353063\n",
      "Epoch 32 of 500, Train Loss: 0.019863\n",
      "best loss:  0.019862744509145012\n",
      "Epoch 33 of 500, Train Loss: 0.019637\n",
      "best loss:  0.019636927392738042\n",
      "Epoch 34 of 500, Train Loss: 0.019426\n",
      "best loss:  0.01942610093744385\n",
      "Epoch 35 of 500, Train Loss: 0.019223\n",
      "best loss:  0.019222929024646535\n",
      "Epoch 36 of 500, Train Loss: 0.019028\n",
      "best loss:  0.019028096510117238\n",
      "Epoch 37 of 500, Train Loss: 0.018842\n",
      "best loss:  0.01884202334137323\n",
      "Epoch 38 of 500, Train Loss: 0.018663\n",
      "best loss:  0.018662575316027007\n",
      "Epoch 39 of 500, Train Loss: 0.018491\n",
      "best loss:  0.01849068215910107\n",
      "Epoch 40 of 500, Train Loss: 0.018327\n",
      "best loss:  0.018327048812521795\n",
      "Epoch 41 of 500, Train Loss: 0.018172\n",
      "best loss:  0.018171694676135487\n",
      "Epoch 42 of 500, Train Loss: 0.018026\n",
      "best loss:  0.018026228197498332\n",
      "Epoch 43 of 500, Train Loss: 0.017890\n",
      "best loss:  0.017890147090070092\n",
      "Epoch 44 of 500, Train Loss: 0.017765\n",
      "best loss:  0.0177650389091433\n",
      "Epoch 45 of 500, Train Loss: 0.017646\n",
      "best loss:  0.017645510201669087\n",
      "Epoch 46 of 500, Train Loss: 0.017535\n",
      "best loss:  0.017535195409549516\n",
      "Epoch 47 of 500, Train Loss: 0.017430\n",
      "best loss:  0.017430170590568752\n",
      "Epoch 48 of 500, Train Loss: 0.017333\n",
      "best loss:  0.01733276400987651\n",
      "Epoch 49 of 500, Train Loss: 0.017241\n",
      "best loss:  0.01724133952570422\n",
      "Epoch 50 of 500, Train Loss: 0.017153\n",
      "best loss:  0.017153025541387774\n",
      "Epoch 51 of 500, Train Loss: 0.017070\n",
      "best loss:  0.017069509633962976\n",
      "Epoch 52 of 500, Train Loss: 0.016990\n",
      "best loss:  0.016989564252367546\n",
      "Epoch 53 of 500, Train Loss: 0.016911\n",
      "best loss:  0.01691147141463607\n",
      "Epoch 54 of 500, Train Loss: 0.016837\n",
      "best loss:  0.016836707475062735\n",
      "Epoch 55 of 500, Train Loss: 0.016763\n",
      "best loss:  0.01676310280129465\n",
      "Epoch 56 of 500, Train Loss: 0.016692\n",
      "best loss:  0.016691581966750715\n",
      "Epoch 57 of 500, Train Loss: 0.016620\n",
      "best loss:  0.016620249301330513\n",
      "Epoch 58 of 500, Train Loss: 0.016552\n",
      "best loss:  0.01655214958437971\n",
      "Epoch 59 of 500, Train Loss: 0.016486\n",
      "best loss:  0.01648638106010811\n",
      "Epoch 60 of 500, Train Loss: 0.016421\n",
      "best loss:  0.01642098757794827\n",
      "Epoch 61 of 500, Train Loss: 0.016355\n",
      "best loss:  0.016354982039752206\n",
      "Epoch 62 of 500, Train Loss: 0.016294\n",
      "best loss:  0.016294016633426424\n",
      "Epoch 63 of 500, Train Loss: 0.016236\n",
      "best loss:  0.01623550969782437\n",
      "Epoch 64 of 500, Train Loss: 0.016180\n",
      "best loss:  0.016179744331319565\n",
      "Epoch 65 of 500, Train Loss: 0.016126\n",
      "best loss:  0.016125811257827378\n",
      "Epoch 66 of 500, Train Loss: 0.016076\n",
      "best loss:  0.016076304177327673\n",
      "Epoch 67 of 500, Train Loss: 0.016030\n",
      "best loss:  0.016029575130317445\n",
      "Epoch 68 of 500, Train Loss: 0.015988\n",
      "best loss:  0.015987714544992693\n",
      "Epoch 69 of 500, Train Loss: 0.015949\n",
      "best loss:  0.015949211181719\n",
      "Epoch 70 of 500, Train Loss: 0.015913\n",
      "best loss:  0.015913007917629857\n",
      "Epoch 71 of 500, Train Loss: 0.015878\n",
      "best loss:  0.015878255680959685\n",
      "Epoch 72 of 500, Train Loss: 0.015848\n",
      "best loss:  0.015847505563454947\n",
      "Epoch 73 of 500, Train Loss: 0.015818\n",
      "best loss:  0.01581844416841352\n",
      "Epoch 74 of 500, Train Loss: 0.015790\n",
      "best loss:  0.01579046527996782\n",
      "Epoch 75 of 500, Train Loss: 0.015766\n",
      "best loss:  0.015765840731842746\n",
      "Epoch 76 of 500, Train Loss: 0.015742\n",
      "best loss:  0.01574207924788298\n",
      "Epoch 77 of 500, Train Loss: 0.015719\n",
      "best loss:  0.01571921359832478\n",
      "Epoch 78 of 500, Train Loss: 0.015698\n",
      "best loss:  0.015698489581655293\n",
      "Epoch 79 of 500, Train Loss: 0.015679\n",
      "best loss:  0.01567890470673316\n",
      "Epoch 80 of 500, Train Loss: 0.015660\n",
      "best loss:  0.01565957323750914\n",
      "Epoch 81 of 500, Train Loss: 0.015641\n",
      "best loss:  0.015641436739752518\n",
      "Epoch 82 of 500, Train Loss: 0.015624\n",
      "best loss:  0.015623985754295303\n",
      "Epoch 83 of 500, Train Loss: 0.015608\n",
      "best loss:  0.01560768015121377\n",
      "Epoch 84 of 500, Train Loss: 0.015593\n",
      "best loss:  0.015592811008294424\n",
      "Epoch 85 of 500, Train Loss: 0.015578\n",
      "best loss:  0.015578090772754693\n",
      "Epoch 86 of 500, Train Loss: 0.015563\n",
      "best loss:  0.015563166500637741\n",
      "Epoch 87 of 500, Train Loss: 0.015548\n",
      "best loss:  0.015548292519488596\n",
      "Epoch 88 of 500, Train Loss: 0.015534\n",
      "best loss:  0.015534167161279958\n",
      "Epoch 89 of 500, Train Loss: 0.015521\n",
      "best loss:  0.015520896446302735\n",
      "Epoch 90 of 500, Train Loss: 0.015507\n",
      "best loss:  0.015506857412988176\n",
      "Epoch 91 of 500, Train Loss: 0.015494\n",
      "best loss:  0.015494088723293557\n",
      "Epoch 92 of 500, Train Loss: 0.015482\n",
      "best loss:  0.015482210304656666\n",
      "Epoch 93 of 500, Train Loss: 0.015469\n",
      "best loss:  0.015469359913290398\n",
      "Epoch 94 of 500, Train Loss: 0.015458\n",
      "best loss:  0.015458166144441727\n",
      "Epoch 95 of 500, Train Loss: 0.015447\n",
      "best loss:  0.015447127087945985\n",
      "Epoch 96 of 500, Train Loss: 0.015436\n",
      "best loss:  0.01543561094972962\n",
      "Epoch 97 of 500, Train Loss: 0.015426\n",
      "best loss:  0.015425670072580723\n",
      "Epoch 98 of 500, Train Loss: 0.015418\n",
      "best loss:  0.015417539575967447\n",
      "Epoch 99 of 500, Train Loss: 0.015409\n",
      "best loss:  0.015409121943444605\n",
      "Epoch 100 of 500, Train Loss: 0.015403\n",
      "best loss:  0.015402601002589186\n",
      "Epoch 101 of 500, Train Loss: 0.015396\n",
      "best loss:  0.015395706390886199\n",
      "Epoch 102 of 500, Train Loss: 0.015387\n",
      "best loss:  0.015387437198276798\n",
      "Epoch 103 of 500, Train Loss: 0.015380\n",
      "best loss:  0.015380023565556667\n",
      "Epoch 104 of 500, Train Loss: 0.015375\n",
      "best loss:  0.015374693892537003\n",
      "Epoch 105 of 500, Train Loss: 0.015370\n",
      "best loss:  0.015369841643715595\n",
      "Epoch 106 of 500, Train Loss: 0.015364\n",
      "best loss:  0.01536441832494763\n",
      "Epoch 107 of 500, Train Loss: 0.015361\n",
      "best loss:  0.015361034254500167\n",
      "Epoch 108 of 500, Train Loss: 0.015360\n",
      "best loss:  0.015359786525910158\n",
      "Epoch 109 of 500, Train Loss: 0.015357\n",
      "best loss:  0.015356807169042412\n",
      "Epoch 110 of 500, Train Loss: 0.015352\n",
      "best loss:  0.01535216083455932\n",
      "Epoch 111 of 500, Train Loss: 0.015348\n",
      "best loss:  0.015347836945926077\n",
      "Epoch 112 of 500, Train Loss: 0.015342\n",
      "best loss:  0.015342336358067529\n",
      "Epoch 113 of 500, Train Loss: 0.015339\n",
      "best loss:  0.015338764343881806\n",
      "Epoch 114 of 500, Train Loss: 0.015336\n",
      "best loss:  0.015335983363460287\n",
      "Epoch 115 of 500, Train Loss: 0.015336\n",
      "best loss:  0.015335766918498792\n",
      "Epoch 116 of 500, Train Loss: 0.015334\n",
      "best loss:  0.015333551447612934\n",
      "Epoch 117 of 500, Train Loss: 0.015335\n",
      "Epoch 118 of 500, Train Loss: 0.015333\n",
      "best loss:  0.015333452746371356\n",
      "Epoch 119 of 500, Train Loss: 0.015334\n",
      "Epoch 120 of 500, Train Loss: 0.015332\n",
      "best loss:  0.01533154355291667\n",
      "Epoch 121 of 500, Train Loss: 0.015330\n",
      "best loss:  0.015329870831694223\n",
      "Epoch 122 of 500, Train Loss: 0.015326\n",
      "best loss:  0.015326328131834103\n",
      "Epoch 123 of 500, Train Loss: 0.015324\n",
      "best loss:  0.015324499169517987\n",
      "Epoch 124 of 500, Train Loss: 0.015322\n",
      "best loss:  0.015321795188356191\n",
      "Epoch 125 of 500, Train Loss: 0.015320\n",
      "best loss:  0.015320469075254374\n",
      "Epoch 126 of 500, Train Loss: 0.015318\n",
      "best loss:  0.015317781126693382\n",
      "Epoch 127 of 500, Train Loss: 0.015316\n",
      "best loss:  0.01531613006819646\n",
      "Epoch 128 of 500, Train Loss: 0.015312\n",
      "best loss:  0.015311930979721161\n",
      "Epoch 129 of 500, Train Loss: 0.015310\n",
      "best loss:  0.015310051981152158\n",
      "Epoch 130 of 500, Train Loss: 0.015306\n",
      "best loss:  0.015306271493521541\n",
      "Epoch 131 of 500, Train Loss: 0.015305\n",
      "best loss:  0.015304586771594058\n",
      "Epoch 132 of 500, Train Loss: 0.015300\n",
      "best loss:  0.015300464500084722\n",
      "Epoch 133 of 500, Train Loss: 0.015298\n",
      "best loss:  0.015297986181902288\n",
      "Epoch 134 of 500, Train Loss: 0.015294\n",
      "best loss:  0.015294449417644303\n",
      "Epoch 135 of 500, Train Loss: 0.015292\n",
      "best loss:  0.01529177342669663\n",
      "Epoch 136 of 500, Train Loss: 0.015289\n",
      "best loss:  0.015288786651510674\n",
      "Epoch 137 of 500, Train Loss: 0.015287\n",
      "best loss:  0.015286872647576352\n",
      "Epoch 138 of 500, Train Loss: 0.015285\n",
      "best loss:  0.015284536870254217\n",
      "Epoch 139 of 500, Train Loss: 0.015283\n",
      "best loss:  0.015283005324944983\n",
      "Epoch 140 of 500, Train Loss: 0.015281\n",
      "best loss:  0.015280880187611031\n",
      "Epoch 141 of 500, Train Loss: 0.015280\n",
      "best loss:  0.01527969082303042\n",
      "Epoch 142 of 500, Train Loss: 0.015278\n",
      "best loss:  0.015277736148185119\n",
      "Epoch 143 of 500, Train Loss: 0.015277\n",
      "best loss:  0.015276852999218242\n",
      "Epoch 144 of 500, Train Loss: 0.015275\n",
      "best loss:  0.015275159279243703\n",
      "Epoch 145 of 500, Train Loss: 0.015274\n",
      "best loss:  0.015274044001283658\n",
      "Epoch 146 of 500, Train Loss: 0.015272\n",
      "best loss:  0.015272151882527396\n",
      "Epoch 147 of 500, Train Loss: 0.015271\n",
      "best loss:  0.015271064190712013\n",
      "Epoch 148 of 500, Train Loss: 0.015269\n",
      "best loss:  0.015268877170061288\n",
      "Epoch 149 of 500, Train Loss: 0.015268\n",
      "best loss:  0.015267883898742684\n",
      "Epoch 150 of 500, Train Loss: 0.015266\n",
      "best loss:  0.015266085417612723\n",
      "Epoch 151 of 500, Train Loss: 0.015265\n",
      "best loss:  0.015265026000732656\n",
      "Epoch 152 of 500, Train Loss: 0.015263\n",
      "best loss:  0.015263376873466655\n",
      "Epoch 153 of 500, Train Loss: 0.015261\n",
      "best loss:  0.015261496708262712\n",
      "Epoch 154 of 500, Train Loss: 0.015258\n",
      "best loss:  0.015258403792083913\n",
      "Epoch 155 of 500, Train Loss: 0.015255\n",
      "best loss:  0.015255424750846082\n",
      "Epoch 156 of 500, Train Loss: 0.015252\n",
      "best loss:  0.015251932068061906\n",
      "Epoch 157 of 500, Train Loss: 0.015248\n",
      "best loss:  0.01524824675153616\n",
      "Epoch 158 of 500, Train Loss: 0.015245\n",
      "best loss:  0.015244739031096083\n",
      "Epoch 159 of 500, Train Loss: 0.015241\n",
      "best loss:  0.015240956126042648\n",
      "Epoch 160 of 500, Train Loss: 0.015237\n",
      "best loss:  0.0152365605853581\n",
      "Epoch 161 of 500, Train Loss: 0.015233\n",
      "best loss:  0.015233443836977547\n",
      "Epoch 162 of 500, Train Loss: 0.015231\n",
      "best loss:  0.015230736742382725\n",
      "Epoch 163 of 500, Train Loss: 0.015229\n",
      "best loss:  0.015229003229575563\n",
      "Epoch 164 of 500, Train Loss: 0.015227\n",
      "best loss:  0.015227270954245226\n",
      "Epoch 165 of 500, Train Loss: 0.015226\n",
      "best loss:  0.015225998528009323\n",
      "Epoch 166 of 500, Train Loss: 0.015224\n",
      "best loss:  0.015223782013220957\n",
      "Epoch 167 of 500, Train Loss: 0.015222\n",
      "best loss:  0.015221935626960453\n",
      "Epoch 168 of 500, Train Loss: 0.015219\n",
      "best loss:  0.01521874871578739\n",
      "Epoch 169 of 500, Train Loss: 0.015216\n",
      "best loss:  0.015215722307952683\n",
      "Epoch 170 of 500, Train Loss: 0.015214\n",
      "best loss:  0.015213586610214885\n",
      "Epoch 171 of 500, Train Loss: 0.015212\n",
      "best loss:  0.0152123840559588\n",
      "Epoch 172 of 500, Train Loss: 0.015211\n",
      "best loss:  0.015211389169490412\n",
      "Epoch 173 of 500, Train Loss: 0.015210\n",
      "best loss:  0.015210307705217915\n",
      "Epoch 174 of 500, Train Loss: 0.015209\n",
      "best loss:  0.01520851177823109\n",
      "Epoch 175 of 500, Train Loss: 0.015207\n",
      "best loss:  0.015207104740687826\n",
      "Epoch 176 of 500, Train Loss: 0.015206\n",
      "best loss:  0.01520555352254287\n",
      "Epoch 177 of 500, Train Loss: 0.015203\n",
      "best loss:  0.015203386141337425\n",
      "Epoch 178 of 500, Train Loss: 0.015202\n",
      "best loss:  0.015201716965367628\n",
      "Epoch 179 of 500, Train Loss: 0.015199\n",
      "best loss:  0.01519867083077615\n",
      "Epoch 180 of 500, Train Loss: 0.015196\n",
      "best loss:  0.015196363732684404\n",
      "Epoch 181 of 500, Train Loss: 0.015194\n",
      "best loss:  0.01519393950002268\n",
      "Epoch 182 of 500, Train Loss: 0.015192\n",
      "best loss:  0.015192085420424047\n",
      "Epoch 183 of 500, Train Loss: 0.015191\n",
      "best loss:  0.015191068640204378\n",
      "Epoch 184 of 500, Train Loss: 0.015189\n",
      "best loss:  0.01518886949724424\n",
      "Epoch 185 of 500, Train Loss: 0.015190\n",
      "Epoch 186 of 500, Train Loss: 0.015190\n",
      "Epoch 187 of 500, Train Loss: 0.015189\n",
      "best loss:  0.01518879616010371\n",
      "Epoch 188 of 500, Train Loss: 0.015188\n",
      "best loss:  0.015187917212240918\n",
      "Epoch 189 of 500, Train Loss: 0.015185\n",
      "best loss:  0.015185289330376212\n",
      "Epoch 190 of 500, Train Loss: 0.015182\n",
      "best loss:  0.015182020833946822\n",
      "Epoch 191 of 500, Train Loss: 0.015178\n",
      "best loss:  0.015177630591287473\n",
      "Epoch 192 of 500, Train Loss: 0.015176\n",
      "best loss:  0.015175695694863883\n",
      "Epoch 193 of 500, Train Loss: 0.015175\n",
      "best loss:  0.015174966125456659\n",
      "Epoch 194 of 500, Train Loss: 0.015173\n",
      "best loss:  0.015173262274641788\n",
      "Epoch 195 of 500, Train Loss: 0.015174\n",
      "Epoch 196 of 500, Train Loss: 0.015172\n",
      "best loss:  0.015171870114570555\n",
      "Epoch 197 of 500, Train Loss: 0.015170\n",
      "best loss:  0.015169771079537134\n",
      "Epoch 198 of 500, Train Loss: 0.015168\n",
      "best loss:  0.015167685004281487\n",
      "Epoch 199 of 500, Train Loss: 0.015167\n",
      "best loss:  0.01516732079669104\n",
      "Epoch 200 of 500, Train Loss: 0.015168\n",
      "Epoch 201 of 500, Train Loss: 0.015166\n",
      "best loss:  0.015166430546615144\n",
      "Epoch 202 of 500, Train Loss: 0.015168\n",
      "Epoch 203 of 500, Train Loss: 0.015167\n",
      "Epoch 204 of 500, Train Loss: 0.015159\n",
      "best loss:  0.015158653506772708\n",
      "Epoch 205 of 500, Train Loss: 0.015155\n",
      "best loss:  0.015155203665955021\n",
      "Epoch 206 of 500, Train Loss: 0.015153\n",
      "best loss:  0.015152814239669634\n",
      "Epoch 207 of 500, Train Loss: 0.015152\n",
      "best loss:  0.015151709726968304\n",
      "Epoch 208 of 500, Train Loss: 0.015150\n",
      "best loss:  0.015150264116922798\n",
      "Epoch 209 of 500, Train Loss: 0.015145\n",
      "best loss:  0.01514537201570744\n",
      "Epoch 210 of 500, Train Loss: 0.015141\n",
      "best loss:  0.015141232952641597\n",
      "Epoch 211 of 500, Train Loss: 0.015138\n",
      "best loss:  0.015137516876525614\n",
      "Epoch 212 of 500, Train Loss: 0.015140\n",
      "Epoch 213 of 500, Train Loss: 0.015137\n",
      "best loss:  0.015137085564526578\n",
      "Epoch 214 of 500, Train Loss: 0.015135\n",
      "best loss:  0.015134784999229519\n",
      "Epoch 215 of 500, Train Loss: 0.015130\n",
      "best loss:  0.015129617629247869\n",
      "Epoch 216 of 500, Train Loss: 0.015127\n",
      "best loss:  0.015127011940069136\n",
      "Epoch 217 of 500, Train Loss: 0.015122\n",
      "best loss:  0.01512241036411873\n",
      "Epoch 218 of 500, Train Loss: 0.015119\n",
      "best loss:  0.015119002077705047\n",
      "Epoch 219 of 500, Train Loss: 0.015114\n",
      "best loss:  0.015113980635006139\n",
      "Epoch 220 of 500, Train Loss: 0.015111\n",
      "best loss:  0.015111223735861018\n",
      "Epoch 221 of 500, Train Loss: 0.015109\n",
      "best loss:  0.015108600607847564\n",
      "Epoch 222 of 500, Train Loss: 0.015107\n",
      "best loss:  0.015106667649197617\n",
      "Epoch 223 of 500, Train Loss: 0.015102\n",
      "best loss:  0.01510195360476577\n",
      "Epoch 224 of 500, Train Loss: 0.015098\n",
      "best loss:  0.015098247092755064\n",
      "Epoch 225 of 500, Train Loss: 0.015097\n",
      "best loss:  0.015096762056269343\n",
      "Epoch 226 of 500, Train Loss: 0.015095\n",
      "best loss:  0.015095374946276722\n",
      "Epoch 227 of 500, Train Loss: 0.015091\n",
      "best loss:  0.015091125214203485\n",
      "Epoch 228 of 500, Train Loss: 0.015087\n",
      "best loss:  0.015086749994309706\n",
      "Epoch 229 of 500, Train Loss: 0.015084\n",
      "best loss:  0.015083988284684949\n",
      "Epoch 230 of 500, Train Loss: 0.015082\n",
      "best loss:  0.015081795369267695\n",
      "Epoch 231 of 500, Train Loss: 0.015079\n",
      "best loss:  0.015079208215167962\n",
      "Epoch 232 of 500, Train Loss: 0.015074\n",
      "best loss:  0.015074058696663714\n",
      "Epoch 233 of 500, Train Loss: 0.015072\n",
      "best loss:  0.015071720401296219\n",
      "Epoch 234 of 500, Train Loss: 0.015070\n",
      "best loss:  0.01506991112309638\n",
      "Epoch 235 of 500, Train Loss: 0.015067\n",
      "best loss:  0.015066644749441726\n",
      "Epoch 236 of 500, Train Loss: 0.015065\n",
      "best loss:  0.015064542084100274\n",
      "Epoch 237 of 500, Train Loss: 0.015061\n",
      "best loss:  0.015060807681369935\n",
      "Epoch 238 of 500, Train Loss: 0.015060\n",
      "best loss:  0.015059698929210634\n",
      "Epoch 239 of 500, Train Loss: 0.015058\n",
      "best loss:  0.015058067828270629\n",
      "Epoch 240 of 500, Train Loss: 0.015055\n",
      "best loss:  0.015054564178305816\n",
      "Epoch 241 of 500, Train Loss: 0.015051\n",
      "best loss:  0.015051266139232402\n",
      "Epoch 242 of 500, Train Loss: 0.015049\n",
      "best loss:  0.015048708787229446\n",
      "Epoch 243 of 500, Train Loss: 0.015047\n",
      "best loss:  0.015047270097127256\n",
      "Epoch 244 of 500, Train Loss: 0.015045\n",
      "best loss:  0.015044663407137733\n",
      "Epoch 245 of 500, Train Loss: 0.015043\n",
      "best loss:  0.015042660644191295\n",
      "Epoch 246 of 500, Train Loss: 0.015041\n",
      "best loss:  0.015040798783943043\n",
      "Epoch 247 of 500, Train Loss: 0.015041\n",
      "best loss:  0.015040567241764316\n",
      "Epoch 248 of 500, Train Loss: 0.015040\n",
      "best loss:  0.01504025997277241\n",
      "Epoch 249 of 500, Train Loss: 0.015038\n",
      "best loss:  0.015038422011055572\n",
      "Epoch 250 of 500, Train Loss: 0.015038\n",
      "best loss:  0.015037925164224291\n",
      "Epoch 251 of 500, Train Loss: 0.015036\n",
      "best loss:  0.015035979016575702\n",
      "Epoch 252 of 500, Train Loss: 0.015034\n",
      "best loss:  0.015034062295832649\n",
      "Epoch 253 of 500, Train Loss: 0.015035\n",
      "Epoch 254 of 500, Train Loss: 0.015033\n",
      "best loss:  0.015033474186834222\n",
      "Epoch 255 of 500, Train Loss: 0.015034\n",
      "Epoch 256 of 500, Train Loss: 0.015032\n",
      "best loss:  0.015032486605427279\n",
      "Epoch 257 of 500, Train Loss: 0.015032\n",
      "best loss:  0.015031897910516854\n",
      "Epoch 258 of 500, Train Loss: 0.015032\n",
      "Epoch 259 of 500, Train Loss: 0.015032\n",
      "best loss:  0.015031678483427987\n",
      "Epoch 260 of 500, Train Loss: 0.015030\n",
      "best loss:  0.015029983761740213\n",
      "Epoch 261 of 500, Train Loss: 0.015028\n",
      "best loss:  0.01502841748021969\n",
      "Epoch 262 of 500, Train Loss: 0.015026\n",
      "best loss:  0.015025991028898518\n",
      "Epoch 263 of 500, Train Loss: 0.015025\n",
      "best loss:  0.01502468678021854\n",
      "Epoch 264 of 500, Train Loss: 0.015024\n",
      "best loss:  0.01502392911162704\n",
      "Epoch 265 of 500, Train Loss: 0.015023\n",
      "best loss:  0.015022879126869762\n",
      "Epoch 266 of 500, Train Loss: 0.015023\n",
      "best loss:  0.015022518126566554\n",
      "Epoch 267 of 500, Train Loss: 0.015023\n",
      "Epoch 268 of 500, Train Loss: 0.015022\n",
      "best loss:  0.015022129234190048\n",
      "Epoch 269 of 500, Train Loss: 0.015023\n",
      "Epoch 270 of 500, Train Loss: 0.015022\n",
      "best loss:  0.015021869391581953\n",
      "Epoch 271 of 500, Train Loss: 0.015022\n",
      "best loss:  0.0150215542722266\n",
      "Epoch 272 of 500, Train Loss: 0.015021\n",
      "best loss:  0.015020735880410543\n",
      "Epoch 273 of 500, Train Loss: 0.015021\n",
      "Epoch 274 of 500, Train Loss: 0.015020\n",
      "best loss:  0.015019694812029109\n",
      "Epoch 275 of 500, Train Loss: 0.015020\n",
      "Epoch 276 of 500, Train Loss: 0.015019\n",
      "best loss:  0.015019065632336853\n",
      "Epoch 277 of 500, Train Loss: 0.015020\n",
      "Epoch 278 of 500, Train Loss: 0.015019\n",
      "Epoch 279 of 500, Train Loss: 0.015021\n",
      "Epoch 280 of 500, Train Loss: 0.015020\n",
      "Epoch 281 of 500, Train Loss: 0.015023\n",
      "Epoch 282 of 500, Train Loss: 0.015021\n",
      "Epoch 283 of 500, Train Loss: 0.015026\n",
      "Epoch 284 of 500, Train Loss: 0.015025\n",
      "Epoch 285 of 500, Train Loss: 0.015033\n",
      "Epoch 286 of 500, Train Loss: 0.015031\n",
      "Epoch 287 of 500, Train Loss: 0.015041\n",
      "Epoch 288 of 500, Train Loss: 0.015038\n",
      "Epoch 289 of 500, Train Loss: 0.015046\n",
      "Epoch 290 of 500, Train Loss: 0.015040\n",
      "Epoch 291 of 500, Train Loss: 0.015049\n",
      "Epoch 292 of 500, Train Loss: 0.015045\n",
      "Epoch 293 of 500, Train Loss: 0.015056\n",
      "Epoch 294 of 500, Train Loss: 0.015055\n",
      "Epoch 295 of 500, Train Loss: 0.015064\n",
      "Epoch 296 of 500, Train Loss: 0.015061\n",
      "Epoch 297 of 500, Train Loss: 0.015075\n",
      "Epoch 298 of 500, Train Loss: 0.015069\n",
      "Epoch 299 of 500, Train Loss: 0.015086\n",
      "Epoch 300 of 500, Train Loss: 0.015077\n",
      "Epoch 301 of 500, Train Loss: 0.015101\n",
      "Epoch 302 of 500, Train Loss: 0.015086\n",
      "Epoch 303 of 500, Train Loss: 0.015114\n",
      "Epoch 304 of 500, Train Loss: 0.015093\n",
      "Epoch 305 of 500, Train Loss: 0.015132\n",
      "Epoch 306 of 500, Train Loss: 0.015117\n",
      "Epoch 307 of 500, Train Loss: 0.015177\n",
      "Epoch 308 of 500, Train Loss: 0.015192\n",
      "Epoch 309 of 500, Train Loss: 0.015279\n",
      "Epoch 310 of 500, Train Loss: 0.015343\n",
      "Epoch 311 of 500, Train Loss: 0.015387\n",
      "Epoch 312 of 500, Train Loss: 0.015426\n",
      "Epoch 313 of 500, Train Loss: 0.015405\n",
      "Epoch 314 of 500, Train Loss: 0.015374\n",
      "Epoch 315 of 500, Train Loss: 0.015354\n",
      "Epoch 316 of 500, Train Loss: 0.015292\n",
      "Epoch 317 of 500, Train Loss: 0.015278\n",
      "Epoch 318 of 500, Train Loss: 0.015219\n",
      "Epoch 319 of 500, Train Loss: 0.015203\n",
      "Epoch 320 of 500, Train Loss: 0.015161\n",
      "Epoch 321 of 500, Train Loss: 0.015140\n",
      "Epoch 322 of 500, Train Loss: 0.015117\n",
      "Epoch 323 of 500, Train Loss: 0.015097\n",
      "Epoch 324 of 500, Train Loss: 0.015089\n",
      "Epoch 325 of 500, Train Loss: 0.015075\n",
      "Epoch 326 of 500, Train Loss: 0.015077\n",
      "Epoch 327 of 500, Train Loss: 0.015070\n",
      "Epoch 328 of 500, Train Loss: 0.015083\n",
      "Epoch 329 of 500, Train Loss: 0.015082\n",
      "Epoch 330 of 500, Train Loss: 0.015101\n",
      "Epoch 331 of 500, Train Loss: 0.015111\n",
      "Epoch 332 of 500, Train Loss: 0.015137\n",
      "Epoch 333 of 500, Train Loss: 0.015160\n",
      "Epoch 334 of 500, Train Loss: 0.015185\n",
      "Epoch 335 of 500, Train Loss: 0.015214\n",
      "Epoch 336 of 500, Train Loss: 0.015222\n",
      "Epoch 337 of 500, Train Loss: 0.015236\n",
      "Epoch 338 of 500, Train Loss: 0.015230\n",
      "Epoch 339 of 500, Train Loss: 0.015224\n",
      "Epoch 340 of 500, Train Loss: 0.015210\n",
      "Epoch 341 of 500, Train Loss: 0.015193\n",
      "Epoch 342 of 500, Train Loss: 0.015174\n",
      "Epoch 343 of 500, Train Loss: 0.015151\n",
      "Epoch 344 of 500, Train Loss: 0.015135\n",
      "Epoch 345 of 500, Train Loss: 0.015116\n",
      "Epoch 346 of 500, Train Loss: 0.015103\n",
      "Epoch 347 of 500, Train Loss: 0.015089\n",
      "Epoch 348 of 500, Train Loss: 0.015079\n",
      "Epoch 349 of 500, Train Loss: 0.015069\n",
      "Epoch 350 of 500, Train Loss: 0.015066\n",
      "Epoch 351 of 500, Train Loss: 0.015059\n",
      "Epoch 352 of 500, Train Loss: 0.015064\n",
      "Epoch 353 of 500, Train Loss: 0.015060\n",
      "Epoch 354 of 500, Train Loss: 0.015071\n",
      "Epoch 355 of 500, Train Loss: 0.015072\n",
      "Epoch 356 of 500, Train Loss: 0.015092\n",
      "Epoch 357 of 500, Train Loss: 0.015105\n",
      "Epoch 358 of 500, Train Loss: 0.015132\n",
      "Epoch 359 of 500, Train Loss: 0.015159\n",
      "Epoch 360 of 500, Train Loss: 0.015181\n",
      "Epoch 361 of 500, Train Loss: 0.015210\n",
      "Epoch 362 of 500, Train Loss: 0.015216\n",
      "Epoch 363 of 500, Train Loss: 0.015230\n",
      "Epoch 364 of 500, Train Loss: 0.015219\n",
      "Epoch 365 of 500, Train Loss: 0.015214\n",
      "Epoch 366 of 500, Train Loss: 0.015196\n",
      "Epoch 367 of 500, Train Loss: 0.015178\n",
      "Epoch 368 of 500, Train Loss: 0.015162\n",
      "Epoch 369 of 500, Train Loss: 0.015142\n",
      "Epoch 370 of 500, Train Loss: 0.015130\n",
      "Epoch 371 of 500, Train Loss: 0.015113\n",
      "Epoch 372 of 500, Train Loss: 0.015105\n",
      "Epoch 373 of 500, Train Loss: 0.015092\n",
      "Epoch 374 of 500, Train Loss: 0.015090\n",
      "Epoch 375 of 500, Train Loss: 0.015080\n",
      "Epoch 376 of 500, Train Loss: 0.015083\n",
      "Epoch 377 of 500, Train Loss: 0.015077\n",
      "Epoch 378 of 500, Train Loss: 0.015085\n",
      "Epoch 379 of 500, Train Loss: 0.015084\n",
      "Epoch 380 of 500, Train Loss: 0.015097\n",
      "Epoch 381 of 500, Train Loss: 0.015103\n",
      "Epoch 382 of 500, Train Loss: 0.015118\n",
      "Epoch 383 of 500, Train Loss: 0.015130\n",
      "Epoch 384 of 500, Train Loss: 0.015144\n",
      "Epoch 385 of 500, Train Loss: 0.015157\n",
      "Epoch 386 of 500, Train Loss: 0.015164\n",
      "Epoch 387 of 500, Train Loss: 0.015172\n",
      "Epoch 388 of 500, Train Loss: 0.015170\n",
      "Epoch 389 of 500, Train Loss: 0.015169\n",
      "Epoch 390 of 500, Train Loss: 0.015162\n",
      "Epoch 391 of 500, Train Loss: 0.015153\n",
      "Epoch 392 of 500, Train Loss: 0.015146\n",
      "Epoch 393 of 500, Train Loss: 0.015133\n",
      "Epoch 394 of 500, Train Loss: 0.015127\n",
      "Epoch 395 of 500, Train Loss: 0.015114\n",
      "Epoch 396 of 500, Train Loss: 0.015110\n",
      "Epoch 397 of 500, Train Loss: 0.015100\n",
      "Epoch 398 of 500, Train Loss: 0.015099\n",
      "Epoch 399 of 500, Train Loss: 0.015091\n",
      "Epoch 400 of 500, Train Loss: 0.015094\n",
      "Epoch 401 of 500, Train Loss: 0.015089\n",
      "Epoch 402 of 500, Train Loss: 0.015094\n",
      "Epoch 403 of 500, Train Loss: 0.015093\n",
      "Epoch 404 of 500, Train Loss: 0.015100\n",
      "Epoch 405 of 500, Train Loss: 0.015102\n",
      "Epoch 406 of 500, Train Loss: 0.015111\n",
      "Epoch 407 of 500, Train Loss: 0.015115\n",
      "Epoch 408 of 500, Train Loss: 0.015123\n",
      "Epoch 409 of 500, Train Loss: 0.015127\n",
      "Epoch 410 of 500, Train Loss: 0.015132\n",
      "Epoch 411 of 500, Train Loss: 0.015134\n",
      "Epoch 412 of 500, Train Loss: 0.015136\n",
      "Epoch 413 of 500, Train Loss: 0.015134\n",
      "Epoch 414 of 500, Train Loss: 0.015133\n",
      "Epoch 415 of 500, Train Loss: 0.015128\n",
      "Epoch 416 of 500, Train Loss: 0.015125\n",
      "Epoch 417 of 500, Train Loss: 0.015118\n",
      "Epoch 418 of 500, Train Loss: 0.015115\n",
      "Epoch 419 of 500, Train Loss: 0.015107\n",
      "Epoch 420 of 500, Train Loss: 0.015106\n",
      "Epoch 421 of 500, Train Loss: 0.015099\n",
      "Epoch 422 of 500, Train Loss: 0.015099\n",
      "Epoch 423 of 500, Train Loss: 0.015093\n",
      "Epoch 424 of 500, Train Loss: 0.015094\n",
      "Epoch 425 of 500, Train Loss: 0.015090\n",
      "Epoch 426 of 500, Train Loss: 0.015093\n",
      "Epoch 427 of 500, Train Loss: 0.015092\n",
      "Epoch 428 of 500, Train Loss: 0.015096\n",
      "Epoch 429 of 500, Train Loss: 0.015095\n",
      "Epoch 430 of 500, Train Loss: 0.015101\n",
      "Epoch 431 of 500, Train Loss: 0.015102\n",
      "Epoch 432 of 500, Train Loss: 0.015107\n",
      "Epoch 433 of 500, Train Loss: 0.015107\n",
      "Epoch 434 of 500, Train Loss: 0.015112\n",
      "Epoch 435 of 500, Train Loss: 0.015111\n",
      "Epoch 436 of 500, Train Loss: 0.015114\n",
      "Epoch 437 of 500, Train Loss: 0.015112\n",
      "Epoch 438 of 500, Train Loss: 0.015113\n",
      "Epoch 439 of 500, Train Loss: 0.015110\n",
      "Epoch 440 of 500, Train Loss: 0.015110\n",
      "Epoch 441 of 500, Train Loss: 0.015108\n",
      "Epoch 442 of 500, Train Loss: 0.015110\n",
      "Epoch 443 of 500, Train Loss: 0.015104\n",
      "Epoch 444 of 500, Train Loss: 0.015103\n",
      "Epoch 445 of 500, Train Loss: 0.015097\n",
      "Epoch 446 of 500, Train Loss: 0.015096\n",
      "Epoch 447 of 500, Train Loss: 0.015093\n",
      "Epoch 448 of 500, Train Loss: 0.015093\n",
      "Epoch 449 of 500, Train Loss: 0.015092\n",
      "Epoch 450 of 500, Train Loss: 0.015093\n",
      "Epoch 451 of 500, Train Loss: 0.015090\n",
      "Epoch 452 of 500, Train Loss: 0.015090\n",
      "Epoch 453 of 500, Train Loss: 0.015089\n",
      "Epoch 454 of 500, Train Loss: 0.015091\n",
      "Epoch 455 of 500, Train Loss: 0.015096\n",
      "Epoch 456 of 500, Train Loss: 0.015098\n",
      "Epoch 457 of 500, Train Loss: 0.015098\n",
      "Epoch 458 of 500, Train Loss: 0.015096\n",
      "Epoch 459 of 500, Train Loss: 0.015098\n",
      "Epoch 460 of 500, Train Loss: 0.015094\n",
      "Epoch 461 of 500, Train Loss: 0.015101\n",
      "Epoch 462 of 500, Train Loss: 0.015105\n",
      "Epoch 463 of 500, Train Loss: 0.015110\n",
      "Epoch 464 of 500, Train Loss: 0.015108\n",
      "Epoch 465 of 500, Train Loss: 0.015106\n",
      "Epoch 466 of 500, Train Loss: 0.015098\n",
      "Epoch 467 of 500, Train Loss: 0.015098\n",
      "Epoch 468 of 500, Train Loss: 0.015095\n",
      "Epoch 469 of 500, Train Loss: 0.015107\n",
      "Epoch 470 of 500, Train Loss: 0.015110\n",
      "Epoch 471 of 500, Train Loss: 0.015109\n",
      "Epoch 472 of 500, Train Loss: 0.015094\n",
      "Epoch 473 of 500, Train Loss: 0.015094\n",
      "Epoch 474 of 500, Train Loss: 0.015084\n",
      "Epoch 475 of 500, Train Loss: 0.015087\n",
      "Epoch 476 of 500, Train Loss: 0.015081\n",
      "Epoch 477 of 500, Train Loss: 0.015093\n",
      "Epoch 478 of 500, Train Loss: 0.015095\n",
      "Epoch 479 of 500, Train Loss: 0.015106\n",
      "Epoch 480 of 500, Train Loss: 0.015095\n",
      "Epoch 481 of 500, Train Loss: 0.015099\n",
      "Epoch 482 of 500, Train Loss: 0.015084\n",
      "Epoch 483 of 500, Train Loss: 0.015088\n",
      "Epoch 484 of 500, Train Loss: 0.015083\n",
      "Epoch 485 of 500, Train Loss: 0.015093\n",
      "Epoch 486 of 500, Train Loss: 0.015093\n",
      "Epoch 487 of 500, Train Loss: 0.015104\n",
      "Epoch 488 of 500, Train Loss: 0.015100\n",
      "Epoch 489 of 500, Train Loss: 0.015109\n",
      "Epoch 490 of 500, Train Loss: 0.015101\n",
      "Epoch 491 of 500, Train Loss: 0.015107\n",
      "Epoch 492 of 500, Train Loss: 0.015097\n",
      "Epoch 493 of 500, Train Loss: 0.015110\n",
      "Epoch 494 of 500, Train Loss: 0.015105\n",
      "Epoch 495 of 500, Train Loss: 0.015117\n",
      "Epoch 496 of 500, Train Loss: 0.015112\n",
      "Epoch 497 of 500, Train Loss: 0.015121\n",
      "Epoch 498 of 500, Train Loss: 0.015108\n",
      "Epoch 499 of 500, Train Loss: 0.015116\n",
      "Epoch 500 of 500, Train Loss: 0.015102\n",
      "latent train shape:  (16395, 20)\n",
      "M: 20, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 56\n",
      "Training the subspace: 0 / 20\n",
      "Training the subspace: 1 / 20\n",
      "Training the subspace: 2 / 20\n",
      "Training the subspace: 3 / 20\n",
      "Training the subspace: 4 / 20\n",
      "Training the subspace: 5 / 20\n",
      "Training the subspace: 6 / 20\n",
      "Training the subspace: 7 / 20\n",
      "Training the subspace: 8 / 20\n",
      "Training the subspace: 9 / 20\n",
      "Training the subspace: 10 / 20\n",
      "Training the subspace: 11 / 20\n",
      "Training the subspace: 12 / 20\n",
      "Training the subspace: 13 / 20\n",
      "Training the subspace: 14 / 20\n",
      "Training the subspace: 15 / 20\n",
      "Training the subspace: 16 / 20\n",
      "Training the subspace: 17 / 20\n",
      "Training the subspace: 18 / 20\n",
      "Training the subspace: 19 / 20\n",
      "Encoding the subspace: 0 / 20\n",
      "Encoding the subspace: 1 / 20\n",
      "Encoding the subspace: 2 / 20\n",
      "Encoding the subspace: 3 / 20\n",
      "Encoding the subspace: 4 / 20\n",
      "Encoding the subspace: 5 / 20\n",
      "Encoding the subspace: 6 / 20\n",
      "Encoding the subspace: 7 / 20\n",
      "Encoding the subspace: 8 / 20\n",
      "Encoding the subspace: 9 / 20\n",
      "Encoding the subspace: 10 / 20\n",
      "Encoding the subspace: 11 / 20\n",
      "Encoding the subspace: 12 / 20\n",
      "Encoding the subspace: 13 / 20\n",
      "Encoding the subspace: 14 / 20\n",
      "Encoding the subspace: 15 / 20\n",
      "Encoding the subspace: 16 / 20\n",
      "Encoding the subspace: 17 / 20\n",
      "Encoding the subspace: 18 / 20\n",
      "Encoding the subspace: 19 / 20\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=30, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.515014\n",
      "best loss:  0.5150144066690474\n",
      "Epoch 2 of 500, Train Loss: 0.062139\n",
      "best loss:  0.0621386606213745\n",
      "Epoch 3 of 500, Train Loss: 0.055481\n",
      "best loss:  0.05548050644394198\n",
      "Epoch 4 of 500, Train Loss: 0.053913\n",
      "best loss:  0.05391343545877959\n",
      "Epoch 5 of 500, Train Loss: 0.051267\n",
      "best loss:  0.05126685382754997\n",
      "Epoch 6 of 500, Train Loss: 0.047485\n",
      "best loss:  0.04748525769114061\n",
      "Epoch 7 of 500, Train Loss: 0.043155\n",
      "best loss:  0.04315465484798142\n",
      "Epoch 8 of 500, Train Loss: 0.039021\n",
      "best loss:  0.03902101913934516\n",
      "Epoch 9 of 500, Train Loss: 0.035411\n",
      "best loss:  0.03541110481303402\n",
      "Epoch 10 of 500, Train Loss: 0.032476\n",
      "best loss:  0.03247575936144102\n",
      "Epoch 11 of 500, Train Loss: 0.030224\n",
      "best loss:  0.030223696052219518\n",
      "Epoch 12 of 500, Train Loss: 0.028522\n",
      "best loss:  0.028521726610925942\n",
      "Epoch 13 of 500, Train Loss: 0.027200\n",
      "best loss:  0.02720027660830091\n",
      "Epoch 14 of 500, Train Loss: 0.026124\n",
      "best loss:  0.026123520423650973\n",
      "Epoch 15 of 500, Train Loss: 0.025193\n",
      "best loss:  0.025192690267346714\n",
      "Epoch 16 of 500, Train Loss: 0.024348\n",
      "best loss:  0.02434798762610168\n",
      "Epoch 17 of 500, Train Loss: 0.023564\n",
      "best loss:  0.023564303287958308\n",
      "Epoch 18 of 500, Train Loss: 0.022840\n",
      "best loss:  0.02283985008099369\n",
      "Epoch 19 of 500, Train Loss: 0.022182\n",
      "best loss:  0.022181837081317175\n",
      "Epoch 20 of 500, Train Loss: 0.021593\n",
      "best loss:  0.021592991809674732\n",
      "Epoch 21 of 500, Train Loss: 0.021073\n",
      "best loss:  0.021073127136558525\n",
      "Epoch 22 of 500, Train Loss: 0.020614\n",
      "best loss:  0.020614085719891817\n",
      "Epoch 23 of 500, Train Loss: 0.020206\n",
      "best loss:  0.020205650169965485\n",
      "Epoch 24 of 500, Train Loss: 0.019839\n",
      "best loss:  0.019839073752879723\n",
      "Epoch 25 of 500, Train Loss: 0.019502\n",
      "best loss:  0.019501960013547277\n",
      "Epoch 26 of 500, Train Loss: 0.019188\n",
      "best loss:  0.0191875044811452\n",
      "Epoch 27 of 500, Train Loss: 0.018888\n",
      "best loss:  0.01888838435984361\n",
      "Epoch 28 of 500, Train Loss: 0.018598\n",
      "best loss:  0.018598400464492612\n",
      "Epoch 29 of 500, Train Loss: 0.018315\n",
      "best loss:  0.01831515884327399\n",
      "Epoch 30 of 500, Train Loss: 0.018041\n",
      "best loss:  0.018040962467401455\n",
      "Epoch 31 of 500, Train Loss: 0.017777\n",
      "best loss:  0.017776914382201813\n",
      "Epoch 32 of 500, Train Loss: 0.017520\n",
      "best loss:  0.01751993246698687\n",
      "Epoch 33 of 500, Train Loss: 0.017276\n",
      "best loss:  0.01727629066088468\n",
      "Epoch 34 of 500, Train Loss: 0.017047\n",
      "best loss:  0.017047424343380312\n",
      "Epoch 35 of 500, Train Loss: 0.016837\n",
      "best loss:  0.016837089339879437\n",
      "Epoch 36 of 500, Train Loss: 0.016642\n",
      "best loss:  0.01664154543117223\n",
      "Epoch 37 of 500, Train Loss: 0.016465\n",
      "best loss:  0.016464513170852346\n",
      "Epoch 38 of 500, Train Loss: 0.016305\n",
      "best loss:  0.016304720645272282\n",
      "Epoch 39 of 500, Train Loss: 0.016159\n",
      "best loss:  0.01615915976080307\n",
      "Epoch 40 of 500, Train Loss: 0.016025\n",
      "best loss:  0.01602506047442317\n",
      "Epoch 41 of 500, Train Loss: 0.015901\n",
      "best loss:  0.015900561991671114\n",
      "Epoch 42 of 500, Train Loss: 0.015785\n",
      "best loss:  0.015784910822949597\n",
      "Epoch 43 of 500, Train Loss: 0.015680\n",
      "best loss:  0.015679598811882106\n",
      "Epoch 44 of 500, Train Loss: 0.015578\n",
      "best loss:  0.015578353265390638\n",
      "Epoch 45 of 500, Train Loss: 0.015484\n",
      "best loss:  0.015483792200043811\n",
      "Epoch 46 of 500, Train Loss: 0.015397\n",
      "best loss:  0.015397403581436117\n",
      "Epoch 47 of 500, Train Loss: 0.015315\n",
      "best loss:  0.015314656281807684\n",
      "Epoch 48 of 500, Train Loss: 0.015236\n",
      "best loss:  0.01523607788073231\n",
      "Epoch 49 of 500, Train Loss: 0.015162\n",
      "best loss:  0.015162155357263478\n",
      "Epoch 50 of 500, Train Loss: 0.015092\n",
      "best loss:  0.01509164783213822\n",
      "Epoch 51 of 500, Train Loss: 0.015025\n",
      "best loss:  0.01502519850655235\n",
      "Epoch 52 of 500, Train Loss: 0.014962\n",
      "best loss:  0.014961503670810786\n",
      "Epoch 53 of 500, Train Loss: 0.014900\n",
      "best loss:  0.01489956079090606\n",
      "Epoch 54 of 500, Train Loss: 0.014841\n",
      "best loss:  0.014841038721086558\n",
      "Epoch 55 of 500, Train Loss: 0.014789\n",
      "best loss:  0.014788571285587453\n",
      "Epoch 56 of 500, Train Loss: 0.014731\n",
      "best loss:  0.01473106415161697\n",
      "Epoch 57 of 500, Train Loss: 0.014681\n",
      "best loss:  0.014681368526576678\n",
      "Epoch 58 of 500, Train Loss: 0.014630\n",
      "best loss:  0.014630167942012249\n",
      "Epoch 59 of 500, Train Loss: 0.014582\n",
      "best loss:  0.014582315027125382\n",
      "Epoch 60 of 500, Train Loss: 0.014535\n",
      "best loss:  0.014534588740974333\n",
      "Epoch 61 of 500, Train Loss: 0.014491\n",
      "best loss:  0.014491470326225511\n",
      "Epoch 62 of 500, Train Loss: 0.014448\n",
      "best loss:  0.01444831758948736\n",
      "Epoch 63 of 500, Train Loss: 0.014406\n",
      "best loss:  0.014406166501684374\n",
      "Epoch 64 of 500, Train Loss: 0.014368\n",
      "best loss:  0.014368078590627009\n",
      "Epoch 65 of 500, Train Loss: 0.014330\n",
      "best loss:  0.014330390050001947\n",
      "Epoch 66 of 500, Train Loss: 0.014291\n",
      "best loss:  0.014291170574345648\n",
      "Epoch 67 of 500, Train Loss: 0.014256\n",
      "best loss:  0.014255892177421037\n",
      "Epoch 68 of 500, Train Loss: 0.014227\n",
      "best loss:  0.014227074639765228\n",
      "Epoch 69 of 500, Train Loss: 0.014192\n",
      "best loss:  0.014191670561896984\n",
      "Epoch 70 of 500, Train Loss: 0.014161\n",
      "best loss:  0.014160583453621109\n",
      "Epoch 71 of 500, Train Loss: 0.014133\n",
      "best loss:  0.01413280413600674\n",
      "Epoch 72 of 500, Train Loss: 0.014102\n",
      "best loss:  0.014101827247095359\n",
      "Epoch 73 of 500, Train Loss: 0.014076\n",
      "best loss:  0.014075914592989323\n",
      "Epoch 74 of 500, Train Loss: 0.014053\n",
      "best loss:  0.0140532763686587\n",
      "Epoch 75 of 500, Train Loss: 0.014027\n",
      "best loss:  0.014027140752814868\n",
      "Epoch 76 of 500, Train Loss: 0.014005\n",
      "best loss:  0.014005059555945413\n",
      "Epoch 77 of 500, Train Loss: 0.013984\n",
      "best loss:  0.01398417895705829\n",
      "Epoch 78 of 500, Train Loss: 0.013960\n",
      "best loss:  0.01396012566478331\n",
      "Epoch 79 of 500, Train Loss: 0.013942\n",
      "best loss:  0.013941885954691386\n",
      "Epoch 80 of 500, Train Loss: 0.013923\n",
      "best loss:  0.013923069040964235\n",
      "Epoch 81 of 500, Train Loss: 0.013904\n",
      "best loss:  0.013903756508858181\n",
      "Epoch 82 of 500, Train Loss: 0.013887\n",
      "best loss:  0.013887312206621968\n",
      "Epoch 83 of 500, Train Loss: 0.013870\n",
      "best loss:  0.01387037752176339\n",
      "Epoch 84 of 500, Train Loss: 0.013855\n",
      "best loss:  0.013855252182562366\n",
      "Epoch 85 of 500, Train Loss: 0.013840\n",
      "best loss:  0.013840232712267211\n",
      "Epoch 86 of 500, Train Loss: 0.013824\n",
      "best loss:  0.013823792211803456\n",
      "Epoch 87 of 500, Train Loss: 0.013810\n",
      "best loss:  0.01381024459057466\n",
      "Epoch 88 of 500, Train Loss: 0.013796\n",
      "best loss:  0.013795681281986306\n",
      "Epoch 89 of 500, Train Loss: 0.013781\n",
      "best loss:  0.013780884935643322\n",
      "Epoch 90 of 500, Train Loss: 0.013766\n",
      "best loss:  0.01376587003464955\n",
      "Epoch 91 of 500, Train Loss: 0.013753\n",
      "best loss:  0.013752604936937217\n",
      "Epoch 92 of 500, Train Loss: 0.013741\n",
      "best loss:  0.013740607785275784\n",
      "Epoch 93 of 500, Train Loss: 0.013730\n",
      "best loss:  0.013729954683700172\n",
      "Epoch 94 of 500, Train Loss: 0.013720\n",
      "best loss:  0.013719728505762308\n",
      "Epoch 95 of 500, Train Loss: 0.013709\n",
      "best loss:  0.013709071062752515\n",
      "Epoch 96 of 500, Train Loss: 0.013699\n",
      "best loss:  0.013699137252194142\n",
      "Epoch 97 of 500, Train Loss: 0.013689\n",
      "best loss:  0.013689097988079916\n",
      "Epoch 98 of 500, Train Loss: 0.013679\n",
      "best loss:  0.013679164465175682\n",
      "Epoch 99 of 500, Train Loss: 0.013671\n",
      "best loss:  0.013671201177513545\n",
      "Epoch 100 of 500, Train Loss: 0.013663\n",
      "best loss:  0.013662557784772962\n",
      "Epoch 101 of 500, Train Loss: 0.013656\n",
      "best loss:  0.013656370953409463\n",
      "Epoch 102 of 500, Train Loss: 0.013650\n",
      "best loss:  0.013649990169381968\n",
      "Epoch 103 of 500, Train Loss: 0.013647\n",
      "best loss:  0.013647256221878522\n",
      "Epoch 104 of 500, Train Loss: 0.013645\n",
      "best loss:  0.01364483976174442\n",
      "Epoch 105 of 500, Train Loss: 0.013643\n",
      "best loss:  0.013643132766136582\n",
      "Epoch 106 of 500, Train Loss: 0.013642\n",
      "best loss:  0.013642497396804103\n",
      "Epoch 107 of 500, Train Loss: 0.013645\n",
      "Epoch 108 of 500, Train Loss: 0.013645\n",
      "Epoch 109 of 500, Train Loss: 0.013655\n",
      "Epoch 110 of 500, Train Loss: 0.013665\n",
      "Epoch 111 of 500, Train Loss: 0.013671\n",
      "Epoch 112 of 500, Train Loss: 0.013674\n",
      "Epoch 113 of 500, Train Loss: 0.013672\n",
      "Epoch 114 of 500, Train Loss: 0.013660\n",
      "Epoch 115 of 500, Train Loss: 0.013658\n",
      "Epoch 116 of 500, Train Loss: 0.013658\n",
      "Epoch 117 of 500, Train Loss: 0.013667\n",
      "Epoch 118 of 500, Train Loss: 0.013691\n",
      "Epoch 119 of 500, Train Loss: 0.013693\n",
      "Epoch 120 of 500, Train Loss: 0.013756\n",
      "Epoch 121 of 500, Train Loss: 0.013730\n",
      "Epoch 122 of 500, Train Loss: 0.013835\n",
      "Epoch 123 of 500, Train Loss: 0.013737\n",
      "Epoch 124 of 500, Train Loss: 0.013796\n",
      "Epoch 125 of 500, Train Loss: 0.013700\n",
      "Epoch 126 of 500, Train Loss: 0.013717\n",
      "Epoch 127 of 500, Train Loss: 0.013670\n",
      "Epoch 128 of 500, Train Loss: 0.013667\n",
      "Epoch 129 of 500, Train Loss: 0.013653\n",
      "Epoch 130 of 500, Train Loss: 0.013643\n",
      "Epoch 131 of 500, Train Loss: 0.013644\n",
      "Epoch 132 of 500, Train Loss: 0.013633\n",
      "best loss:  0.013633455930218143\n",
      "Epoch 133 of 500, Train Loss: 0.013637\n",
      "Epoch 134 of 500, Train Loss: 0.013629\n",
      "best loss:  0.013629371361931526\n",
      "Epoch 135 of 500, Train Loss: 0.013618\n",
      "best loss:  0.013618025533690717\n",
      "Epoch 136 of 500, Train Loss: 0.013612\n",
      "best loss:  0.013612104020516439\n",
      "Epoch 137 of 500, Train Loss: 0.013599\n",
      "best loss:  0.013598673672382829\n",
      "Epoch 138 of 500, Train Loss: 0.013593\n",
      "best loss:  0.013593123677500719\n",
      "Epoch 139 of 500, Train Loss: 0.013586\n",
      "best loss:  0.013586045181028574\n",
      "Epoch 140 of 500, Train Loss: 0.013587\n",
      "Epoch 141 of 500, Train Loss: 0.013584\n",
      "best loss:  0.01358357292366243\n",
      "Epoch 142 of 500, Train Loss: 0.013589\n",
      "Epoch 143 of 500, Train Loss: 0.013585\n",
      "Epoch 144 of 500, Train Loss: 0.013591\n",
      "Epoch 145 of 500, Train Loss: 0.013587\n",
      "Epoch 146 of 500, Train Loss: 0.013600\n",
      "Epoch 147 of 500, Train Loss: 0.013598\n",
      "Epoch 148 of 500, Train Loss: 0.013609\n",
      "Epoch 149 of 500, Train Loss: 0.013597\n",
      "Epoch 150 of 500, Train Loss: 0.013601\n",
      "Epoch 151 of 500, Train Loss: 0.013591\n",
      "Epoch 152 of 500, Train Loss: 0.013584\n",
      "Epoch 153 of 500, Train Loss: 0.013587\n",
      "Epoch 154 of 500, Train Loss: 0.013577\n",
      "best loss:  0.013576723597745245\n",
      "Epoch 155 of 500, Train Loss: 0.013569\n",
      "best loss:  0.013568916417389401\n",
      "Epoch 156 of 500, Train Loss: 0.013562\n",
      "best loss:  0.013562456897573632\n",
      "Epoch 157 of 500, Train Loss: 0.013554\n",
      "best loss:  0.013554380127082601\n",
      "Epoch 158 of 500, Train Loss: 0.013550\n",
      "best loss:  0.013550268796357406\n",
      "Epoch 159 of 500, Train Loss: 0.013539\n",
      "best loss:  0.013538907521202178\n",
      "Epoch 160 of 500, Train Loss: 0.013539\n",
      "Epoch 161 of 500, Train Loss: 0.013532\n",
      "best loss:  0.01353158256401806\n",
      "Epoch 162 of 500, Train Loss: 0.013533\n",
      "Epoch 163 of 500, Train Loss: 0.013528\n",
      "best loss:  0.013527991639336484\n",
      "Epoch 164 of 500, Train Loss: 0.013539\n",
      "Epoch 165 of 500, Train Loss: 0.013525\n",
      "best loss:  0.013525492514622607\n",
      "Epoch 166 of 500, Train Loss: 0.013535\n",
      "Epoch 167 of 500, Train Loss: 0.013520\n",
      "best loss:  0.01351956321673771\n",
      "Epoch 168 of 500, Train Loss: 0.013529\n",
      "Epoch 169 of 500, Train Loss: 0.013507\n",
      "best loss:  0.013506863934715285\n",
      "Epoch 170 of 500, Train Loss: 0.013522\n",
      "Epoch 171 of 500, Train Loss: 0.013499\n",
      "best loss:  0.013499388674746968\n",
      "Epoch 172 of 500, Train Loss: 0.013517\n",
      "Epoch 173 of 500, Train Loss: 0.013496\n",
      "best loss:  0.013495916883549617\n",
      "Epoch 174 of 500, Train Loss: 0.013516\n",
      "Epoch 175 of 500, Train Loss: 0.013490\n",
      "best loss:  0.013489966888103432\n",
      "Epoch 176 of 500, Train Loss: 0.013510\n",
      "Epoch 177 of 500, Train Loss: 0.013483\n",
      "best loss:  0.013482919654785475\n",
      "Epoch 178 of 500, Train Loss: 0.013504\n",
      "Epoch 179 of 500, Train Loss: 0.013478\n",
      "best loss:  0.013478213611199672\n",
      "Epoch 180 of 500, Train Loss: 0.013503\n",
      "Epoch 181 of 500, Train Loss: 0.013477\n",
      "best loss:  0.013477316042525665\n",
      "Epoch 182 of 500, Train Loss: 0.013501\n",
      "Epoch 183 of 500, Train Loss: 0.013476\n",
      "best loss:  0.013476097688725031\n",
      "Epoch 184 of 500, Train Loss: 0.013496\n",
      "Epoch 185 of 500, Train Loss: 0.013470\n",
      "best loss:  0.013470290249029477\n",
      "Epoch 186 of 500, Train Loss: 0.013493\n",
      "Epoch 187 of 500, Train Loss: 0.013465\n",
      "best loss:  0.013465232283053848\n",
      "Epoch 188 of 500, Train Loss: 0.013488\n",
      "Epoch 189 of 500, Train Loss: 0.013462\n",
      "best loss:  0.01346207801464678\n",
      "Epoch 190 of 500, Train Loss: 0.013483\n",
      "Epoch 191 of 500, Train Loss: 0.013459\n",
      "best loss:  0.013458719547673728\n",
      "Epoch 192 of 500, Train Loss: 0.013482\n",
      "Epoch 193 of 500, Train Loss: 0.013456\n",
      "best loss:  0.013455812569550607\n",
      "Epoch 194 of 500, Train Loss: 0.013477\n",
      "Epoch 195 of 500, Train Loss: 0.013454\n",
      "best loss:  0.01345421948926812\n",
      "Epoch 196 of 500, Train Loss: 0.013475\n",
      "Epoch 197 of 500, Train Loss: 0.013455\n",
      "Epoch 198 of 500, Train Loss: 0.013476\n",
      "Epoch 199 of 500, Train Loss: 0.013459\n",
      "Epoch 200 of 500, Train Loss: 0.013478\n",
      "Epoch 201 of 500, Train Loss: 0.013462\n",
      "Epoch 202 of 500, Train Loss: 0.013482\n",
      "Epoch 203 of 500, Train Loss: 0.013468\n",
      "Epoch 204 of 500, Train Loss: 0.013487\n",
      "Epoch 205 of 500, Train Loss: 0.013472\n",
      "Epoch 206 of 500, Train Loss: 0.013491\n",
      "Epoch 207 of 500, Train Loss: 0.013477\n",
      "Epoch 208 of 500, Train Loss: 0.013495\n",
      "Epoch 209 of 500, Train Loss: 0.013482\n",
      "Epoch 210 of 500, Train Loss: 0.013496\n",
      "Epoch 211 of 500, Train Loss: 0.013485\n",
      "Epoch 212 of 500, Train Loss: 0.013496\n",
      "Epoch 213 of 500, Train Loss: 0.013488\n",
      "Epoch 214 of 500, Train Loss: 0.013495\n",
      "Epoch 215 of 500, Train Loss: 0.013489\n",
      "Epoch 216 of 500, Train Loss: 0.013492\n",
      "Epoch 217 of 500, Train Loss: 0.013487\n",
      "Epoch 218 of 500, Train Loss: 0.013490\n",
      "Epoch 219 of 500, Train Loss: 0.013484\n",
      "Epoch 220 of 500, Train Loss: 0.013484\n",
      "Epoch 221 of 500, Train Loss: 0.013477\n",
      "Epoch 222 of 500, Train Loss: 0.013480\n",
      "Epoch 223 of 500, Train Loss: 0.013471\n",
      "Epoch 224 of 500, Train Loss: 0.013472\n",
      "Epoch 225 of 500, Train Loss: 0.013462\n",
      "Epoch 226 of 500, Train Loss: 0.013470\n",
      "Epoch 227 of 500, Train Loss: 0.013458\n",
      "Epoch 228 of 500, Train Loss: 0.013465\n",
      "Epoch 229 of 500, Train Loss: 0.013452\n",
      "best loss:  0.013452438307770839\n",
      "Epoch 230 of 500, Train Loss: 0.013467\n",
      "Epoch 231 of 500, Train Loss: 0.013453\n",
      "Epoch 232 of 500, Train Loss: 0.013468\n",
      "Epoch 233 of 500, Train Loss: 0.013452\n",
      "best loss:  0.01345241139191672\n",
      "Epoch 234 of 500, Train Loss: 0.013473\n",
      "Epoch 235 of 500, Train Loss: 0.013456\n",
      "Epoch 236 of 500, Train Loss: 0.013478\n",
      "Epoch 237 of 500, Train Loss: 0.013458\n",
      "Epoch 238 of 500, Train Loss: 0.013484\n",
      "Epoch 239 of 500, Train Loss: 0.013463\n",
      "Epoch 240 of 500, Train Loss: 0.013491\n",
      "Epoch 241 of 500, Train Loss: 0.013465\n",
      "Epoch 242 of 500, Train Loss: 0.013494\n",
      "Epoch 243 of 500, Train Loss: 0.013464\n",
      "Epoch 244 of 500, Train Loss: 0.013495\n",
      "Epoch 245 of 500, Train Loss: 0.013459\n",
      "Epoch 246 of 500, Train Loss: 0.013488\n",
      "Epoch 247 of 500, Train Loss: 0.013449\n",
      "best loss:  0.01344895725478707\n",
      "Epoch 248 of 500, Train Loss: 0.013481\n",
      "Epoch 249 of 500, Train Loss: 0.013439\n",
      "best loss:  0.013439155401221599\n",
      "Epoch 250 of 500, Train Loss: 0.013472\n",
      "Epoch 251 of 500, Train Loss: 0.013428\n",
      "best loss:  0.013428397232110706\n",
      "Epoch 252 of 500, Train Loss: 0.013467\n",
      "Epoch 253 of 500, Train Loss: 0.013421\n",
      "best loss:  0.013421478452722275\n",
      "Epoch 254 of 500, Train Loss: 0.013464\n",
      "Epoch 255 of 500, Train Loss: 0.013417\n",
      "best loss:  0.013417399561713096\n",
      "Epoch 256 of 500, Train Loss: 0.013466\n",
      "Epoch 257 of 500, Train Loss: 0.013417\n",
      "best loss:  0.013417140682013128\n",
      "Epoch 258 of 500, Train Loss: 0.013470\n",
      "Epoch 259 of 500, Train Loss: 0.013419\n",
      "Epoch 260 of 500, Train Loss: 0.013476\n",
      "Epoch 261 of 500, Train Loss: 0.013424\n",
      "Epoch 262 of 500, Train Loss: 0.013482\n",
      "Epoch 263 of 500, Train Loss: 0.013427\n",
      "Epoch 264 of 500, Train Loss: 0.013488\n",
      "Epoch 265 of 500, Train Loss: 0.013432\n",
      "Epoch 266 of 500, Train Loss: 0.013494\n",
      "Epoch 267 of 500, Train Loss: 0.013437\n",
      "Epoch 268 of 500, Train Loss: 0.013498\n",
      "Epoch 269 of 500, Train Loss: 0.013442\n",
      "Epoch 270 of 500, Train Loss: 0.013501\n",
      "Epoch 271 of 500, Train Loss: 0.013446\n",
      "Epoch 272 of 500, Train Loss: 0.013503\n",
      "Epoch 273 of 500, Train Loss: 0.013451\n",
      "Epoch 274 of 500, Train Loss: 0.013506\n",
      "Epoch 275 of 500, Train Loss: 0.013456\n",
      "Epoch 276 of 500, Train Loss: 0.013510\n",
      "Epoch 277 of 500, Train Loss: 0.013463\n",
      "Epoch 278 of 500, Train Loss: 0.013512\n",
      "Epoch 279 of 500, Train Loss: 0.013468\n",
      "Epoch 280 of 500, Train Loss: 0.013510\n",
      "Epoch 281 of 500, Train Loss: 0.013468\n",
      "Epoch 282 of 500, Train Loss: 0.013501\n",
      "Epoch 283 of 500, Train Loss: 0.013462\n",
      "Epoch 284 of 500, Train Loss: 0.013487\n",
      "Epoch 285 of 500, Train Loss: 0.013452\n",
      "Epoch 286 of 500, Train Loss: 0.013472\n",
      "Epoch 287 of 500, Train Loss: 0.013443\n",
      "Epoch 288 of 500, Train Loss: 0.013461\n",
      "Epoch 289 of 500, Train Loss: 0.013436\n",
      "Epoch 290 of 500, Train Loss: 0.013457\n",
      "Epoch 291 of 500, Train Loss: 0.013434\n",
      "Epoch 292 of 500, Train Loss: 0.013459\n",
      "Epoch 293 of 500, Train Loss: 0.013435\n",
      "Epoch 294 of 500, Train Loss: 0.013465\n",
      "Epoch 295 of 500, Train Loss: 0.013440\n",
      "Epoch 296 of 500, Train Loss: 0.013474\n",
      "Epoch 297 of 500, Train Loss: 0.013449\n",
      "Epoch 298 of 500, Train Loss: 0.013487\n",
      "Epoch 299 of 500, Train Loss: 0.013461\n",
      "Epoch 300 of 500, Train Loss: 0.013502\n",
      "Epoch 301 of 500, Train Loss: 0.013476\n",
      "Epoch 302 of 500, Train Loss: 0.013517\n",
      "Epoch 303 of 500, Train Loss: 0.013492\n",
      "Epoch 304 of 500, Train Loss: 0.013531\n",
      "Epoch 305 of 500, Train Loss: 0.013506\n",
      "Epoch 306 of 500, Train Loss: 0.013542\n",
      "Epoch 307 of 500, Train Loss: 0.013516\n",
      "Epoch 308 of 500, Train Loss: 0.013548\n",
      "Epoch 309 of 500, Train Loss: 0.013519\n",
      "Epoch 310 of 500, Train Loss: 0.013545\n",
      "Epoch 311 of 500, Train Loss: 0.013516\n",
      "Epoch 312 of 500, Train Loss: 0.013537\n",
      "Epoch 313 of 500, Train Loss: 0.013508\n",
      "Epoch 314 of 500, Train Loss: 0.013525\n",
      "Epoch 315 of 500, Train Loss: 0.013497\n",
      "Epoch 316 of 500, Train Loss: 0.013512\n",
      "Epoch 317 of 500, Train Loss: 0.013486\n",
      "Epoch 318 of 500, Train Loss: 0.013498\n",
      "Epoch 319 of 500, Train Loss: 0.013474\n",
      "Epoch 320 of 500, Train Loss: 0.013485\n",
      "Epoch 321 of 500, Train Loss: 0.013464\n",
      "Epoch 322 of 500, Train Loss: 0.013475\n",
      "Epoch 323 of 500, Train Loss: 0.013455\n",
      "Epoch 324 of 500, Train Loss: 0.013466\n",
      "Epoch 325 of 500, Train Loss: 0.013448\n",
      "Epoch 326 of 500, Train Loss: 0.013461\n",
      "Epoch 327 of 500, Train Loss: 0.013444\n",
      "Epoch 328 of 500, Train Loss: 0.013456\n",
      "Epoch 329 of 500, Train Loss: 0.013440\n",
      "Epoch 330 of 500, Train Loss: 0.013452\n",
      "Epoch 331 of 500, Train Loss: 0.013436\n",
      "Epoch 332 of 500, Train Loss: 0.013447\n",
      "Epoch 333 of 500, Train Loss: 0.013432\n",
      "Epoch 334 of 500, Train Loss: 0.013441\n",
      "Epoch 335 of 500, Train Loss: 0.013427\n",
      "Epoch 336 of 500, Train Loss: 0.013435\n",
      "Epoch 337 of 500, Train Loss: 0.013422\n",
      "Epoch 338 of 500, Train Loss: 0.013428\n",
      "Epoch 339 of 500, Train Loss: 0.013416\n",
      "best loss:  0.013416085988075595\n",
      "Epoch 340 of 500, Train Loss: 0.013421\n",
      "Epoch 341 of 500, Train Loss: 0.013411\n",
      "best loss:  0.01341068566494347\n",
      "Epoch 342 of 500, Train Loss: 0.013415\n",
      "Epoch 343 of 500, Train Loss: 0.013406\n",
      "best loss:  0.013406041407436061\n",
      "Epoch 344 of 500, Train Loss: 0.013410\n",
      "Epoch 345 of 500, Train Loss: 0.013402\n",
      "best loss:  0.013402170830364737\n",
      "Epoch 346 of 500, Train Loss: 0.013406\n",
      "Epoch 347 of 500, Train Loss: 0.013399\n",
      "best loss:  0.013399017597738178\n",
      "Epoch 348 of 500, Train Loss: 0.013403\n",
      "Epoch 349 of 500, Train Loss: 0.013397\n",
      "best loss:  0.01339652564452398\n",
      "Epoch 350 of 500, Train Loss: 0.013401\n",
      "Epoch 351 of 500, Train Loss: 0.013395\n",
      "best loss:  0.013394651344531511\n",
      "Epoch 352 of 500, Train Loss: 0.013400\n",
      "Epoch 353 of 500, Train Loss: 0.013394\n",
      "best loss:  0.013393550421076993\n",
      "Epoch 354 of 500, Train Loss: 0.013399\n",
      "Epoch 355 of 500, Train Loss: 0.013393\n",
      "best loss:  0.013393263395942474\n",
      "Epoch 356 of 500, Train Loss: 0.013400\n",
      "Epoch 357 of 500, Train Loss: 0.013394\n",
      "Epoch 358 of 500, Train Loss: 0.013401\n",
      "Epoch 359 of 500, Train Loss: 0.013395\n",
      "Epoch 360 of 500, Train Loss: 0.013402\n",
      "Epoch 361 of 500, Train Loss: 0.013396\n",
      "Epoch 362 of 500, Train Loss: 0.013404\n",
      "Epoch 363 of 500, Train Loss: 0.013398\n",
      "Epoch 364 of 500, Train Loss: 0.013407\n",
      "Epoch 365 of 500, Train Loss: 0.013400\n",
      "Epoch 366 of 500, Train Loss: 0.013410\n",
      "Epoch 367 of 500, Train Loss: 0.013402\n",
      "Epoch 368 of 500, Train Loss: 0.013412\n",
      "Epoch 369 of 500, Train Loss: 0.013404\n",
      "Epoch 370 of 500, Train Loss: 0.013414\n",
      "Epoch 371 of 500, Train Loss: 0.013405\n",
      "Epoch 372 of 500, Train Loss: 0.013416\n",
      "Epoch 373 of 500, Train Loss: 0.013405\n",
      "Epoch 374 of 500, Train Loss: 0.013415\n",
      "Epoch 375 of 500, Train Loss: 0.013404\n",
      "Epoch 376 of 500, Train Loss: 0.013414\n",
      "Epoch 377 of 500, Train Loss: 0.013402\n",
      "Epoch 378 of 500, Train Loss: 0.013412\n",
      "Epoch 379 of 500, Train Loss: 0.013400\n",
      "Epoch 380 of 500, Train Loss: 0.013410\n",
      "Epoch 381 of 500, Train Loss: 0.013397\n",
      "Epoch 382 of 500, Train Loss: 0.013407\n",
      "Epoch 383 of 500, Train Loss: 0.013395\n",
      "Epoch 384 of 500, Train Loss: 0.013405\n",
      "Epoch 385 of 500, Train Loss: 0.013392\n",
      "best loss:  0.013392444710832003\n",
      "Epoch 386 of 500, Train Loss: 0.013403\n",
      "Epoch 387 of 500, Train Loss: 0.013391\n",
      "best loss:  0.013390532720025427\n",
      "Epoch 388 of 500, Train Loss: 0.013402\n",
      "Epoch 389 of 500, Train Loss: 0.013389\n",
      "best loss:  0.013389271749643782\n",
      "Epoch 390 of 500, Train Loss: 0.013401\n",
      "Epoch 391 of 500, Train Loss: 0.013389\n",
      "best loss:  0.013388642111735405\n",
      "Epoch 392 of 500, Train Loss: 0.013401\n",
      "Epoch 393 of 500, Train Loss: 0.013388\n",
      "best loss:  0.013388257910715878\n",
      "Epoch 394 of 500, Train Loss: 0.013401\n",
      "Epoch 395 of 500, Train Loss: 0.013388\n",
      "best loss:  0.01338794685168329\n",
      "Epoch 396 of 500, Train Loss: 0.013401\n",
      "Epoch 397 of 500, Train Loss: 0.013388\n",
      "best loss:  0.013387782907550216\n",
      "Epoch 398 of 500, Train Loss: 0.013401\n",
      "Epoch 399 of 500, Train Loss: 0.013388\n",
      "Epoch 400 of 500, Train Loss: 0.013401\n",
      "Epoch 401 of 500, Train Loss: 0.013388\n",
      "Epoch 402 of 500, Train Loss: 0.013401\n",
      "Epoch 403 of 500, Train Loss: 0.013388\n",
      "Epoch 404 of 500, Train Loss: 0.013401\n",
      "Epoch 405 of 500, Train Loss: 0.013388\n",
      "Epoch 406 of 500, Train Loss: 0.013401\n",
      "Epoch 407 of 500, Train Loss: 0.013388\n",
      "Epoch 408 of 500, Train Loss: 0.013401\n",
      "Epoch 409 of 500, Train Loss: 0.013387\n",
      "best loss:  0.013387250639101159\n",
      "Epoch 410 of 500, Train Loss: 0.013399\n",
      "Epoch 411 of 500, Train Loss: 0.013386\n",
      "best loss:  0.013385999820541677\n",
      "Epoch 412 of 500, Train Loss: 0.013397\n",
      "Epoch 413 of 500, Train Loss: 0.013385\n",
      "best loss:  0.013385209661020442\n",
      "Epoch 414 of 500, Train Loss: 0.013396\n",
      "Epoch 415 of 500, Train Loss: 0.013385\n",
      "best loss:  0.013385001816804874\n",
      "Epoch 416 of 500, Train Loss: 0.013394\n",
      "Epoch 417 of 500, Train Loss: 0.013384\n",
      "best loss:  0.013384116876711564\n",
      "Epoch 418 of 500, Train Loss: 0.013393\n",
      "Epoch 419 of 500, Train Loss: 0.013384\n",
      "best loss:  0.013383735428372932\n",
      "Epoch 420 of 500, Train Loss: 0.013392\n",
      "Epoch 421 of 500, Train Loss: 0.013383\n",
      "best loss:  0.013383299690561167\n",
      "Epoch 422 of 500, Train Loss: 0.013390\n",
      "Epoch 423 of 500, Train Loss: 0.013383\n",
      "best loss:  0.013382667384801664\n",
      "Epoch 424 of 500, Train Loss: 0.013389\n",
      "Epoch 425 of 500, Train Loss: 0.013382\n",
      "best loss:  0.013382140926063754\n",
      "Epoch 426 of 500, Train Loss: 0.013387\n",
      "Epoch 427 of 500, Train Loss: 0.013381\n",
      "best loss:  0.013381385242849833\n",
      "Epoch 428 of 500, Train Loss: 0.013385\n",
      "Epoch 429 of 500, Train Loss: 0.013380\n",
      "best loss:  0.013380137346405176\n",
      "Epoch 430 of 500, Train Loss: 0.013383\n",
      "Epoch 431 of 500, Train Loss: 0.013378\n",
      "best loss:  0.013378139365060155\n",
      "Epoch 432 of 500, Train Loss: 0.013380\n",
      "Epoch 433 of 500, Train Loss: 0.013376\n",
      "best loss:  0.01337588308293322\n",
      "Epoch 434 of 500, Train Loss: 0.013377\n",
      "Epoch 435 of 500, Train Loss: 0.013373\n",
      "best loss:  0.013373050509316689\n",
      "Epoch 436 of 500, Train Loss: 0.013374\n",
      "Epoch 437 of 500, Train Loss: 0.013370\n",
      "best loss:  0.013370202971819081\n",
      "Epoch 438 of 500, Train Loss: 0.013371\n",
      "Epoch 439 of 500, Train Loss: 0.013368\n",
      "best loss:  0.01336758542271663\n",
      "Epoch 440 of 500, Train Loss: 0.013369\n",
      "Epoch 441 of 500, Train Loss: 0.013365\n",
      "best loss:  0.013364602875256868\n",
      "Epoch 442 of 500, Train Loss: 0.013366\n",
      "Epoch 443 of 500, Train Loss: 0.013362\n",
      "best loss:  0.01336232516526555\n",
      "Epoch 444 of 500, Train Loss: 0.013363\n",
      "Epoch 445 of 500, Train Loss: 0.013359\n",
      "best loss:  0.01335878236410605\n",
      "Epoch 446 of 500, Train Loss: 0.013360\n",
      "Epoch 447 of 500, Train Loss: 0.013356\n",
      "best loss:  0.013356116364796668\n",
      "Epoch 448 of 500, Train Loss: 0.013357\n",
      "Epoch 449 of 500, Train Loss: 0.013353\n",
      "best loss:  0.013353032933759265\n",
      "Epoch 450 of 500, Train Loss: 0.013354\n",
      "Epoch 451 of 500, Train Loss: 0.013351\n",
      "best loss:  0.013350795219876161\n",
      "Epoch 452 of 500, Train Loss: 0.013353\n",
      "Epoch 453 of 500, Train Loss: 0.013349\n",
      "best loss:  0.013349097134101442\n",
      "Epoch 454 of 500, Train Loss: 0.013351\n",
      "Epoch 455 of 500, Train Loss: 0.013347\n",
      "best loss:  0.013347119384656422\n",
      "Epoch 456 of 500, Train Loss: 0.013349\n",
      "Epoch 457 of 500, Train Loss: 0.013345\n",
      "best loss:  0.013345138747615075\n",
      "Epoch 458 of 500, Train Loss: 0.013347\n",
      "Epoch 459 of 500, Train Loss: 0.013345\n",
      "best loss:  0.013344575865771766\n",
      "Epoch 460 of 500, Train Loss: 0.013347\n",
      "Epoch 461 of 500, Train Loss: 0.013344\n",
      "best loss:  0.01334355589112469\n",
      "Epoch 462 of 500, Train Loss: 0.013346\n",
      "Epoch 463 of 500, Train Loss: 0.013342\n",
      "best loss:  0.01334226404744482\n",
      "Epoch 464 of 500, Train Loss: 0.013346\n",
      "Epoch 465 of 500, Train Loss: 0.013342\n",
      "best loss:  0.013341823742136561\n",
      "Epoch 466 of 500, Train Loss: 0.013346\n",
      "Epoch 467 of 500, Train Loss: 0.013342\n",
      "best loss:  0.013341598513007322\n",
      "Epoch 468 of 500, Train Loss: 0.013346\n",
      "Epoch 469 of 500, Train Loss: 0.013342\n",
      "Epoch 470 of 500, Train Loss: 0.013349\n",
      "Epoch 471 of 500, Train Loss: 0.013344\n",
      "Epoch 472 of 500, Train Loss: 0.013351\n",
      "Epoch 473 of 500, Train Loss: 0.013346\n",
      "Epoch 474 of 500, Train Loss: 0.013353\n",
      "Epoch 475 of 500, Train Loss: 0.013349\n",
      "Epoch 476 of 500, Train Loss: 0.013356\n",
      "Epoch 477 of 500, Train Loss: 0.013352\n",
      "Epoch 478 of 500, Train Loss: 0.013358\n",
      "Epoch 479 of 500, Train Loss: 0.013356\n",
      "Epoch 480 of 500, Train Loss: 0.013362\n",
      "Epoch 481 of 500, Train Loss: 0.013361\n",
      "Epoch 482 of 500, Train Loss: 0.013365\n",
      "Epoch 483 of 500, Train Loss: 0.013365\n",
      "Epoch 484 of 500, Train Loss: 0.013367\n",
      "Epoch 485 of 500, Train Loss: 0.013368\n",
      "Epoch 486 of 500, Train Loss: 0.013367\n",
      "Epoch 487 of 500, Train Loss: 0.013368\n",
      "Epoch 488 of 500, Train Loss: 0.013366\n",
      "Epoch 489 of 500, Train Loss: 0.013368\n",
      "Epoch 490 of 500, Train Loss: 0.013366\n",
      "Epoch 491 of 500, Train Loss: 0.013369\n",
      "Epoch 492 of 500, Train Loss: 0.013367\n",
      "Epoch 493 of 500, Train Loss: 0.013371\n",
      "Epoch 494 of 500, Train Loss: 0.013372\n",
      "Epoch 495 of 500, Train Loss: 0.013378\n",
      "Epoch 496 of 500, Train Loss: 0.013379\n",
      "Epoch 497 of 500, Train Loss: 0.013385\n",
      "Epoch 498 of 500, Train Loss: 0.013386\n",
      "Epoch 499 of 500, Train Loss: 0.013391\n",
      "Epoch 500 of 500, Train Loss: 0.013393\n",
      "latent train shape:  (16395, 30)\n",
      "M: 30, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 43\n",
      "Training the subspace: 0 / 30\n",
      "Training the subspace: 1 / 30\n",
      "Training the subspace: 2 / 30\n",
      "Training the subspace: 3 / 30\n",
      "Training the subspace: 4 / 30\n",
      "Training the subspace: 5 / 30\n",
      "Training the subspace: 6 / 30\n",
      "Training the subspace: 7 / 30\n",
      "Training the subspace: 8 / 30\n",
      "Training the subspace: 9 / 30\n",
      "Training the subspace: 10 / 30\n",
      "Training the subspace: 11 / 30\n",
      "Training the subspace: 12 / 30\n",
      "Training the subspace: 13 / 30\n",
      "Training the subspace: 14 / 30\n",
      "Training the subspace: 15 / 30\n",
      "Training the subspace: 16 / 30\n",
      "Training the subspace: 17 / 30\n",
      "Training the subspace: 18 / 30\n",
      "Training the subspace: 19 / 30\n",
      "Training the subspace: 20 / 30\n",
      "Training the subspace: 21 / 30\n",
      "Training the subspace: 22 / 30\n",
      "Training the subspace: 23 / 30\n",
      "Training the subspace: 24 / 30\n",
      "Training the subspace: 25 / 30\n",
      "Training the subspace: 26 / 30\n",
      "Training the subspace: 27 / 30\n",
      "Training the subspace: 28 / 30\n",
      "Training the subspace: 29 / 30\n",
      "Encoding the subspace: 0 / 30\n",
      "Encoding the subspace: 1 / 30\n",
      "Encoding the subspace: 2 / 30\n",
      "Encoding the subspace: 3 / 30\n",
      "Encoding the subspace: 4 / 30\n",
      "Encoding the subspace: 5 / 30\n",
      "Encoding the subspace: 6 / 30\n",
      "Encoding the subspace: 7 / 30\n",
      "Encoding the subspace: 8 / 30\n",
      "Encoding the subspace: 9 / 30\n",
      "Encoding the subspace: 10 / 30\n",
      "Encoding the subspace: 11 / 30\n",
      "Encoding the subspace: 12 / 30\n",
      "Encoding the subspace: 13 / 30\n",
      "Encoding the subspace: 14 / 30\n",
      "Encoding the subspace: 15 / 30\n",
      "Encoding the subspace: 16 / 30\n",
      "Encoding the subspace: 17 / 30\n",
      "Encoding the subspace: 18 / 30\n",
      "Encoding the subspace: 19 / 30\n",
      "Encoding the subspace: 20 / 30\n",
      "Encoding the subspace: 21 / 30\n",
      "Encoding the subspace: 22 / 30\n",
      "Encoding the subspace: 23 / 30\n",
      "Encoding the subspace: 24 / 30\n",
      "Encoding the subspace: 25 / 30\n",
      "Encoding the subspace: 26 / 30\n",
      "Encoding the subspace: 27 / 30\n",
      "Encoding the subspace: 28 / 30\n",
      "Encoding the subspace: 29 / 30\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=45, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.433267\n",
      "best loss:  0.43326748054214687\n",
      "Epoch 2 of 500, Train Loss: 0.056239\n",
      "best loss:  0.056239443695327886\n",
      "Epoch 3 of 500, Train Loss: 0.053028\n",
      "best loss:  0.05302794486042401\n",
      "Epoch 4 of 500, Train Loss: 0.048894\n",
      "best loss:  0.048894127100785514\n",
      "Epoch 5 of 500, Train Loss: 0.043167\n",
      "best loss:  0.04316667672661819\n",
      "Epoch 6 of 500, Train Loss: 0.037410\n",
      "best loss:  0.03741007542387445\n",
      "Epoch 7 of 500, Train Loss: 0.032904\n",
      "best loss:  0.03290363127643661\n",
      "Epoch 8 of 500, Train Loss: 0.029757\n",
      "best loss:  0.029756787363006625\n",
      "Epoch 9 of 500, Train Loss: 0.027602\n",
      "best loss:  0.027602390701568492\n",
      "Epoch 10 of 500, Train Loss: 0.026025\n",
      "best loss:  0.02602547799758685\n",
      "Epoch 11 of 500, Train Loss: 0.024758\n",
      "best loss:  0.024758383179130393\n",
      "Epoch 12 of 500, Train Loss: 0.023665\n",
      "best loss:  0.02366482185618591\n",
      "Epoch 13 of 500, Train Loss: 0.022684\n",
      "best loss:  0.022683531993931492\n",
      "Epoch 14 of 500, Train Loss: 0.021805\n",
      "best loss:  0.02180512819927395\n",
      "Epoch 15 of 500, Train Loss: 0.021025\n",
      "best loss:  0.021024850813957118\n",
      "Epoch 16 of 500, Train Loss: 0.020336\n",
      "best loss:  0.02033602042699435\n",
      "Epoch 17 of 500, Train Loss: 0.019727\n",
      "best loss:  0.01972743674057767\n",
      "Epoch 18 of 500, Train Loss: 0.019183\n",
      "best loss:  0.019182514275655208\n",
      "Epoch 19 of 500, Train Loss: 0.018689\n",
      "best loss:  0.018688920262637874\n",
      "Epoch 20 of 500, Train Loss: 0.018232\n",
      "best loss:  0.018231590467283156\n",
      "Epoch 21 of 500, Train Loss: 0.017801\n",
      "best loss:  0.01780136562152874\n",
      "Epoch 22 of 500, Train Loss: 0.017393\n",
      "best loss:  0.01739290424116324\n",
      "Epoch 23 of 500, Train Loss: 0.017013\n",
      "best loss:  0.017012665416890133\n",
      "Epoch 24 of 500, Train Loss: 0.016652\n",
      "best loss:  0.016651913357173028\n",
      "Epoch 25 of 500, Train Loss: 0.016319\n",
      "best loss:  0.01631862252892912\n",
      "Epoch 26 of 500, Train Loss: 0.016010\n",
      "best loss:  0.01600978178691386\n",
      "Epoch 27 of 500, Train Loss: 0.015724\n",
      "best loss:  0.015724165405935513\n",
      "Epoch 28 of 500, Train Loss: 0.015468\n",
      "best loss:  0.01546784845165468\n",
      "Epoch 29 of 500, Train Loss: 0.015232\n",
      "best loss:  0.015231858595552341\n",
      "Epoch 30 of 500, Train Loss: 0.015019\n",
      "best loss:  0.01501880953645211\n",
      "Epoch 31 of 500, Train Loss: 0.014818\n",
      "best loss:  0.014817719179694721\n",
      "Epoch 32 of 500, Train Loss: 0.014637\n",
      "best loss:  0.014636926220092822\n",
      "Epoch 33 of 500, Train Loss: 0.014472\n",
      "best loss:  0.014471604267014335\n",
      "Epoch 34 of 500, Train Loss: 0.014316\n",
      "best loss:  0.014316360359693798\n",
      "Epoch 35 of 500, Train Loss: 0.014173\n",
      "best loss:  0.01417311312032452\n",
      "Epoch 36 of 500, Train Loss: 0.014038\n",
      "best loss:  0.014037773444934553\n",
      "Epoch 37 of 500, Train Loss: 0.013916\n",
      "best loss:  0.013915572244838212\n",
      "Epoch 38 of 500, Train Loss: 0.013806\n",
      "best loss:  0.013805735415450564\n",
      "Epoch 39 of 500, Train Loss: 0.013695\n",
      "best loss:  0.013695448106145775\n",
      "Epoch 40 of 500, Train Loss: 0.013590\n",
      "best loss:  0.013590097186641132\n",
      "Epoch 41 of 500, Train Loss: 0.013495\n",
      "best loss:  0.013495367291101883\n",
      "Epoch 42 of 500, Train Loss: 0.013407\n",
      "best loss:  0.013406912639625226\n",
      "Epoch 43 of 500, Train Loss: 0.013324\n",
      "best loss:  0.013324276244542164\n",
      "Epoch 44 of 500, Train Loss: 0.013242\n",
      "best loss:  0.013242369274926452\n",
      "Epoch 45 of 500, Train Loss: 0.013174\n",
      "best loss:  0.013174222766912665\n",
      "Epoch 46 of 500, Train Loss: 0.013104\n",
      "best loss:  0.013103555298641382\n",
      "Epoch 47 of 500, Train Loss: 0.013041\n",
      "best loss:  0.013041062528482285\n",
      "Epoch 48 of 500, Train Loss: 0.012982\n",
      "best loss:  0.01298235696668253\n",
      "Epoch 49 of 500, Train Loss: 0.012928\n",
      "best loss:  0.012928089274042973\n",
      "Epoch 50 of 500, Train Loss: 0.012871\n",
      "best loss:  0.01287140987241323\n",
      "Epoch 51 of 500, Train Loss: 0.012822\n",
      "best loss:  0.012822295602929034\n",
      "Epoch 52 of 500, Train Loss: 0.012776\n",
      "best loss:  0.012776166371651023\n",
      "Epoch 53 of 500, Train Loss: 0.012728\n",
      "best loss:  0.012727583062974014\n",
      "Epoch 54 of 500, Train Loss: 0.012676\n",
      "best loss:  0.012676427738926944\n",
      "Epoch 55 of 500, Train Loss: 0.012638\n",
      "best loss:  0.0126382744062941\n",
      "Epoch 56 of 500, Train Loss: 0.012601\n",
      "best loss:  0.012600891877781571\n",
      "Epoch 57 of 500, Train Loss: 0.012561\n",
      "best loss:  0.012561133656607465\n",
      "Epoch 58 of 500, Train Loss: 0.012522\n",
      "best loss:  0.012522495658337247\n",
      "Epoch 59 of 500, Train Loss: 0.012497\n",
      "best loss:  0.01249667410305593\n",
      "Epoch 60 of 500, Train Loss: 0.012457\n",
      "best loss:  0.012456578373333237\n",
      "Epoch 61 of 500, Train Loss: 0.012428\n",
      "best loss:  0.012427572193672852\n",
      "Epoch 62 of 500, Train Loss: 0.012393\n",
      "best loss:  0.012392991423460677\n",
      "Epoch 63 of 500, Train Loss: 0.012370\n",
      "best loss:  0.012369804767676983\n",
      "Epoch 64 of 500, Train Loss: 0.012335\n",
      "best loss:  0.012335167143019205\n",
      "Epoch 65 of 500, Train Loss: 0.012314\n",
      "best loss:  0.012313654133914645\n",
      "Epoch 66 of 500, Train Loss: 0.012289\n",
      "best loss:  0.01228856914942947\n",
      "Epoch 67 of 500, Train Loss: 0.012263\n",
      "best loss:  0.012263425989771377\n",
      "Epoch 68 of 500, Train Loss: 0.012240\n",
      "best loss:  0.012239821062760803\n",
      "Epoch 69 of 500, Train Loss: 0.012217\n",
      "best loss:  0.01221708195891354\n",
      "Epoch 70 of 500, Train Loss: 0.012198\n",
      "best loss:  0.012197727937830699\n",
      "Epoch 71 of 500, Train Loss: 0.012177\n",
      "best loss:  0.012176631764840884\n",
      "Epoch 72 of 500, Train Loss: 0.012160\n",
      "best loss:  0.012159881392058865\n",
      "Epoch 73 of 500, Train Loss: 0.012141\n",
      "best loss:  0.012141380589924637\n",
      "Epoch 74 of 500, Train Loss: 0.012128\n",
      "best loss:  0.012128029295014102\n",
      "Epoch 75 of 500, Train Loss: 0.012112\n",
      "best loss:  0.012112263248296628\n",
      "Epoch 76 of 500, Train Loss: 0.012094\n",
      "best loss:  0.01209424349344913\n",
      "Epoch 77 of 500, Train Loss: 0.012086\n",
      "best loss:  0.012085871399622122\n",
      "Epoch 78 of 500, Train Loss: 0.012068\n",
      "best loss:  0.012068286206250172\n",
      "Epoch 79 of 500, Train Loss: 0.012054\n",
      "best loss:  0.012053536936368202\n",
      "Epoch 80 of 500, Train Loss: 0.012044\n",
      "best loss:  0.012043621632913056\n",
      "Epoch 81 of 500, Train Loss: 0.012030\n",
      "best loss:  0.012030218662459461\n",
      "Epoch 82 of 500, Train Loss: 0.012020\n",
      "best loss:  0.012020424488695454\n",
      "Epoch 83 of 500, Train Loss: 0.012009\n",
      "best loss:  0.012009399402151138\n",
      "Epoch 84 of 500, Train Loss: 0.011999\n",
      "best loss:  0.011998617215273368\n",
      "Epoch 85 of 500, Train Loss: 0.011988\n",
      "best loss:  0.011988436280932033\n",
      "Epoch 86 of 500, Train Loss: 0.011979\n",
      "best loss:  0.011978591757654141\n",
      "Epoch 87 of 500, Train Loss: 0.011967\n",
      "best loss:  0.01196680679333454\n",
      "Epoch 88 of 500, Train Loss: 0.011958\n",
      "best loss:  0.011958236107377104\n",
      "Epoch 89 of 500, Train Loss: 0.011947\n",
      "best loss:  0.011946961876476913\n",
      "Epoch 90 of 500, Train Loss: 0.011935\n",
      "best loss:  0.011934690876158185\n",
      "Epoch 91 of 500, Train Loss: 0.011921\n",
      "best loss:  0.011920834251665041\n",
      "Epoch 92 of 500, Train Loss: 0.011909\n",
      "best loss:  0.011908548256556517\n",
      "Epoch 93 of 500, Train Loss: 0.011898\n",
      "best loss:  0.011897877165316713\n",
      "Epoch 94 of 500, Train Loss: 0.011888\n",
      "best loss:  0.011888061108856368\n",
      "Epoch 95 of 500, Train Loss: 0.011883\n",
      "best loss:  0.011882595627549711\n",
      "Epoch 96 of 500, Train Loss: 0.011878\n",
      "best loss:  0.01187772030322614\n",
      "Epoch 97 of 500, Train Loss: 0.011870\n",
      "best loss:  0.011870349748461694\n",
      "Epoch 98 of 500, Train Loss: 0.011862\n",
      "best loss:  0.011861564142222503\n",
      "Epoch 99 of 500, Train Loss: 0.011857\n",
      "best loss:  0.011857480145859994\n",
      "Epoch 100 of 500, Train Loss: 0.011857\n",
      "best loss:  0.011857184023635151\n",
      "Epoch 101 of 500, Train Loss: 0.011862\n",
      "Epoch 102 of 500, Train Loss: 0.011871\n",
      "Epoch 103 of 500, Train Loss: 0.011876\n",
      "Epoch 104 of 500, Train Loss: 0.011879\n",
      "Epoch 105 of 500, Train Loss: 0.011878\n",
      "Epoch 106 of 500, Train Loss: 0.011875\n",
      "Epoch 107 of 500, Train Loss: 0.011869\n",
      "Epoch 108 of 500, Train Loss: 0.011858\n",
      "Epoch 109 of 500, Train Loss: 0.011849\n",
      "best loss:  0.011848616349585752\n",
      "Epoch 110 of 500, Train Loss: 0.011844\n",
      "best loss:  0.011843607953073666\n",
      "Epoch 111 of 500, Train Loss: 0.011838\n",
      "best loss:  0.01183791013724214\n",
      "Epoch 112 of 500, Train Loss: 0.011833\n",
      "best loss:  0.011833429498987835\n",
      "Epoch 113 of 500, Train Loss: 0.011830\n",
      "best loss:  0.011829518911256384\n",
      "Epoch 114 of 500, Train Loss: 0.011827\n",
      "best loss:  0.011826739129168162\n",
      "Epoch 115 of 500, Train Loss: 0.011828\n",
      "Epoch 116 of 500, Train Loss: 0.011829\n",
      "Epoch 117 of 500, Train Loss: 0.011835\n",
      "Epoch 118 of 500, Train Loss: 0.011839\n",
      "Epoch 119 of 500, Train Loss: 0.011841\n",
      "Epoch 120 of 500, Train Loss: 0.011843\n",
      "Epoch 121 of 500, Train Loss: 0.011845\n",
      "Epoch 122 of 500, Train Loss: 0.011847\n",
      "Epoch 123 of 500, Train Loss: 0.011855\n",
      "Epoch 124 of 500, Train Loss: 0.011855\n",
      "Epoch 125 of 500, Train Loss: 0.011860\n",
      "Epoch 126 of 500, Train Loss: 0.011854\n",
      "Epoch 127 of 500, Train Loss: 0.011848\n",
      "Epoch 128 of 500, Train Loss: 0.011843\n",
      "Epoch 129 of 500, Train Loss: 0.011847\n",
      "Epoch 130 of 500, Train Loss: 0.011841\n",
      "Epoch 131 of 500, Train Loss: 0.011846\n",
      "Epoch 132 of 500, Train Loss: 0.011834\n",
      "Epoch 133 of 500, Train Loss: 0.011839\n",
      "Epoch 134 of 500, Train Loss: 0.011836\n",
      "Epoch 135 of 500, Train Loss: 0.011847\n",
      "Epoch 136 of 500, Train Loss: 0.011839\n",
      "Epoch 137 of 500, Train Loss: 0.011838\n",
      "Epoch 138 of 500, Train Loss: 0.011829\n",
      "Epoch 139 of 500, Train Loss: 0.011834\n",
      "Epoch 140 of 500, Train Loss: 0.011830\n",
      "Epoch 141 of 500, Train Loss: 0.011838\n",
      "Epoch 142 of 500, Train Loss: 0.011825\n",
      "best loss:  0.01182512249113981\n",
      "Epoch 143 of 500, Train Loss: 0.011830\n",
      "Epoch 144 of 500, Train Loss: 0.011820\n",
      "best loss:  0.011820044523049075\n",
      "Epoch 145 of 500, Train Loss: 0.011836\n",
      "Epoch 146 of 500, Train Loss: 0.011839\n",
      "Epoch 147 of 500, Train Loss: 0.011869\n",
      "Epoch 148 of 500, Train Loss: 0.011915\n",
      "Epoch 149 of 500, Train Loss: 0.011985\n",
      "Epoch 150 of 500, Train Loss: 0.012188\n",
      "Epoch 151 of 500, Train Loss: 0.012246\n",
      "Epoch 152 of 500, Train Loss: 0.012617\n",
      "Epoch 153 of 500, Train Loss: 0.012435\n",
      "Epoch 154 of 500, Train Loss: 0.012568\n",
      "Epoch 155 of 500, Train Loss: 0.012409\n",
      "Epoch 156 of 500, Train Loss: 0.012402\n",
      "Epoch 157 of 500, Train Loss: 0.012332\n",
      "Epoch 158 of 500, Train Loss: 0.012217\n",
      "Epoch 159 of 500, Train Loss: 0.012215\n",
      "Epoch 160 of 500, Train Loss: 0.012091\n",
      "Epoch 161 of 500, Train Loss: 0.012086\n",
      "Epoch 162 of 500, Train Loss: 0.011973\n",
      "Epoch 163 of 500, Train Loss: 0.011964\n",
      "Epoch 164 of 500, Train Loss: 0.011883\n",
      "Epoch 165 of 500, Train Loss: 0.011860\n",
      "Epoch 166 of 500, Train Loss: 0.011819\n",
      "best loss:  0.011818698277428942\n",
      "Epoch 167 of 500, Train Loss: 0.011805\n",
      "best loss:  0.011805075289236648\n",
      "Epoch 168 of 500, Train Loss: 0.011801\n",
      "best loss:  0.011801005958497358\n",
      "Epoch 169 of 500, Train Loss: 0.011791\n",
      "best loss:  0.011790620026475563\n",
      "Epoch 170 of 500, Train Loss: 0.011810\n",
      "Epoch 171 of 500, Train Loss: 0.011827\n",
      "Epoch 172 of 500, Train Loss: 0.011891\n",
      "Epoch 173 of 500, Train Loss: 0.011916\n",
      "Epoch 174 of 500, Train Loss: 0.012001\n",
      "Epoch 175 of 500, Train Loss: 0.011999\n",
      "Epoch 176 of 500, Train Loss: 0.012025\n",
      "Epoch 177 of 500, Train Loss: 0.012011\n",
      "Epoch 178 of 500, Train Loss: 0.012001\n",
      "Epoch 179 of 500, Train Loss: 0.011997\n",
      "Epoch 180 of 500, Train Loss: 0.011963\n",
      "Epoch 181 of 500, Train Loss: 0.011961\n",
      "Epoch 182 of 500, Train Loss: 0.011900\n",
      "Epoch 183 of 500, Train Loss: 0.011904\n",
      "Epoch 184 of 500, Train Loss: 0.011844\n",
      "Epoch 185 of 500, Train Loss: 0.011847\n",
      "Epoch 186 of 500, Train Loss: 0.011799\n",
      "Epoch 187 of 500, Train Loss: 0.011808\n",
      "Epoch 188 of 500, Train Loss: 0.011779\n",
      "best loss:  0.011779037366716856\n",
      "Epoch 189 of 500, Train Loss: 0.011791\n",
      "Epoch 190 of 500, Train Loss: 0.011777\n",
      "best loss:  0.011777445337669237\n",
      "Epoch 191 of 500, Train Loss: 0.011797\n",
      "Epoch 192 of 500, Train Loss: 0.011798\n",
      "Epoch 193 of 500, Train Loss: 0.011812\n",
      "Epoch 194 of 500, Train Loss: 0.011814\n",
      "Epoch 195 of 500, Train Loss: 0.011824\n",
      "Epoch 196 of 500, Train Loss: 0.011819\n",
      "Epoch 197 of 500, Train Loss: 0.011821\n",
      "Epoch 198 of 500, Train Loss: 0.011808\n",
      "Epoch 199 of 500, Train Loss: 0.011811\n",
      "Epoch 200 of 500, Train Loss: 0.011794\n",
      "Epoch 201 of 500, Train Loss: 0.011800\n",
      "Epoch 202 of 500, Train Loss: 0.011780\n",
      "Epoch 203 of 500, Train Loss: 0.011790\n",
      "Epoch 204 of 500, Train Loss: 0.011772\n",
      "best loss:  0.01177204769851943\n",
      "Epoch 205 of 500, Train Loss: 0.011783\n",
      "Epoch 206 of 500, Train Loss: 0.011768\n",
      "best loss:  0.011768105391786585\n",
      "Epoch 207 of 500, Train Loss: 0.011779\n",
      "Epoch 208 of 500, Train Loss: 0.011760\n",
      "best loss:  0.011759509714550425\n",
      "Epoch 209 of 500, Train Loss: 0.011772\n",
      "Epoch 210 of 500, Train Loss: 0.011752\n",
      "best loss:  0.011751805981052606\n",
      "Epoch 211 of 500, Train Loss: 0.011764\n",
      "Epoch 212 of 500, Train Loss: 0.011745\n",
      "best loss:  0.011745185821742188\n",
      "Epoch 213 of 500, Train Loss: 0.011758\n",
      "Epoch 214 of 500, Train Loss: 0.011746\n",
      "Epoch 215 of 500, Train Loss: 0.011761\n",
      "Epoch 216 of 500, Train Loss: 0.011755\n",
      "Epoch 217 of 500, Train Loss: 0.011766\n",
      "Epoch 218 of 500, Train Loss: 0.011759\n",
      "Epoch 219 of 500, Train Loss: 0.011769\n",
      "Epoch 220 of 500, Train Loss: 0.011758\n",
      "Epoch 221 of 500, Train Loss: 0.011765\n",
      "Epoch 222 of 500, Train Loss: 0.011753\n",
      "Epoch 223 of 500, Train Loss: 0.011758\n",
      "Epoch 224 of 500, Train Loss: 0.011745\n",
      "Epoch 225 of 500, Train Loss: 0.011752\n",
      "Epoch 226 of 500, Train Loss: 0.011738\n",
      "best loss:  0.01173836379600761\n",
      "Epoch 227 of 500, Train Loss: 0.011749\n",
      "Epoch 228 of 500, Train Loss: 0.011737\n",
      "best loss:  0.011736749488203389\n",
      "Epoch 229 of 500, Train Loss: 0.011748\n",
      "Epoch 230 of 500, Train Loss: 0.011740\n",
      "Epoch 231 of 500, Train Loss: 0.011749\n",
      "Epoch 232 of 500, Train Loss: 0.011740\n",
      "Epoch 233 of 500, Train Loss: 0.011749\n",
      "Epoch 234 of 500, Train Loss: 0.011741\n",
      "Epoch 235 of 500, Train Loss: 0.011750\n",
      "Epoch 236 of 500, Train Loss: 0.011741\n",
      "Epoch 237 of 500, Train Loss: 0.011749\n",
      "Epoch 238 of 500, Train Loss: 0.011739\n",
      "Epoch 239 of 500, Train Loss: 0.011747\n",
      "Epoch 240 of 500, Train Loss: 0.011736\n",
      "best loss:  0.011736004746676054\n",
      "Epoch 241 of 500, Train Loss: 0.011742\n",
      "Epoch 242 of 500, Train Loss: 0.011730\n",
      "best loss:  0.011730444869351422\n",
      "Epoch 243 of 500, Train Loss: 0.011736\n",
      "Epoch 244 of 500, Train Loss: 0.011723\n",
      "best loss:  0.011722806118164291\n",
      "Epoch 245 of 500, Train Loss: 0.011731\n",
      "Epoch 246 of 500, Train Loss: 0.011719\n",
      "best loss:  0.011718952653130025\n",
      "Epoch 247 of 500, Train Loss: 0.011726\n",
      "Epoch 248 of 500, Train Loss: 0.011719\n",
      "best loss:  0.011718835563456653\n",
      "Epoch 249 of 500, Train Loss: 0.011728\n",
      "Epoch 250 of 500, Train Loss: 0.011723\n",
      "Epoch 251 of 500, Train Loss: 0.011730\n",
      "Epoch 252 of 500, Train Loss: 0.011724\n",
      "Epoch 253 of 500, Train Loss: 0.011733\n",
      "Epoch 254 of 500, Train Loss: 0.011731\n",
      "Epoch 255 of 500, Train Loss: 0.011735\n",
      "Epoch 256 of 500, Train Loss: 0.011737\n",
      "Epoch 257 of 500, Train Loss: 0.011738\n",
      "Epoch 258 of 500, Train Loss: 0.011738\n",
      "Epoch 259 of 500, Train Loss: 0.011738\n",
      "Epoch 260 of 500, Train Loss: 0.011735\n",
      "Epoch 261 of 500, Train Loss: 0.011727\n",
      "Epoch 262 of 500, Train Loss: 0.011719\n",
      "Epoch 263 of 500, Train Loss: 0.011715\n",
      "best loss:  0.01171489369215321\n",
      "Epoch 264 of 500, Train Loss: 0.011701\n",
      "best loss:  0.011700775015416966\n",
      "Epoch 265 of 500, Train Loss: 0.011702\n",
      "Epoch 266 of 500, Train Loss: 0.011689\n",
      "best loss:  0.01168885375323431\n",
      "Epoch 267 of 500, Train Loss: 0.011693\n",
      "Epoch 268 of 500, Train Loss: 0.011683\n",
      "best loss:  0.011682533193830863\n",
      "Epoch 269 of 500, Train Loss: 0.011690\n",
      "Epoch 270 of 500, Train Loss: 0.011688\n",
      "Epoch 271 of 500, Train Loss: 0.011698\n",
      "Epoch 272 of 500, Train Loss: 0.011703\n",
      "Epoch 273 of 500, Train Loss: 0.011708\n",
      "Epoch 274 of 500, Train Loss: 0.011719\n",
      "Epoch 275 of 500, Train Loss: 0.011720\n",
      "Epoch 276 of 500, Train Loss: 0.011726\n",
      "Epoch 277 of 500, Train Loss: 0.011720\n",
      "Epoch 278 of 500, Train Loss: 0.011715\n",
      "Epoch 279 of 500, Train Loss: 0.011707\n",
      "Epoch 280 of 500, Train Loss: 0.011697\n",
      "Epoch 281 of 500, Train Loss: 0.011692\n",
      "Epoch 282 of 500, Train Loss: 0.011681\n",
      "best loss:  0.01168134399462318\n",
      "Epoch 283 of 500, Train Loss: 0.011678\n",
      "best loss:  0.01167786015139427\n",
      "Epoch 284 of 500, Train Loss: 0.011669\n",
      "best loss:  0.01166930825105695\n",
      "Epoch 285 of 500, Train Loss: 0.011670\n",
      "Epoch 286 of 500, Train Loss: 0.011667\n",
      "best loss:  0.011666593964995867\n",
      "Epoch 287 of 500, Train Loss: 0.011675\n",
      "Epoch 288 of 500, Train Loss: 0.011679\n",
      "Epoch 289 of 500, Train Loss: 0.011686\n",
      "Epoch 290 of 500, Train Loss: 0.011689\n",
      "Epoch 291 of 500, Train Loss: 0.011693\n",
      "Epoch 292 of 500, Train Loss: 0.011694\n",
      "Epoch 293 of 500, Train Loss: 0.011697\n",
      "Epoch 294 of 500, Train Loss: 0.011692\n",
      "Epoch 295 of 500, Train Loss: 0.011692\n",
      "Epoch 296 of 500, Train Loss: 0.011683\n",
      "Epoch 297 of 500, Train Loss: 0.011682\n",
      "Epoch 298 of 500, Train Loss: 0.011672\n",
      "Epoch 299 of 500, Train Loss: 0.011670\n",
      "Epoch 300 of 500, Train Loss: 0.011661\n",
      "best loss:  0.011661278281781404\n",
      "Epoch 301 of 500, Train Loss: 0.011659\n",
      "best loss:  0.01165912928551078\n",
      "Epoch 302 of 500, Train Loss: 0.011654\n",
      "best loss:  0.011654299743317408\n",
      "Epoch 303 of 500, Train Loss: 0.011658\n",
      "Epoch 304 of 500, Train Loss: 0.011656\n",
      "Epoch 305 of 500, Train Loss: 0.011663\n",
      "Epoch 306 of 500, Train Loss: 0.011661\n",
      "Epoch 307 of 500, Train Loss: 0.011669\n",
      "Epoch 308 of 500, Train Loss: 0.011670\n",
      "Epoch 309 of 500, Train Loss: 0.011676\n",
      "Epoch 310 of 500, Train Loss: 0.011673\n",
      "Epoch 311 of 500, Train Loss: 0.011677\n",
      "Epoch 312 of 500, Train Loss: 0.011673\n",
      "Epoch 313 of 500, Train Loss: 0.011677\n",
      "Epoch 314 of 500, Train Loss: 0.011671\n",
      "Epoch 315 of 500, Train Loss: 0.011674\n",
      "Epoch 316 of 500, Train Loss: 0.011667\n",
      "Epoch 317 of 500, Train Loss: 0.011668\n",
      "Epoch 318 of 500, Train Loss: 0.011662\n",
      "Epoch 319 of 500, Train Loss: 0.011664\n",
      "Epoch 320 of 500, Train Loss: 0.011660\n",
      "Epoch 321 of 500, Train Loss: 0.011662\n",
      "Epoch 322 of 500, Train Loss: 0.011657\n",
      "Epoch 323 of 500, Train Loss: 0.011660\n",
      "Epoch 324 of 500, Train Loss: 0.011658\n",
      "Epoch 325 of 500, Train Loss: 0.011659\n",
      "Epoch 326 of 500, Train Loss: 0.011653\n",
      "best loss:  0.011653092371023716\n",
      "Epoch 327 of 500, Train Loss: 0.011654\n",
      "Epoch 328 of 500, Train Loss: 0.011649\n",
      "best loss:  0.011648673673504206\n",
      "Epoch 329 of 500, Train Loss: 0.011650\n",
      "Epoch 330 of 500, Train Loss: 0.011646\n",
      "best loss:  0.011646016009170887\n",
      "Epoch 331 of 500, Train Loss: 0.011647\n",
      "Epoch 332 of 500, Train Loss: 0.011643\n",
      "best loss:  0.01164258641694842\n",
      "Epoch 333 of 500, Train Loss: 0.011644\n",
      "Epoch 334 of 500, Train Loss: 0.011641\n",
      "best loss:  0.011641176456049474\n",
      "Epoch 335 of 500, Train Loss: 0.011645\n",
      "Epoch 336 of 500, Train Loss: 0.011642\n",
      "Epoch 337 of 500, Train Loss: 0.011643\n",
      "Epoch 338 of 500, Train Loss: 0.011641\n",
      "best loss:  0.011640915956123472\n",
      "Epoch 339 of 500, Train Loss: 0.011639\n",
      "best loss:  0.011639114649101818\n",
      "Epoch 340 of 500, Train Loss: 0.011637\n",
      "best loss:  0.011637354922203831\n",
      "Epoch 341 of 500, Train Loss: 0.011636\n",
      "best loss:  0.011635502282567971\n",
      "Epoch 342 of 500, Train Loss: 0.011635\n",
      "best loss:  0.011634881131359116\n",
      "Epoch 343 of 500, Train Loss: 0.011635\n",
      "Epoch 344 of 500, Train Loss: 0.011632\n",
      "best loss:  0.011632133001717017\n",
      "Epoch 345 of 500, Train Loss: 0.011633\n",
      "Epoch 346 of 500, Train Loss: 0.011632\n",
      "Epoch 347 of 500, Train Loss: 0.011632\n",
      "Epoch 348 of 500, Train Loss: 0.011633\n",
      "Epoch 349 of 500, Train Loss: 0.011634\n",
      "Epoch 350 of 500, Train Loss: 0.011633\n",
      "Epoch 351 of 500, Train Loss: 0.011635\n",
      "Epoch 352 of 500, Train Loss: 0.011633\n",
      "Epoch 353 of 500, Train Loss: 0.011635\n",
      "Epoch 354 of 500, Train Loss: 0.011633\n",
      "Epoch 355 of 500, Train Loss: 0.011633\n",
      "Epoch 356 of 500, Train Loss: 0.011630\n",
      "best loss:  0.0116300767520217\n",
      "Epoch 357 of 500, Train Loss: 0.011630\n",
      "best loss:  0.01162999402810621\n",
      "Epoch 358 of 500, Train Loss: 0.011627\n",
      "best loss:  0.011627139741001657\n",
      "Epoch 359 of 500, Train Loss: 0.011629\n",
      "Epoch 360 of 500, Train Loss: 0.011626\n",
      "best loss:  0.011625558035917465\n",
      "Epoch 361 of 500, Train Loss: 0.011626\n",
      "Epoch 362 of 500, Train Loss: 0.011624\n",
      "best loss:  0.01162413007407377\n",
      "Epoch 363 of 500, Train Loss: 0.011625\n",
      "Epoch 364 of 500, Train Loss: 0.011622\n",
      "best loss:  0.011622245335958667\n",
      "Epoch 365 of 500, Train Loss: 0.011624\n",
      "Epoch 366 of 500, Train Loss: 0.011622\n",
      "best loss:  0.011622013415881368\n",
      "Epoch 367 of 500, Train Loss: 0.011623\n",
      "Epoch 368 of 500, Train Loss: 0.011621\n",
      "best loss:  0.01162079811550121\n",
      "Epoch 369 of 500, Train Loss: 0.011625\n",
      "Epoch 370 of 500, Train Loss: 0.011621\n",
      "best loss:  0.011620579333096027\n",
      "Epoch 371 of 500, Train Loss: 0.011624\n",
      "Epoch 372 of 500, Train Loss: 0.011620\n",
      "best loss:  0.011620369603787007\n",
      "Epoch 373 of 500, Train Loss: 0.011624\n",
      "Epoch 374 of 500, Train Loss: 0.011622\n",
      "Epoch 375 of 500, Train Loss: 0.011625\n",
      "Epoch 376 of 500, Train Loss: 0.011622\n",
      "Epoch 377 of 500, Train Loss: 0.011626\n",
      "Epoch 378 of 500, Train Loss: 0.011624\n",
      "Epoch 379 of 500, Train Loss: 0.011626\n",
      "Epoch 380 of 500, Train Loss: 0.011624\n",
      "Epoch 381 of 500, Train Loss: 0.011626\n",
      "Epoch 382 of 500, Train Loss: 0.011623\n",
      "Epoch 383 of 500, Train Loss: 0.011627\n",
      "Epoch 384 of 500, Train Loss: 0.011623\n",
      "Epoch 385 of 500, Train Loss: 0.011626\n",
      "Epoch 386 of 500, Train Loss: 0.011623\n",
      "Epoch 387 of 500, Train Loss: 0.011625\n",
      "Epoch 388 of 500, Train Loss: 0.011620\n",
      "best loss:  0.011619669328851612\n",
      "Epoch 389 of 500, Train Loss: 0.011622\n",
      "Epoch 390 of 500, Train Loss: 0.011621\n",
      "Epoch 391 of 500, Train Loss: 0.011623\n",
      "Epoch 392 of 500, Train Loss: 0.011621\n",
      "Epoch 393 of 500, Train Loss: 0.011623\n",
      "Epoch 394 of 500, Train Loss: 0.011621\n",
      "Epoch 395 of 500, Train Loss: 0.011620\n",
      "Epoch 396 of 500, Train Loss: 0.011620\n",
      "Epoch 397 of 500, Train Loss: 0.011620\n",
      "Epoch 398 of 500, Train Loss: 0.011622\n",
      "Epoch 399 of 500, Train Loss: 0.011620\n",
      "Epoch 400 of 500, Train Loss: 0.011623\n",
      "Epoch 401 of 500, Train Loss: 0.011621\n",
      "Epoch 402 of 500, Train Loss: 0.011623\n",
      "Epoch 403 of 500, Train Loss: 0.011621\n",
      "Epoch 404 of 500, Train Loss: 0.011623\n",
      "Epoch 405 of 500, Train Loss: 0.011620\n",
      "Epoch 406 of 500, Train Loss: 0.011621\n",
      "Epoch 407 of 500, Train Loss: 0.011619\n",
      "best loss:  0.011619151867371117\n",
      "Epoch 408 of 500, Train Loss: 0.011621\n",
      "Epoch 409 of 500, Train Loss: 0.011619\n",
      "Epoch 410 of 500, Train Loss: 0.011621\n",
      "Epoch 411 of 500, Train Loss: 0.011619\n",
      "best loss:  0.01161873773802541\n",
      "Epoch 412 of 500, Train Loss: 0.011619\n",
      "Epoch 413 of 500, Train Loss: 0.011617\n",
      "best loss:  0.01161705868897595\n",
      "Epoch 414 of 500, Train Loss: 0.011618\n",
      "Epoch 415 of 500, Train Loss: 0.011617\n",
      "Epoch 416 of 500, Train Loss: 0.011619\n",
      "Epoch 417 of 500, Train Loss: 0.011618\n",
      "Epoch 418 of 500, Train Loss: 0.011619\n",
      "Epoch 419 of 500, Train Loss: 0.011619\n",
      "Epoch 420 of 500, Train Loss: 0.011619\n",
      "Epoch 421 of 500, Train Loss: 0.011620\n",
      "Epoch 422 of 500, Train Loss: 0.011620\n",
      "Epoch 423 of 500, Train Loss: 0.011619\n",
      "Epoch 424 of 500, Train Loss: 0.011621\n",
      "Epoch 425 of 500, Train Loss: 0.011620\n",
      "Epoch 426 of 500, Train Loss: 0.011619\n",
      "Epoch 427 of 500, Train Loss: 0.011620\n",
      "Epoch 428 of 500, Train Loss: 0.011620\n",
      "Epoch 429 of 500, Train Loss: 0.011620\n",
      "Epoch 430 of 500, Train Loss: 0.011621\n",
      "Epoch 431 of 500, Train Loss: 0.011621\n",
      "Epoch 432 of 500, Train Loss: 0.011622\n",
      "Epoch 433 of 500, Train Loss: 0.011621\n",
      "Epoch 434 of 500, Train Loss: 0.011623\n",
      "Epoch 435 of 500, Train Loss: 0.011623\n",
      "Epoch 436 of 500, Train Loss: 0.011625\n",
      "Epoch 437 of 500, Train Loss: 0.011624\n",
      "Epoch 438 of 500, Train Loss: 0.011625\n",
      "Epoch 439 of 500, Train Loss: 0.011623\n",
      "Epoch 440 of 500, Train Loss: 0.011627\n",
      "Epoch 441 of 500, Train Loss: 0.011625\n",
      "Epoch 442 of 500, Train Loss: 0.011627\n",
      "Epoch 443 of 500, Train Loss: 0.011625\n",
      "Epoch 444 of 500, Train Loss: 0.011629\n",
      "Epoch 445 of 500, Train Loss: 0.011625\n",
      "Epoch 446 of 500, Train Loss: 0.011629\n",
      "Epoch 447 of 500, Train Loss: 0.011625\n",
      "Epoch 448 of 500, Train Loss: 0.011629\n",
      "Epoch 449 of 500, Train Loss: 0.011627\n",
      "Epoch 450 of 500, Train Loss: 0.011630\n",
      "Epoch 451 of 500, Train Loss: 0.011627\n",
      "Epoch 452 of 500, Train Loss: 0.011630\n",
      "Epoch 453 of 500, Train Loss: 0.011628\n",
      "Epoch 454 of 500, Train Loss: 0.011631\n",
      "Epoch 455 of 500, Train Loss: 0.011629\n",
      "Epoch 456 of 500, Train Loss: 0.011633\n",
      "Epoch 457 of 500, Train Loss: 0.011630\n",
      "Epoch 458 of 500, Train Loss: 0.011633\n",
      "Epoch 459 of 500, Train Loss: 0.011629\n",
      "Epoch 460 of 500, Train Loss: 0.011633\n",
      "Epoch 461 of 500, Train Loss: 0.011629\n",
      "Epoch 462 of 500, Train Loss: 0.011635\n",
      "Epoch 463 of 500, Train Loss: 0.011630\n",
      "Epoch 464 of 500, Train Loss: 0.011636\n",
      "Epoch 465 of 500, Train Loss: 0.011631\n",
      "Epoch 466 of 500, Train Loss: 0.011636\n",
      "Epoch 467 of 500, Train Loss: 0.011630\n",
      "Epoch 468 of 500, Train Loss: 0.011637\n",
      "Epoch 469 of 500, Train Loss: 0.011629\n",
      "Epoch 470 of 500, Train Loss: 0.011636\n",
      "Epoch 471 of 500, Train Loss: 0.011629\n",
      "Epoch 472 of 500, Train Loss: 0.011639\n",
      "Epoch 473 of 500, Train Loss: 0.011629\n",
      "Epoch 474 of 500, Train Loss: 0.011640\n",
      "Epoch 475 of 500, Train Loss: 0.011630\n",
      "Epoch 476 of 500, Train Loss: 0.011642\n",
      "Epoch 477 of 500, Train Loss: 0.011632\n",
      "Epoch 478 of 500, Train Loss: 0.011645\n",
      "Epoch 479 of 500, Train Loss: 0.011634\n",
      "Epoch 480 of 500, Train Loss: 0.011648\n",
      "Epoch 481 of 500, Train Loss: 0.011637\n",
      "Epoch 482 of 500, Train Loss: 0.011652\n",
      "Epoch 483 of 500, Train Loss: 0.011640\n",
      "Epoch 484 of 500, Train Loss: 0.011656\n",
      "Epoch 485 of 500, Train Loss: 0.011643\n",
      "Epoch 486 of 500, Train Loss: 0.011658\n",
      "Epoch 487 of 500, Train Loss: 0.011643\n",
      "Epoch 488 of 500, Train Loss: 0.011658\n",
      "Epoch 489 of 500, Train Loss: 0.011643\n",
      "Epoch 490 of 500, Train Loss: 0.011659\n",
      "Epoch 491 of 500, Train Loss: 0.011645\n",
      "Epoch 492 of 500, Train Loss: 0.011661\n",
      "Epoch 493 of 500, Train Loss: 0.011646\n",
      "Epoch 494 of 500, Train Loss: 0.011661\n",
      "Epoch 495 of 500, Train Loss: 0.011647\n",
      "Epoch 496 of 500, Train Loss: 0.011662\n",
      "Epoch 497 of 500, Train Loss: 0.011648\n",
      "Epoch 498 of 500, Train Loss: 0.011662\n",
      "Epoch 499 of 500, Train Loss: 0.011648\n",
      "Epoch 500 of 500, Train Loss: 0.011660\n",
      "latent train shape:  (16395, 45)\n",
      "M: 45, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 80\n",
      "Training the subspace: 0 / 45\n",
      "Training the subspace: 1 / 45\n",
      "Training the subspace: 2 / 45\n",
      "Training the subspace: 3 / 45\n",
      "Training the subspace: 4 / 45\n",
      "Training the subspace: 5 / 45\n",
      "Training the subspace: 6 / 45\n",
      "Training the subspace: 7 / 45\n",
      "Training the subspace: 8 / 45\n",
      "Training the subspace: 9 / 45\n",
      "Training the subspace: 10 / 45\n",
      "Training the subspace: 11 / 45\n",
      "Training the subspace: 12 / 45\n",
      "Training the subspace: 13 / 45\n",
      "Training the subspace: 14 / 45\n",
      "Training the subspace: 15 / 45\n",
      "Training the subspace: 16 / 45\n",
      "Training the subspace: 17 / 45\n",
      "Training the subspace: 18 / 45\n",
      "Training the subspace: 19 / 45\n",
      "Training the subspace: 20 / 45\n",
      "Training the subspace: 21 / 45\n",
      "Training the subspace: 22 / 45\n",
      "Training the subspace: 23 / 45\n",
      "Training the subspace: 24 / 45\n",
      "Training the subspace: 25 / 45\n",
      "Training the subspace: 26 / 45\n",
      "Training the subspace: 27 / 45\n",
      "Training the subspace: 28 / 45\n",
      "Training the subspace: 29 / 45\n",
      "Training the subspace: 30 / 45\n",
      "Training the subspace: 31 / 45\n",
      "Training the subspace: 32 / 45\n",
      "Training the subspace: 33 / 45\n",
      "Training the subspace: 34 / 45\n",
      "Training the subspace: 35 / 45\n",
      "Training the subspace: 36 / 45\n",
      "Training the subspace: 37 / 45\n",
      "Training the subspace: 38 / 45\n",
      "Training the subspace: 39 / 45\n",
      "Training the subspace: 40 / 45\n",
      "Training the subspace: 41 / 45\n",
      "Training the subspace: 42 / 45\n",
      "Training the subspace: 43 / 45\n",
      "Training the subspace: 44 / 45\n",
      "Encoding the subspace: 0 / 45\n",
      "Encoding the subspace: 1 / 45\n",
      "Encoding the subspace: 2 / 45\n",
      "Encoding the subspace: 3 / 45\n",
      "Encoding the subspace: 4 / 45\n",
      "Encoding the subspace: 5 / 45\n",
      "Encoding the subspace: 6 / 45\n",
      "Encoding the subspace: 7 / 45\n",
      "Encoding the subspace: 8 / 45\n",
      "Encoding the subspace: 9 / 45\n",
      "Encoding the subspace: 10 / 45\n",
      "Encoding the subspace: 11 / 45\n",
      "Encoding the subspace: 12 / 45\n",
      "Encoding the subspace: 13 / 45\n",
      "Encoding the subspace: 14 / 45\n",
      "Encoding the subspace: 15 / 45\n",
      "Encoding the subspace: 16 / 45\n",
      "Encoding the subspace: 17 / 45\n",
      "Encoding the subspace: 18 / 45\n",
      "Encoding the subspace: 19 / 45\n",
      "Encoding the subspace: 20 / 45\n",
      "Encoding the subspace: 21 / 45\n",
      "Encoding the subspace: 22 / 45\n",
      "Encoding the subspace: 23 / 45\n",
      "Encoding the subspace: 24 / 45\n",
      "Encoding the subspace: 25 / 45\n",
      "Encoding the subspace: 26 / 45\n",
      "Encoding the subspace: 27 / 45\n",
      "Encoding the subspace: 28 / 45\n",
      "Encoding the subspace: 29 / 45\n",
      "Encoding the subspace: 30 / 45\n",
      "Encoding the subspace: 31 / 45\n",
      "Encoding the subspace: 32 / 45\n",
      "Encoding the subspace: 33 / 45\n",
      "Encoding the subspace: 34 / 45\n",
      "Encoding the subspace: 35 / 45\n",
      "Encoding the subspace: 36 / 45\n",
      "Encoding the subspace: 37 / 45\n",
      "Encoding the subspace: 38 / 45\n",
      "Encoding the subspace: 39 / 45\n",
      "Encoding the subspace: 40 / 45\n",
      "Encoding the subspace: 41 / 45\n",
      "Encoding the subspace: 42 / 45\n",
      "Encoding the subspace: 43 / 45\n",
      "Encoding the subspace: 44 / 45\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=60, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.387235\n",
      "best loss:  0.3872354827431399\n",
      "Epoch 2 of 500, Train Loss: 0.054989\n",
      "best loss:  0.05498914527454118\n",
      "Epoch 3 of 500, Train Loss: 0.051369\n",
      "best loss:  0.051368678397004486\n",
      "Epoch 4 of 500, Train Loss: 0.045477\n",
      "best loss:  0.04547669163358203\n",
      "Epoch 5 of 500, Train Loss: 0.038459\n",
      "best loss:  0.03845857645614188\n",
      "Epoch 6 of 500, Train Loss: 0.032725\n",
      "best loss:  0.03272484296024958\n",
      "Epoch 7 of 500, Train Loss: 0.028934\n",
      "best loss:  0.02893376298828137\n",
      "Epoch 8 of 500, Train Loss: 0.026529\n",
      "best loss:  0.0265290828614835\n",
      "Epoch 9 of 500, Train Loss: 0.024822\n",
      "best loss:  0.02482181756040659\n",
      "Epoch 10 of 500, Train Loss: 0.023431\n",
      "best loss:  0.02343134304360566\n",
      "Epoch 11 of 500, Train Loss: 0.022231\n",
      "best loss:  0.022230675675032674\n",
      "Epoch 12 of 500, Train Loss: 0.021182\n",
      "best loss:  0.021182363094805284\n",
      "Epoch 13 of 500, Train Loss: 0.020269\n",
      "best loss:  0.02026937208738532\n",
      "Epoch 14 of 500, Train Loss: 0.019473\n",
      "best loss:  0.019473027965178455\n",
      "Epoch 15 of 500, Train Loss: 0.018775\n",
      "best loss:  0.018774626726941603\n",
      "Epoch 16 of 500, Train Loss: 0.018143\n",
      "best loss:  0.01814281425931556\n",
      "Epoch 17 of 500, Train Loss: 0.017569\n",
      "best loss:  0.017568765818177498\n",
      "Epoch 18 of 500, Train Loss: 0.017039\n",
      "best loss:  0.017039291434984494\n",
      "Epoch 19 of 500, Train Loss: 0.016542\n",
      "best loss:  0.016541756781372204\n",
      "Epoch 20 of 500, Train Loss: 0.016074\n",
      "best loss:  0.01607367889495393\n",
      "Epoch 21 of 500, Train Loss: 0.015647\n",
      "best loss:  0.015646722696562722\n",
      "Epoch 22 of 500, Train Loss: 0.015251\n",
      "best loss:  0.015251439323968276\n",
      "Epoch 23 of 500, Train Loss: 0.014887\n",
      "best loss:  0.014887441931989102\n",
      "Epoch 24 of 500, Train Loss: 0.014548\n",
      "best loss:  0.014547614789975793\n",
      "Epoch 25 of 500, Train Loss: 0.014240\n",
      "best loss:  0.014239757609429098\n",
      "Epoch 26 of 500, Train Loss: 0.013969\n",
      "best loss:  0.013968890626931043\n",
      "Epoch 27 of 500, Train Loss: 0.013710\n",
      "best loss:  0.013710353648030135\n",
      "Epoch 28 of 500, Train Loss: 0.013478\n",
      "best loss:  0.013477717504823985\n",
      "Epoch 29 of 500, Train Loss: 0.013267\n",
      "best loss:  0.013267373083639428\n",
      "Epoch 30 of 500, Train Loss: 0.013087\n",
      "best loss:  0.013087457429225983\n",
      "Epoch 31 of 500, Train Loss: 0.012914\n",
      "best loss:  0.012914196713538769\n",
      "Epoch 32 of 500, Train Loss: 0.012753\n",
      "best loss:  0.012752797212966089\n",
      "Epoch 33 of 500, Train Loss: 0.012611\n",
      "best loss:  0.012611220897601326\n",
      "Epoch 34 of 500, Train Loss: 0.012480\n",
      "best loss:  0.012479677829077438\n",
      "Epoch 35 of 500, Train Loss: 0.012360\n",
      "best loss:  0.012360317983606817\n",
      "Epoch 36 of 500, Train Loss: 0.012251\n",
      "best loss:  0.012250706280244139\n",
      "Epoch 37 of 500, Train Loss: 0.012150\n",
      "best loss:  0.012150266898124543\n",
      "Epoch 38 of 500, Train Loss: 0.012059\n",
      "best loss:  0.012058918293729756\n",
      "Epoch 39 of 500, Train Loss: 0.011969\n",
      "best loss:  0.011969356815458831\n",
      "Epoch 40 of 500, Train Loss: 0.011888\n",
      "best loss:  0.011888130025598834\n",
      "Epoch 41 of 500, Train Loss: 0.011806\n",
      "best loss:  0.011805933198526993\n",
      "Epoch 42 of 500, Train Loss: 0.011728\n",
      "best loss:  0.011727992221198674\n",
      "Epoch 43 of 500, Train Loss: 0.011667\n",
      "best loss:  0.011666712561412895\n",
      "Epoch 44 of 500, Train Loss: 0.011597\n",
      "best loss:  0.011597251074430135\n",
      "Epoch 45 of 500, Train Loss: 0.011539\n",
      "best loss:  0.011538797541988053\n",
      "Epoch 46 of 500, Train Loss: 0.011478\n",
      "best loss:  0.011477538423189467\n",
      "Epoch 47 of 500, Train Loss: 0.011426\n",
      "best loss:  0.01142592862196079\n",
      "Epoch 48 of 500, Train Loss: 0.011371\n",
      "best loss:  0.011370511094510713\n",
      "Epoch 49 of 500, Train Loss: 0.011319\n",
      "best loss:  0.011319398049079242\n",
      "Epoch 50 of 500, Train Loss: 0.011276\n",
      "best loss:  0.011276165134785155\n",
      "Epoch 51 of 500, Train Loss: 0.011227\n",
      "best loss:  0.011227281964492313\n",
      "Epoch 52 of 500, Train Loss: 0.011178\n",
      "best loss:  0.011178331235297342\n",
      "Epoch 53 of 500, Train Loss: 0.011137\n",
      "best loss:  0.011137301808217714\n",
      "Epoch 54 of 500, Train Loss: 0.011099\n",
      "best loss:  0.011098925761680855\n",
      "Epoch 55 of 500, Train Loss: 0.011057\n",
      "best loss:  0.011056697641678928\n",
      "Epoch 56 of 500, Train Loss: 0.011019\n",
      "best loss:  0.01101930163696484\n",
      "Epoch 57 of 500, Train Loss: 0.010989\n",
      "best loss:  0.010988870144924582\n",
      "Epoch 58 of 500, Train Loss: 0.010959\n",
      "best loss:  0.010959476997688695\n",
      "Epoch 59 of 500, Train Loss: 0.010929\n",
      "best loss:  0.010929152359121097\n",
      "Epoch 60 of 500, Train Loss: 0.010897\n",
      "best loss:  0.01089665941716273\n",
      "Epoch 61 of 500, Train Loss: 0.010867\n",
      "best loss:  0.01086716222847079\n",
      "Epoch 62 of 500, Train Loss: 0.010842\n",
      "best loss:  0.01084218961184451\n",
      "Epoch 63 of 500, Train Loss: 0.010816\n",
      "best loss:  0.010816377930206882\n",
      "Epoch 64 of 500, Train Loss: 0.010793\n",
      "best loss:  0.010792596789316974\n",
      "Epoch 65 of 500, Train Loss: 0.010771\n",
      "best loss:  0.010770754957013915\n",
      "Epoch 66 of 500, Train Loss: 0.010747\n",
      "best loss:  0.010747230259675439\n",
      "Epoch 67 of 500, Train Loss: 0.010728\n",
      "best loss:  0.010728111870761947\n",
      "Epoch 68 of 500, Train Loss: 0.010715\n",
      "best loss:  0.010714732878367677\n",
      "Epoch 69 of 500, Train Loss: 0.010696\n",
      "best loss:  0.010695697157869347\n",
      "Epoch 70 of 500, Train Loss: 0.010677\n",
      "best loss:  0.010676874142377493\n",
      "Epoch 71 of 500, Train Loss: 0.010659\n",
      "best loss:  0.010658548861722166\n",
      "Epoch 72 of 500, Train Loss: 0.010643\n",
      "best loss:  0.010642655863983434\n",
      "Epoch 73 of 500, Train Loss: 0.010626\n",
      "best loss:  0.010625539399307157\n",
      "Epoch 74 of 500, Train Loss: 0.010614\n",
      "best loss:  0.010613679852248205\n",
      "Epoch 75 of 500, Train Loss: 0.010602\n",
      "best loss:  0.01060245423455896\n",
      "Epoch 76 of 500, Train Loss: 0.010589\n",
      "best loss:  0.010588835553620769\n",
      "Epoch 77 of 500, Train Loss: 0.010579\n",
      "best loss:  0.010579187634049973\n",
      "Epoch 78 of 500, Train Loss: 0.010572\n",
      "best loss:  0.010572230281256855\n",
      "Epoch 79 of 500, Train Loss: 0.010563\n",
      "best loss:  0.01056326454238652\n",
      "Epoch 80 of 500, Train Loss: 0.010561\n",
      "best loss:  0.010561220386348007\n",
      "Epoch 81 of 500, Train Loss: 0.010558\n",
      "best loss:  0.010558431384775448\n",
      "Epoch 82 of 500, Train Loss: 0.010551\n",
      "best loss:  0.010551072064879199\n",
      "Epoch 83 of 500, Train Loss: 0.010550\n",
      "best loss:  0.010549751747726254\n",
      "Epoch 84 of 500, Train Loss: 0.010551\n",
      "Epoch 85 of 500, Train Loss: 0.010548\n",
      "best loss:  0.010548415328935511\n",
      "Epoch 86 of 500, Train Loss: 0.010546\n",
      "best loss:  0.010546013820837901\n",
      "Epoch 87 of 500, Train Loss: 0.010540\n",
      "best loss:  0.010540244956803148\n",
      "Epoch 88 of 500, Train Loss: 0.010533\n",
      "best loss:  0.010532828115667721\n",
      "Epoch 89 of 500, Train Loss: 0.010534\n",
      "Epoch 90 of 500, Train Loss: 0.010538\n",
      "Epoch 91 of 500, Train Loss: 0.010541\n",
      "Epoch 92 of 500, Train Loss: 0.010541\n",
      "Epoch 93 of 500, Train Loss: 0.010542\n",
      "Epoch 94 of 500, Train Loss: 0.010541\n",
      "Epoch 95 of 500, Train Loss: 0.010539\n",
      "Epoch 96 of 500, Train Loss: 0.010540\n",
      "Epoch 97 of 500, Train Loss: 0.010540\n",
      "Epoch 98 of 500, Train Loss: 0.010543\n",
      "Epoch 99 of 500, Train Loss: 0.010544\n",
      "Epoch 100 of 500, Train Loss: 0.010545\n",
      "Epoch 101 of 500, Train Loss: 0.010541\n",
      "Epoch 102 of 500, Train Loss: 0.010540\n",
      "Epoch 103 of 500, Train Loss: 0.010538\n",
      "Epoch 104 of 500, Train Loss: 0.010540\n",
      "Epoch 105 of 500, Train Loss: 0.010540\n",
      "Epoch 106 of 500, Train Loss: 0.010541\n",
      "Epoch 107 of 500, Train Loss: 0.010538\n",
      "Epoch 108 of 500, Train Loss: 0.010542\n",
      "Epoch 109 of 500, Train Loss: 0.010540\n",
      "Epoch 110 of 500, Train Loss: 0.010540\n",
      "Epoch 111 of 500, Train Loss: 0.010531\n",
      "best loss:  0.010530643490329927\n",
      "Epoch 112 of 500, Train Loss: 0.010539\n",
      "Epoch 113 of 500, Train Loss: 0.010529\n",
      "best loss:  0.010529136447174575\n",
      "Epoch 114 of 500, Train Loss: 0.010544\n",
      "Epoch 115 of 500, Train Loss: 0.010541\n",
      "Epoch 116 of 500, Train Loss: 0.010586\n",
      "Epoch 117 of 500, Train Loss: 0.010608\n",
      "Epoch 118 of 500, Train Loss: 0.010700\n",
      "Epoch 119 of 500, Train Loss: 0.010749\n",
      "Epoch 120 of 500, Train Loss: 0.010822\n",
      "Epoch 121 of 500, Train Loss: 0.010823\n",
      "Epoch 122 of 500, Train Loss: 0.010796\n",
      "Epoch 123 of 500, Train Loss: 0.010741\n",
      "Epoch 124 of 500, Train Loss: 0.010660\n",
      "Epoch 125 of 500, Train Loss: 0.010619\n",
      "Epoch 126 of 500, Train Loss: 0.010603\n",
      "Epoch 127 of 500, Train Loss: 0.010659\n",
      "Epoch 128 of 500, Train Loss: 0.010705\n",
      "Epoch 129 of 500, Train Loss: 0.010933\n",
      "Epoch 130 of 500, Train Loss: 0.010893\n",
      "Epoch 131 of 500, Train Loss: 0.011087\n",
      "Epoch 132 of 500, Train Loss: 0.010974\n",
      "Epoch 133 of 500, Train Loss: 0.011088\n",
      "Epoch 134 of 500, Train Loss: 0.010963\n",
      "Epoch 135 of 500, Train Loss: 0.010965\n",
      "Epoch 136 of 500, Train Loss: 0.010884\n",
      "Epoch 137 of 500, Train Loss: 0.010826\n",
      "Epoch 138 of 500, Train Loss: 0.010741\n",
      "Epoch 139 of 500, Train Loss: 0.010691\n",
      "Epoch 140 of 500, Train Loss: 0.010626\n",
      "Epoch 141 of 500, Train Loss: 0.010593\n",
      "Epoch 142 of 500, Train Loss: 0.010554\n",
      "Epoch 143 of 500, Train Loss: 0.010535\n",
      "Epoch 144 of 500, Train Loss: 0.010507\n",
      "best loss:  0.010506697052655662\n",
      "Epoch 145 of 500, Train Loss: 0.010505\n",
      "best loss:  0.010504897818999914\n",
      "Epoch 146 of 500, Train Loss: 0.010495\n",
      "best loss:  0.010495028309632353\n",
      "Epoch 147 of 500, Train Loss: 0.010503\n",
      "Epoch 148 of 500, Train Loss: 0.010520\n",
      "Epoch 149 of 500, Train Loss: 0.010535\n",
      "Epoch 150 of 500, Train Loss: 0.010589\n",
      "Epoch 151 of 500, Train Loss: 0.010595\n",
      "Epoch 152 of 500, Train Loss: 0.010654\n",
      "Epoch 153 of 500, Train Loss: 0.010635\n",
      "Epoch 154 of 500, Train Loss: 0.010680\n",
      "Epoch 155 of 500, Train Loss: 0.010654\n",
      "Epoch 156 of 500, Train Loss: 0.010686\n",
      "Epoch 157 of 500, Train Loss: 0.010658\n",
      "Epoch 158 of 500, Train Loss: 0.010674\n",
      "Epoch 159 of 500, Train Loss: 0.010633\n",
      "Epoch 160 of 500, Train Loss: 0.010634\n",
      "Epoch 161 of 500, Train Loss: 0.010591\n",
      "Epoch 162 of 500, Train Loss: 0.010584\n",
      "Epoch 163 of 500, Train Loss: 0.010543\n",
      "Epoch 164 of 500, Train Loss: 0.010535\n",
      "Epoch 165 of 500, Train Loss: 0.010503\n",
      "Epoch 166 of 500, Train Loss: 0.010492\n",
      "best loss:  0.010492360107117881\n",
      "Epoch 167 of 500, Train Loss: 0.010470\n",
      "best loss:  0.010469708570153741\n",
      "Epoch 168 of 500, Train Loss: 0.010474\n",
      "Epoch 169 of 500, Train Loss: 0.010458\n",
      "best loss:  0.010457765839496707\n",
      "Epoch 170 of 500, Train Loss: 0.010461\n",
      "Epoch 171 of 500, Train Loss: 0.010458\n",
      "Epoch 172 of 500, Train Loss: 0.010475\n",
      "Epoch 173 of 500, Train Loss: 0.010473\n",
      "Epoch 174 of 500, Train Loss: 0.010496\n",
      "Epoch 175 of 500, Train Loss: 0.010509\n",
      "Epoch 176 of 500, Train Loss: 0.010554\n",
      "Epoch 177 of 500, Train Loss: 0.010560\n",
      "Epoch 178 of 500, Train Loss: 0.010603\n",
      "Epoch 179 of 500, Train Loss: 0.010600\n",
      "Epoch 180 of 500, Train Loss: 0.010635\n",
      "Epoch 181 of 500, Train Loss: 0.010612\n",
      "Epoch 182 of 500, Train Loss: 0.010635\n",
      "Epoch 183 of 500, Train Loss: 0.010606\n",
      "Epoch 184 of 500, Train Loss: 0.010620\n",
      "Epoch 185 of 500, Train Loss: 0.010586\n",
      "Epoch 186 of 500, Train Loss: 0.010587\n",
      "Epoch 187 of 500, Train Loss: 0.010565\n",
      "Epoch 188 of 500, Train Loss: 0.010561\n",
      "Epoch 189 of 500, Train Loss: 0.010548\n",
      "Epoch 190 of 500, Train Loss: 0.010546\n",
      "Epoch 191 of 500, Train Loss: 0.010541\n",
      "Epoch 192 of 500, Train Loss: 0.010530\n",
      "Epoch 193 of 500, Train Loss: 0.010513\n",
      "Epoch 194 of 500, Train Loss: 0.010520\n",
      "Epoch 195 of 500, Train Loss: 0.010501\n",
      "Epoch 196 of 500, Train Loss: 0.010502\n",
      "Epoch 197 of 500, Train Loss: 0.010502\n",
      "Epoch 198 of 500, Train Loss: 0.010520\n",
      "Epoch 199 of 500, Train Loss: 0.010508\n",
      "Epoch 200 of 500, Train Loss: 0.010526\n",
      "Epoch 201 of 500, Train Loss: 0.010531\n",
      "Epoch 202 of 500, Train Loss: 0.010548\n",
      "Epoch 203 of 500, Train Loss: 0.010544\n",
      "Epoch 204 of 500, Train Loss: 0.010547\n",
      "Epoch 205 of 500, Train Loss: 0.010539\n",
      "Epoch 206 of 500, Train Loss: 0.010521\n",
      "Epoch 207 of 500, Train Loss: 0.010526\n",
      "Epoch 208 of 500, Train Loss: 0.010511\n",
      "Epoch 209 of 500, Train Loss: 0.010510\n",
      "Epoch 210 of 500, Train Loss: 0.010516\n",
      "Epoch 211 of 500, Train Loss: 0.010503\n",
      "Epoch 212 of 500, Train Loss: 0.010502\n",
      "Epoch 213 of 500, Train Loss: 0.010487\n",
      "Epoch 214 of 500, Train Loss: 0.010495\n",
      "Epoch 215 of 500, Train Loss: 0.010473\n",
      "Epoch 216 of 500, Train Loss: 0.010464\n",
      "Epoch 217 of 500, Train Loss: 0.010454\n",
      "best loss:  0.01045354248519514\n",
      "Epoch 218 of 500, Train Loss: 0.010457\n",
      "Epoch 219 of 500, Train Loss: 0.010447\n",
      "best loss:  0.010447317991895752\n",
      "Epoch 220 of 500, Train Loss: 0.010441\n",
      "best loss:  0.010440915025447589\n",
      "Epoch 221 of 500, Train Loss: 0.010426\n",
      "best loss:  0.010426285563514192\n",
      "Epoch 222 of 500, Train Loss: 0.010425\n",
      "best loss:  0.0104252960584343\n",
      "Epoch 223 of 500, Train Loss: 0.010416\n",
      "best loss:  0.010416402820815412\n",
      "Epoch 224 of 500, Train Loss: 0.010424\n",
      "Epoch 225 of 500, Train Loss: 0.010422\n",
      "Epoch 226 of 500, Train Loss: 0.010413\n",
      "best loss:  0.010412977249320965\n",
      "Epoch 227 of 500, Train Loss: 0.010408\n",
      "best loss:  0.010407630073957059\n",
      "Epoch 228 of 500, Train Loss: 0.010417\n",
      "Epoch 229 of 500, Train Loss: 0.010447\n",
      "Epoch 230 of 500, Train Loss: 0.010467\n",
      "Epoch 231 of 500, Train Loss: 0.010481\n",
      "Epoch 232 of 500, Train Loss: 0.010536\n",
      "Epoch 233 of 500, Train Loss: 0.010530\n",
      "Epoch 234 of 500, Train Loss: 0.010556\n",
      "Epoch 235 of 500, Train Loss: 0.010544\n",
      "Epoch 236 of 500, Train Loss: 0.010533\n",
      "Epoch 237 of 500, Train Loss: 0.010507\n",
      "Epoch 238 of 500, Train Loss: 0.010512\n",
      "Epoch 239 of 500, Train Loss: 0.010486\n",
      "Epoch 240 of 500, Train Loss: 0.010480\n",
      "Epoch 241 of 500, Train Loss: 0.010454\n",
      "Epoch 242 of 500, Train Loss: 0.010451\n",
      "Epoch 243 of 500, Train Loss: 0.010434\n",
      "Epoch 244 of 500, Train Loss: 0.010451\n",
      "Epoch 245 of 500, Train Loss: 0.010434\n",
      "Epoch 246 of 500, Train Loss: 0.010435\n",
      "Epoch 247 of 500, Train Loss: 0.010408\n",
      "Epoch 248 of 500, Train Loss: 0.010415\n",
      "Epoch 249 of 500, Train Loss: 0.010386\n",
      "best loss:  0.010385947658223742\n",
      "Epoch 250 of 500, Train Loss: 0.010401\n",
      "Epoch 251 of 500, Train Loss: 0.010394\n",
      "Epoch 252 of 500, Train Loss: 0.010414\n",
      "Epoch 253 of 500, Train Loss: 0.010427\n",
      "Epoch 254 of 500, Train Loss: 0.010445\n",
      "Epoch 255 of 500, Train Loss: 0.010472\n",
      "Epoch 256 of 500, Train Loss: 0.010474\n",
      "Epoch 257 of 500, Train Loss: 0.010519\n",
      "Epoch 258 of 500, Train Loss: 0.010521\n",
      "Epoch 259 of 500, Train Loss: 0.010570\n",
      "Epoch 260 of 500, Train Loss: 0.010563\n",
      "Epoch 261 of 500, Train Loss: 0.010567\n",
      "Epoch 262 of 500, Train Loss: 0.010543\n",
      "Epoch 263 of 500, Train Loss: 0.010541\n",
      "Epoch 264 of 500, Train Loss: 0.010537\n",
      "Epoch 265 of 500, Train Loss: 0.010543\n",
      "Epoch 266 of 500, Train Loss: 0.010530\n",
      "Epoch 267 of 500, Train Loss: 0.010500\n",
      "Epoch 268 of 500, Train Loss: 0.010472\n",
      "Epoch 269 of 500, Train Loss: 0.010463\n",
      "Epoch 270 of 500, Train Loss: 0.010450\n",
      "Epoch 271 of 500, Train Loss: 0.010461\n",
      "Epoch 272 of 500, Train Loss: 0.010449\n",
      "Epoch 273 of 500, Train Loss: 0.010437\n",
      "Epoch 274 of 500, Train Loss: 0.010423\n",
      "Epoch 275 of 500, Train Loss: 0.010424\n",
      "Epoch 276 of 500, Train Loss: 0.010462\n",
      "Epoch 277 of 500, Train Loss: 0.010469\n",
      "Epoch 278 of 500, Train Loss: 0.010523\n",
      "Epoch 279 of 500, Train Loss: 0.010506\n",
      "Epoch 280 of 500, Train Loss: 0.010526\n",
      "Epoch 281 of 500, Train Loss: 0.010506\n",
      "Epoch 282 of 500, Train Loss: 0.010532\n",
      "Epoch 283 of 500, Train Loss: 0.010522\n",
      "Epoch 284 of 500, Train Loss: 0.010532\n",
      "Epoch 285 of 500, Train Loss: 0.010505\n",
      "Epoch 286 of 500, Train Loss: 0.010511\n",
      "Epoch 287 of 500, Train Loss: 0.010492\n",
      "Epoch 288 of 500, Train Loss: 0.010486\n",
      "Epoch 289 of 500, Train Loss: 0.010453\n",
      "Epoch 290 of 500, Train Loss: 0.010464\n",
      "Epoch 291 of 500, Train Loss: 0.010433\n",
      "Epoch 292 of 500, Train Loss: 0.010442\n",
      "Epoch 293 of 500, Train Loss: 0.010406\n",
      "Epoch 294 of 500, Train Loss: 0.010410\n",
      "Epoch 295 of 500, Train Loss: 0.010387\n",
      "Epoch 296 of 500, Train Loss: 0.010404\n",
      "Epoch 297 of 500, Train Loss: 0.010391\n",
      "Epoch 298 of 500, Train Loss: 0.010414\n",
      "Epoch 299 of 500, Train Loss: 0.010403\n",
      "Epoch 300 of 500, Train Loss: 0.010431\n",
      "Epoch 301 of 500, Train Loss: 0.010427\n",
      "Epoch 302 of 500, Train Loss: 0.010484\n",
      "Epoch 303 of 500, Train Loss: 0.010480\n",
      "Epoch 304 of 500, Train Loss: 0.010527\n",
      "Epoch 305 of 500, Train Loss: 0.010504\n",
      "Epoch 306 of 500, Train Loss: 0.010536\n",
      "Epoch 307 of 500, Train Loss: 0.010533\n",
      "Epoch 308 of 500, Train Loss: 0.010558\n",
      "Epoch 309 of 500, Train Loss: 0.010547\n",
      "Epoch 310 of 500, Train Loss: 0.010568\n",
      "Epoch 311 of 500, Train Loss: 0.010540\n",
      "Epoch 312 of 500, Train Loss: 0.010534\n",
      "Epoch 313 of 500, Train Loss: 0.010491\n",
      "Epoch 314 of 500, Train Loss: 0.010496\n",
      "Epoch 315 of 500, Train Loss: 0.010460\n",
      "Epoch 316 of 500, Train Loss: 0.010468\n",
      "Epoch 317 of 500, Train Loss: 0.010419\n",
      "Epoch 318 of 500, Train Loss: 0.010402\n",
      "Epoch 319 of 500, Train Loss: 0.010368\n",
      "best loss:  0.01036816329008073\n",
      "Epoch 320 of 500, Train Loss: 0.010373\n",
      "Epoch 321 of 500, Train Loss: 0.010367\n",
      "best loss:  0.010367260705846607\n",
      "Epoch 322 of 500, Train Loss: 0.010368\n",
      "Epoch 323 of 500, Train Loss: 0.010364\n",
      "best loss:  0.010363585500307417\n",
      "Epoch 324 of 500, Train Loss: 0.010355\n",
      "best loss:  0.010355181275625973\n",
      "Epoch 325 of 500, Train Loss: 0.010354\n",
      "best loss:  0.010353899821336741\n",
      "Epoch 326 of 500, Train Loss: 0.010363\n",
      "Epoch 327 of 500, Train Loss: 0.010377\n",
      "Epoch 328 of 500, Train Loss: 0.010384\n",
      "Epoch 329 of 500, Train Loss: 0.010414\n",
      "Epoch 330 of 500, Train Loss: 0.010414\n",
      "Epoch 331 of 500, Train Loss: 0.010441\n",
      "Epoch 332 of 500, Train Loss: 0.010450\n",
      "Epoch 333 of 500, Train Loss: 0.010491\n",
      "Epoch 334 of 500, Train Loss: 0.010502\n",
      "Epoch 335 of 500, Train Loss: 0.010545\n",
      "Epoch 336 of 500, Train Loss: 0.010575\n",
      "Epoch 337 of 500, Train Loss: 0.010633\n",
      "Epoch 338 of 500, Train Loss: 0.010601\n",
      "Epoch 339 of 500, Train Loss: 0.010593\n",
      "Epoch 340 of 500, Train Loss: 0.010570\n",
      "Epoch 341 of 500, Train Loss: 0.010532\n",
      "Epoch 342 of 500, Train Loss: 0.010528\n",
      "Epoch 343 of 500, Train Loss: 0.010492\n",
      "Epoch 344 of 500, Train Loss: 0.010494\n",
      "Epoch 345 of 500, Train Loss: 0.010475\n",
      "Epoch 346 of 500, Train Loss: 0.010469\n",
      "Epoch 347 of 500, Train Loss: 0.010432\n",
      "Epoch 348 of 500, Train Loss: 0.010403\n",
      "Epoch 349 of 500, Train Loss: 0.010369\n",
      "Epoch 350 of 500, Train Loss: 0.010350\n",
      "best loss:  0.01034987081621066\n",
      "Epoch 351 of 500, Train Loss: 0.010341\n",
      "best loss:  0.010340887201384121\n",
      "Epoch 352 of 500, Train Loss: 0.010335\n",
      "best loss:  0.010335149950087937\n",
      "Epoch 353 of 500, Train Loss: 0.010329\n",
      "best loss:  0.010329309361806326\n",
      "Epoch 354 of 500, Train Loss: 0.010333\n",
      "Epoch 355 of 500, Train Loss: 0.010326\n",
      "best loss:  0.010326244917295715\n",
      "Epoch 356 of 500, Train Loss: 0.010341\n",
      "Epoch 357 of 500, Train Loss: 0.010358\n",
      "Epoch 358 of 500, Train Loss: 0.010401\n",
      "Epoch 359 of 500, Train Loss: 0.010390\n",
      "Epoch 360 of 500, Train Loss: 0.010433\n",
      "Epoch 361 of 500, Train Loss: 0.010436\n",
      "Epoch 362 of 500, Train Loss: 0.010521\n",
      "Epoch 363 of 500, Train Loss: 0.010529\n",
      "Epoch 364 of 500, Train Loss: 0.010652\n",
      "Epoch 365 of 500, Train Loss: 0.010654\n",
      "Epoch 366 of 500, Train Loss: 0.010758\n",
      "Epoch 367 of 500, Train Loss: 0.010742\n",
      "Epoch 368 of 500, Train Loss: 0.010818\n",
      "Epoch 369 of 500, Train Loss: 0.010748\n",
      "Epoch 370 of 500, Train Loss: 0.010722\n",
      "Epoch 371 of 500, Train Loss: 0.010668\n",
      "Epoch 372 of 500, Train Loss: 0.010630\n",
      "Epoch 373 of 500, Train Loss: 0.010615\n",
      "Epoch 374 of 500, Train Loss: 0.010557\n",
      "Epoch 375 of 500, Train Loss: 0.010553\n",
      "Epoch 376 of 500, Train Loss: 0.010517\n",
      "Epoch 377 of 500, Train Loss: 0.010507\n",
      "Epoch 378 of 500, Train Loss: 0.010473\n",
      "Epoch 379 of 500, Train Loss: 0.010461\n",
      "Epoch 380 of 500, Train Loss: 0.010434\n",
      "Epoch 381 of 500, Train Loss: 0.010438\n",
      "Epoch 382 of 500, Train Loss: 0.010411\n",
      "Epoch 383 of 500, Train Loss: 0.010427\n",
      "Epoch 384 of 500, Train Loss: 0.010407\n",
      "Epoch 385 of 500, Train Loss: 0.010430\n",
      "Epoch 386 of 500, Train Loss: 0.010402\n",
      "Epoch 387 of 500, Train Loss: 0.010438\n",
      "Epoch 388 of 500, Train Loss: 0.010410\n",
      "Epoch 389 of 500, Train Loss: 0.010456\n",
      "Epoch 390 of 500, Train Loss: 0.010443\n",
      "Epoch 391 of 500, Train Loss: 0.010512\n",
      "Epoch 392 of 500, Train Loss: 0.010517\n",
      "Epoch 393 of 500, Train Loss: 0.010606\n",
      "Epoch 394 of 500, Train Loss: 0.010614\n",
      "Epoch 395 of 500, Train Loss: 0.010681\n",
      "Epoch 396 of 500, Train Loss: 0.010681\n",
      "Epoch 397 of 500, Train Loss: 0.010705\n",
      "Epoch 398 of 500, Train Loss: 0.010695\n",
      "Epoch 399 of 500, Train Loss: 0.010678\n",
      "Epoch 400 of 500, Train Loss: 0.010667\n",
      "Epoch 401 of 500, Train Loss: 0.010614\n",
      "Epoch 402 of 500, Train Loss: 0.010596\n",
      "Epoch 403 of 500, Train Loss: 0.010518\n",
      "Epoch 404 of 500, Train Loss: 0.010493\n",
      "Epoch 405 of 500, Train Loss: 0.010427\n",
      "Epoch 406 of 500, Train Loss: 0.010426\n",
      "Epoch 407 of 500, Train Loss: 0.010387\n",
      "Epoch 408 of 500, Train Loss: 0.010401\n",
      "Epoch 409 of 500, Train Loss: 0.010372\n",
      "Epoch 410 of 500, Train Loss: 0.010394\n",
      "Epoch 411 of 500, Train Loss: 0.010384\n",
      "Epoch 412 of 500, Train Loss: 0.010393\n",
      "Epoch 413 of 500, Train Loss: 0.010381\n",
      "Epoch 414 of 500, Train Loss: 0.010396\n",
      "Epoch 415 of 500, Train Loss: 0.010382\n",
      "Epoch 416 of 500, Train Loss: 0.010397\n",
      "Epoch 417 of 500, Train Loss: 0.010392\n",
      "Epoch 418 of 500, Train Loss: 0.010437\n",
      "Epoch 419 of 500, Train Loss: 0.010451\n",
      "Epoch 420 of 500, Train Loss: 0.010537\n",
      "Epoch 421 of 500, Train Loss: 0.010544\n",
      "Epoch 422 of 500, Train Loss: 0.010621\n",
      "Epoch 423 of 500, Train Loss: 0.010612\n",
      "Epoch 424 of 500, Train Loss: 0.010629\n",
      "Epoch 425 of 500, Train Loss: 0.010605\n",
      "Epoch 426 of 500, Train Loss: 0.010603\n",
      "Epoch 427 of 500, Train Loss: 0.010571\n",
      "Epoch 428 of 500, Train Loss: 0.010525\n",
      "Epoch 429 of 500, Train Loss: 0.010503\n",
      "Epoch 430 of 500, Train Loss: 0.010448\n",
      "Epoch 431 of 500, Train Loss: 0.010439\n",
      "Epoch 432 of 500, Train Loss: 0.010389\n",
      "Epoch 433 of 500, Train Loss: 0.010391\n",
      "Epoch 434 of 500, Train Loss: 0.010355\n",
      "Epoch 435 of 500, Train Loss: 0.010365\n",
      "Epoch 436 of 500, Train Loss: 0.010340\n",
      "Epoch 437 of 500, Train Loss: 0.010358\n",
      "Epoch 438 of 500, Train Loss: 0.010341\n",
      "Epoch 439 of 500, Train Loss: 0.010365\n",
      "Epoch 440 of 500, Train Loss: 0.010357\n",
      "Epoch 441 of 500, Train Loss: 0.010374\n",
      "Epoch 442 of 500, Train Loss: 0.010349\n",
      "Epoch 443 of 500, Train Loss: 0.010353\n",
      "Epoch 444 of 500, Train Loss: 0.010327\n",
      "Epoch 445 of 500, Train Loss: 0.010328\n",
      "Epoch 446 of 500, Train Loss: 0.010321\n",
      "best loss:  0.010321121747346501\n",
      "Epoch 447 of 500, Train Loss: 0.010346\n",
      "Epoch 448 of 500, Train Loss: 0.010375\n",
      "Epoch 449 of 500, Train Loss: 0.010455\n",
      "Epoch 450 of 500, Train Loss: 0.010499\n",
      "Epoch 451 of 500, Train Loss: 0.010600\n",
      "Epoch 452 of 500, Train Loss: 0.010592\n",
      "Epoch 453 of 500, Train Loss: 0.010669\n",
      "Epoch 454 of 500, Train Loss: 0.010644\n",
      "Epoch 455 of 500, Train Loss: 0.010718\n",
      "Epoch 456 of 500, Train Loss: 0.010678\n",
      "Epoch 457 of 500, Train Loss: 0.010679\n",
      "Epoch 458 of 500, Train Loss: 0.010606\n",
      "Epoch 459 of 500, Train Loss: 0.010565\n",
      "Epoch 460 of 500, Train Loss: 0.010532\n",
      "Epoch 461 of 500, Train Loss: 0.010495\n",
      "Epoch 462 of 500, Train Loss: 0.010482\n",
      "Epoch 463 of 500, Train Loss: 0.010459\n",
      "Epoch 464 of 500, Train Loss: 0.010441\n",
      "Epoch 465 of 500, Train Loss: 0.010440\n",
      "Epoch 466 of 500, Train Loss: 0.010414\n",
      "Epoch 467 of 500, Train Loss: 0.010420\n",
      "Epoch 468 of 500, Train Loss: 0.010392\n",
      "Epoch 469 of 500, Train Loss: 0.010403\n",
      "Epoch 470 of 500, Train Loss: 0.010375\n",
      "Epoch 471 of 500, Train Loss: 0.010397\n",
      "Epoch 472 of 500, Train Loss: 0.010371\n",
      "Epoch 473 of 500, Train Loss: 0.010403\n",
      "Epoch 474 of 500, Train Loss: 0.010391\n",
      "Epoch 475 of 500, Train Loss: 0.010442\n",
      "Epoch 476 of 500, Train Loss: 0.010445\n",
      "Epoch 477 of 500, Train Loss: 0.010515\n",
      "Epoch 478 of 500, Train Loss: 0.010507\n",
      "Epoch 479 of 500, Train Loss: 0.010563\n",
      "Epoch 480 of 500, Train Loss: 0.010537\n",
      "Epoch 481 of 500, Train Loss: 0.010559\n",
      "Epoch 482 of 500, Train Loss: 0.010554\n",
      "Epoch 483 of 500, Train Loss: 0.010565\n",
      "Epoch 484 of 500, Train Loss: 0.010573\n",
      "Epoch 485 of 500, Train Loss: 0.010556\n",
      "Epoch 486 of 500, Train Loss: 0.010552\n",
      "Epoch 487 of 500, Train Loss: 0.010502\n",
      "Epoch 488 of 500, Train Loss: 0.010487\n",
      "Epoch 489 of 500, Train Loss: 0.010471\n",
      "Epoch 490 of 500, Train Loss: 0.010462\n",
      "Epoch 491 of 500, Train Loss: 0.010462\n",
      "Epoch 492 of 500, Train Loss: 0.010452\n",
      "Epoch 493 of 500, Train Loss: 0.010461\n",
      "Epoch 494 of 500, Train Loss: 0.010449\n",
      "Epoch 495 of 500, Train Loss: 0.010462\n",
      "Epoch 496 of 500, Train Loss: 0.010445\n",
      "Epoch 497 of 500, Train Loss: 0.010455\n",
      "Epoch 498 of 500, Train Loss: 0.010437\n",
      "Epoch 499 of 500, Train Loss: 0.010447\n",
      "Epoch 500 of 500, Train Loss: 0.010432\n",
      "latent train shape:  (16395, 60)\n",
      "M: 60, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 45\n",
      "Training the subspace: 0 / 60\n",
      "Training the subspace: 1 / 60\n",
      "Training the subspace: 2 / 60\n",
      "Training the subspace: 3 / 60\n",
      "Training the subspace: 4 / 60\n",
      "Training the subspace: 5 / 60\n",
      "Training the subspace: 6 / 60\n",
      "Training the subspace: 7 / 60\n",
      "Training the subspace: 8 / 60\n",
      "Training the subspace: 9 / 60\n",
      "Training the subspace: 10 / 60\n",
      "Training the subspace: 11 / 60\n",
      "Training the subspace: 12 / 60\n",
      "Training the subspace: 13 / 60\n",
      "Training the subspace: 14 / 60\n",
      "Training the subspace: 15 / 60\n",
      "Training the subspace: 16 / 60\n",
      "Training the subspace: 17 / 60\n",
      "Training the subspace: 18 / 60\n",
      "Training the subspace: 19 / 60\n",
      "Training the subspace: 20 / 60\n",
      "Training the subspace: 21 / 60\n",
      "Training the subspace: 22 / 60\n",
      "Training the subspace: 23 / 60\n",
      "Training the subspace: 24 / 60\n",
      "Training the subspace: 25 / 60\n",
      "Training the subspace: 26 / 60\n",
      "Training the subspace: 27 / 60\n",
      "Training the subspace: 28 / 60\n",
      "Training the subspace: 29 / 60\n",
      "Training the subspace: 30 / 60\n",
      "Training the subspace: 31 / 60\n",
      "Training the subspace: 32 / 60\n",
      "Training the subspace: 33 / 60\n",
      "Training the subspace: 34 / 60\n",
      "Training the subspace: 35 / 60\n",
      "Training the subspace: 36 / 60\n",
      "Training the subspace: 37 / 60\n",
      "Training the subspace: 38 / 60\n",
      "Training the subspace: 39 / 60\n",
      "Training the subspace: 40 / 60\n",
      "Training the subspace: 41 / 60\n",
      "Training the subspace: 42 / 60\n",
      "Training the subspace: 43 / 60\n",
      "Training the subspace: 44 / 60\n",
      "Training the subspace: 45 / 60\n",
      "Training the subspace: 46 / 60\n",
      "Training the subspace: 47 / 60\n",
      "Training the subspace: 48 / 60\n",
      "Training the subspace: 49 / 60\n",
      "Training the subspace: 50 / 60\n",
      "Training the subspace: 51 / 60\n",
      "Training the subspace: 52 / 60\n",
      "Training the subspace: 53 / 60\n",
      "Training the subspace: 54 / 60\n",
      "Training the subspace: 55 / 60\n",
      "Training the subspace: 56 / 60\n",
      "Training the subspace: 57 / 60\n",
      "Training the subspace: 58 / 60\n",
      "Training the subspace: 59 / 60\n",
      "Encoding the subspace: 0 / 60\n",
      "Encoding the subspace: 1 / 60\n",
      "Encoding the subspace: 2 / 60\n",
      "Encoding the subspace: 3 / 60\n",
      "Encoding the subspace: 4 / 60\n",
      "Encoding the subspace: 5 / 60\n",
      "Encoding the subspace: 6 / 60\n",
      "Encoding the subspace: 7 / 60\n",
      "Encoding the subspace: 8 / 60\n",
      "Encoding the subspace: 9 / 60\n",
      "Encoding the subspace: 10 / 60\n",
      "Encoding the subspace: 11 / 60\n",
      "Encoding the subspace: 12 / 60\n",
      "Encoding the subspace: 13 / 60\n",
      "Encoding the subspace: 14 / 60\n",
      "Encoding the subspace: 15 / 60\n",
      "Encoding the subspace: 16 / 60\n",
      "Encoding the subspace: 17 / 60\n",
      "Encoding the subspace: 18 / 60\n",
      "Encoding the subspace: 19 / 60\n",
      "Encoding the subspace: 20 / 60\n",
      "Encoding the subspace: 21 / 60\n",
      "Encoding the subspace: 22 / 60\n",
      "Encoding the subspace: 23 / 60\n",
      "Encoding the subspace: 24 / 60\n",
      "Encoding the subspace: 25 / 60\n",
      "Encoding the subspace: 26 / 60\n",
      "Encoding the subspace: 27 / 60\n",
      "Encoding the subspace: 28 / 60\n",
      "Encoding the subspace: 29 / 60\n",
      "Encoding the subspace: 30 / 60\n",
      "Encoding the subspace: 31 / 60\n",
      "Encoding the subspace: 32 / 60\n",
      "Encoding the subspace: 33 / 60\n",
      "Encoding the subspace: 34 / 60\n",
      "Encoding the subspace: 35 / 60\n",
      "Encoding the subspace: 36 / 60\n",
      "Encoding the subspace: 37 / 60\n",
      "Encoding the subspace: 38 / 60\n",
      "Encoding the subspace: 39 / 60\n",
      "Encoding the subspace: 40 / 60\n",
      "Encoding the subspace: 41 / 60\n",
      "Encoding the subspace: 42 / 60\n",
      "Encoding the subspace: 43 / 60\n",
      "Encoding the subspace: 44 / 60\n",
      "Encoding the subspace: 45 / 60\n",
      "Encoding the subspace: 46 / 60\n",
      "Encoding the subspace: 47 / 60\n",
      "Encoding the subspace: 48 / 60\n",
      "Encoding the subspace: 49 / 60\n",
      "Encoding the subspace: 50 / 60\n",
      "Encoding the subspace: 51 / 60\n",
      "Encoding the subspace: 52 / 60\n",
      "Encoding the subspace: 53 / 60\n",
      "Encoding the subspace: 54 / 60\n",
      "Encoding the subspace: 55 / 60\n",
      "Encoding the subspace: 56 / 60\n",
      "Encoding the subspace: 57 / 60\n",
      "Encoding the subspace: 58 / 60\n",
      "Encoding the subspace: 59 / 60\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=90, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.319945\n",
      "best loss:  0.319945469650245\n",
      "Epoch 2 of 500, Train Loss: 0.052630\n",
      "best loss:  0.052629781059844984\n",
      "Epoch 3 of 500, Train Loss: 0.045482\n",
      "best loss:  0.045481695225029144\n",
      "Epoch 4 of 500, Train Loss: 0.036777\n",
      "best loss:  0.036776895202559154\n",
      "Epoch 5 of 500, Train Loss: 0.030464\n",
      "best loss:  0.03046428434263785\n",
      "Epoch 6 of 500, Train Loss: 0.026886\n",
      "best loss:  0.02688644854387976\n",
      "Epoch 7 of 500, Train Loss: 0.024603\n",
      "best loss:  0.024602529920399362\n",
      "Epoch 8 of 500, Train Loss: 0.022796\n",
      "best loss:  0.022795972464400408\n",
      "Epoch 9 of 500, Train Loss: 0.021291\n",
      "best loss:  0.021290742610797806\n",
      "Epoch 10 of 500, Train Loss: 0.020030\n",
      "best loss:  0.020029598930733306\n",
      "Epoch 11 of 500, Train Loss: 0.018974\n",
      "best loss:  0.018974406193931002\n",
      "Epoch 12 of 500, Train Loss: 0.018050\n",
      "best loss:  0.018049810833682112\n",
      "Epoch 13 of 500, Train Loss: 0.017235\n",
      "best loss:  0.017234883442396556\n",
      "Epoch 14 of 500, Train Loss: 0.016500\n",
      "best loss:  0.016500231056373077\n",
      "Epoch 15 of 500, Train Loss: 0.015828\n",
      "best loss:  0.015827718919347034\n",
      "Epoch 16 of 500, Train Loss: 0.015208\n",
      "best loss:  0.015207622813912787\n",
      "Epoch 17 of 500, Train Loss: 0.014640\n",
      "best loss:  0.014639695046168152\n",
      "Epoch 18 of 500, Train Loss: 0.014139\n",
      "best loss:  0.01413860282695549\n",
      "Epoch 19 of 500, Train Loss: 0.013661\n",
      "best loss:  0.01366116733257767\n",
      "Epoch 20 of 500, Train Loss: 0.013234\n",
      "best loss:  0.013234133021084794\n",
      "Epoch 21 of 500, Train Loss: 0.012846\n",
      "best loss:  0.012845849287795452\n",
      "Epoch 22 of 500, Train Loss: 0.012503\n",
      "best loss:  0.012503100837293883\n",
      "Epoch 23 of 500, Train Loss: 0.012189\n",
      "best loss:  0.012189340468382376\n",
      "Epoch 24 of 500, Train Loss: 0.011904\n",
      "best loss:  0.011903828678899914\n",
      "Epoch 25 of 500, Train Loss: 0.011654\n",
      "best loss:  0.011653512259882102\n",
      "Epoch 26 of 500, Train Loss: 0.011418\n",
      "best loss:  0.011418372019042456\n",
      "Epoch 27 of 500, Train Loss: 0.011204\n",
      "best loss:  0.011204254917047789\n",
      "Epoch 28 of 500, Train Loss: 0.011006\n",
      "best loss:  0.011006261039093902\n",
      "Epoch 29 of 500, Train Loss: 0.010826\n",
      "best loss:  0.010826331677904366\n",
      "Epoch 30 of 500, Train Loss: 0.010660\n",
      "best loss:  0.01065982688006732\n",
      "Epoch 31 of 500, Train Loss: 0.010512\n",
      "best loss:  0.010512413332385891\n",
      "Epoch 32 of 500, Train Loss: 0.010369\n",
      "best loss:  0.010369087224117798\n",
      "Epoch 33 of 500, Train Loss: 0.010239\n",
      "best loss:  0.010239493715158995\n",
      "Epoch 34 of 500, Train Loss: 0.010125\n",
      "best loss:  0.010124563354576482\n",
      "Epoch 35 of 500, Train Loss: 0.010025\n",
      "best loss:  0.01002488978673675\n",
      "Epoch 36 of 500, Train Loss: 0.009924\n",
      "best loss:  0.009923894059855144\n",
      "Epoch 37 of 500, Train Loss: 0.009837\n",
      "best loss:  0.009837008664806829\n",
      "Epoch 38 of 500, Train Loss: 0.009762\n",
      "best loss:  0.009761569128992656\n",
      "Epoch 39 of 500, Train Loss: 0.009678\n",
      "best loss:  0.00967784524751311\n",
      "Epoch 40 of 500, Train Loss: 0.009605\n",
      "best loss:  0.009605064165445456\n",
      "Epoch 41 of 500, Train Loss: 0.009546\n",
      "best loss:  0.009546179616893281\n",
      "Epoch 42 of 500, Train Loss: 0.009476\n",
      "best loss:  0.009476235492742447\n",
      "Epoch 43 of 500, Train Loss: 0.009421\n",
      "best loss:  0.00942127099529165\n",
      "Epoch 44 of 500, Train Loss: 0.009369\n",
      "best loss:  0.009368560892054198\n",
      "Epoch 45 of 500, Train Loss: 0.009327\n",
      "best loss:  0.009327433593079354\n",
      "Epoch 46 of 500, Train Loss: 0.009276\n",
      "best loss:  0.009276147763846738\n",
      "Epoch 47 of 500, Train Loss: 0.009237\n",
      "best loss:  0.009236617695947191\n",
      "Epoch 48 of 500, Train Loss: 0.009198\n",
      "best loss:  0.00919789347396947\n",
      "Epoch 49 of 500, Train Loss: 0.009161\n",
      "best loss:  0.009161396354031678\n",
      "Epoch 50 of 500, Train Loss: 0.009133\n",
      "best loss:  0.009133037150830262\n",
      "Epoch 51 of 500, Train Loss: 0.009093\n",
      "best loss:  0.009093175269934067\n",
      "Epoch 52 of 500, Train Loss: 0.009067\n",
      "best loss:  0.009066733532894743\n",
      "Epoch 53 of 500, Train Loss: 0.009037\n",
      "best loss:  0.009036710828519246\n",
      "Epoch 54 of 500, Train Loss: 0.009020\n",
      "best loss:  0.00902030156052657\n",
      "Epoch 55 of 500, Train Loss: 0.008980\n",
      "best loss:  0.008980245853647005\n",
      "Epoch 56 of 500, Train Loss: 0.008957\n",
      "best loss:  0.008956769287614188\n",
      "Epoch 57 of 500, Train Loss: 0.008941\n",
      "best loss:  0.008940972131383677\n",
      "Epoch 58 of 500, Train Loss: 0.008916\n",
      "best loss:  0.008916331919644735\n",
      "Epoch 59 of 500, Train Loss: 0.008891\n",
      "best loss:  0.008890903142665317\n",
      "Epoch 60 of 500, Train Loss: 0.008867\n",
      "best loss:  0.00886689572987172\n",
      "Epoch 61 of 500, Train Loss: 0.008848\n",
      "best loss:  0.008847704422100656\n",
      "Epoch 62 of 500, Train Loss: 0.008827\n",
      "best loss:  0.008826739591651943\n",
      "Epoch 63 of 500, Train Loss: 0.008811\n",
      "best loss:  0.008810798385786975\n",
      "Epoch 64 of 500, Train Loss: 0.008802\n",
      "best loss:  0.008801707053171695\n",
      "Epoch 65 of 500, Train Loss: 0.008784\n",
      "best loss:  0.008783912883693346\n",
      "Epoch 66 of 500, Train Loss: 0.008769\n",
      "best loss:  0.008769317097711926\n",
      "Epoch 67 of 500, Train Loss: 0.008758\n",
      "best loss:  0.00875825771647722\n",
      "Epoch 68 of 500, Train Loss: 0.008741\n",
      "best loss:  0.008741122739734273\n",
      "Epoch 69 of 500, Train Loss: 0.008731\n",
      "best loss:  0.008731381281242028\n",
      "Epoch 70 of 500, Train Loss: 0.008726\n",
      "best loss:  0.00872577427432497\n",
      "Epoch 71 of 500, Train Loss: 0.008711\n",
      "best loss:  0.008711332017704736\n",
      "Epoch 72 of 500, Train Loss: 0.008700\n",
      "best loss:  0.00869993915688427\n",
      "Epoch 73 of 500, Train Loss: 0.008691\n",
      "best loss:  0.008690650518008963\n",
      "Epoch 74 of 500, Train Loss: 0.008671\n",
      "best loss:  0.008670600627611262\n",
      "Epoch 75 of 500, Train Loss: 0.008656\n",
      "best loss:  0.00865633610244335\n",
      "Epoch 76 of 500, Train Loss: 0.008647\n",
      "best loss:  0.00864659286523328\n",
      "Epoch 77 of 500, Train Loss: 0.008633\n",
      "best loss:  0.00863274858911186\n",
      "Epoch 78 of 500, Train Loss: 0.008624\n",
      "best loss:  0.008623892778793328\n",
      "Epoch 79 of 500, Train Loss: 0.008617\n",
      "best loss:  0.00861700655869479\n",
      "Epoch 80 of 500, Train Loss: 0.008616\n",
      "best loss:  0.008615562403840807\n",
      "Epoch 81 of 500, Train Loss: 0.008614\n",
      "best loss:  0.008614045594996672\n",
      "Epoch 82 of 500, Train Loss: 0.008613\n",
      "best loss:  0.008612846448144862\n",
      "Epoch 83 of 500, Train Loss: 0.008630\n",
      "Epoch 84 of 500, Train Loss: 0.008642\n",
      "Epoch 85 of 500, Train Loss: 0.008653\n",
      "Epoch 86 of 500, Train Loss: 0.008646\n",
      "Epoch 87 of 500, Train Loss: 0.008634\n",
      "Epoch 88 of 500, Train Loss: 0.008620\n",
      "Epoch 89 of 500, Train Loss: 0.008604\n",
      "best loss:  0.008603752813732447\n",
      "Epoch 90 of 500, Train Loss: 0.008593\n",
      "best loss:  0.008592817381876268\n",
      "Epoch 91 of 500, Train Loss: 0.008588\n",
      "best loss:  0.008587958980978885\n",
      "Epoch 92 of 500, Train Loss: 0.008582\n",
      "best loss:  0.008582098697638861\n",
      "Epoch 93 of 500, Train Loss: 0.008585\n",
      "Epoch 94 of 500, Train Loss: 0.008577\n",
      "best loss:  0.008577317465435129\n",
      "Epoch 95 of 500, Train Loss: 0.008578\n",
      "Epoch 96 of 500, Train Loss: 0.008572\n",
      "best loss:  0.008571856795848968\n",
      "Epoch 97 of 500, Train Loss: 0.008561\n",
      "best loss:  0.00856080965807683\n",
      "Epoch 98 of 500, Train Loss: 0.008556\n",
      "best loss:  0.008556434697410192\n",
      "Epoch 99 of 500, Train Loss: 0.008560\n",
      "Epoch 100 of 500, Train Loss: 0.008571\n",
      "Epoch 101 of 500, Train Loss: 0.008591\n",
      "Epoch 102 of 500, Train Loss: 0.008605\n",
      "Epoch 103 of 500, Train Loss: 0.008641\n",
      "Epoch 104 of 500, Train Loss: 0.008652\n",
      "Epoch 105 of 500, Train Loss: 0.008674\n",
      "Epoch 106 of 500, Train Loss: 0.008704\n",
      "Epoch 107 of 500, Train Loss: 0.008721\n",
      "Epoch 108 of 500, Train Loss: 0.008767\n",
      "Epoch 109 of 500, Train Loss: 0.008791\n",
      "Epoch 110 of 500, Train Loss: 0.008814\n",
      "Epoch 111 of 500, Train Loss: 0.008825\n",
      "Epoch 112 of 500, Train Loss: 0.008773\n",
      "Epoch 113 of 500, Train Loss: 0.008750\n",
      "Epoch 114 of 500, Train Loss: 0.008709\n",
      "Epoch 115 of 500, Train Loss: 0.008716\n",
      "Epoch 116 of 500, Train Loss: 0.008712\n",
      "Epoch 117 of 500, Train Loss: 0.008766\n",
      "Epoch 118 of 500, Train Loss: 0.008787\n",
      "Epoch 119 of 500, Train Loss: 0.008871\n",
      "Epoch 120 of 500, Train Loss: 0.008895\n",
      "Epoch 121 of 500, Train Loss: 0.009031\n",
      "Epoch 122 of 500, Train Loss: 0.009085\n",
      "Epoch 123 of 500, Train Loss: 0.009221\n",
      "Epoch 124 of 500, Train Loss: 0.009284\n",
      "Epoch 125 of 500, Train Loss: 0.009388\n",
      "Epoch 126 of 500, Train Loss: 0.009370\n",
      "Epoch 127 of 500, Train Loss: 0.009304\n",
      "Epoch 128 of 500, Train Loss: 0.009243\n",
      "Epoch 129 of 500, Train Loss: 0.009171\n",
      "Epoch 130 of 500, Train Loss: 0.009129\n",
      "Epoch 131 of 500, Train Loss: 0.009111\n",
      "Epoch 132 of 500, Train Loss: 0.009061\n",
      "Epoch 133 of 500, Train Loss: 0.009039\n",
      "Epoch 134 of 500, Train Loss: 0.008969\n",
      "Epoch 135 of 500, Train Loss: 0.008944\n",
      "Epoch 136 of 500, Train Loss: 0.008858\n",
      "Epoch 137 of 500, Train Loss: 0.008842\n",
      "Epoch 138 of 500, Train Loss: 0.008768\n",
      "Epoch 139 of 500, Train Loss: 0.008767\n",
      "Epoch 140 of 500, Train Loss: 0.008724\n",
      "Epoch 141 of 500, Train Loss: 0.008740\n",
      "Epoch 142 of 500, Train Loss: 0.008724\n",
      "Epoch 143 of 500, Train Loss: 0.008755\n",
      "Epoch 144 of 500, Train Loss: 0.008751\n",
      "Epoch 145 of 500, Train Loss: 0.008803\n",
      "Epoch 146 of 500, Train Loss: 0.008797\n",
      "Epoch 147 of 500, Train Loss: 0.008837\n",
      "Epoch 148 of 500, Train Loss: 0.008819\n",
      "Epoch 149 of 500, Train Loss: 0.008837\n",
      "Epoch 150 of 500, Train Loss: 0.008821\n",
      "Epoch 151 of 500, Train Loss: 0.008828\n",
      "Epoch 152 of 500, Train Loss: 0.008800\n",
      "Epoch 153 of 500, Train Loss: 0.008793\n",
      "Epoch 154 of 500, Train Loss: 0.008768\n",
      "Epoch 155 of 500, Train Loss: 0.008765\n",
      "Epoch 156 of 500, Train Loss: 0.008756\n",
      "Epoch 157 of 500, Train Loss: 0.008747\n",
      "Epoch 158 of 500, Train Loss: 0.008754\n",
      "Epoch 159 of 500, Train Loss: 0.008742\n",
      "Epoch 160 of 500, Train Loss: 0.008753\n",
      "Epoch 161 of 500, Train Loss: 0.008742\n",
      "Epoch 162 of 500, Train Loss: 0.008753\n",
      "Epoch 163 of 500, Train Loss: 0.008748\n",
      "Epoch 164 of 500, Train Loss: 0.008754\n",
      "Epoch 165 of 500, Train Loss: 0.008753\n",
      "Epoch 166 of 500, Train Loss: 0.008758\n",
      "Epoch 167 of 500, Train Loss: 0.008763\n",
      "Epoch 168 of 500, Train Loss: 0.008761\n",
      "Epoch 169 of 500, Train Loss: 0.008764\n",
      "Epoch 170 of 500, Train Loss: 0.008763\n",
      "Epoch 171 of 500, Train Loss: 0.008772\n",
      "Epoch 172 of 500, Train Loss: 0.008774\n",
      "Epoch 173 of 500, Train Loss: 0.008789\n",
      "Epoch 174 of 500, Train Loss: 0.008793\n",
      "Epoch 175 of 500, Train Loss: 0.008813\n",
      "Epoch 176 of 500, Train Loss: 0.008816\n",
      "Epoch 177 of 500, Train Loss: 0.008851\n",
      "Epoch 178 of 500, Train Loss: 0.008844\n",
      "Epoch 179 of 500, Train Loss: 0.008891\n",
      "Epoch 180 of 500, Train Loss: 0.008876\n",
      "Epoch 181 of 500, Train Loss: 0.008935\n",
      "Epoch 182 of 500, Train Loss: 0.008916\n",
      "Epoch 183 of 500, Train Loss: 0.008979\n",
      "Epoch 184 of 500, Train Loss: 0.008949\n",
      "Epoch 185 of 500, Train Loss: 0.009001\n",
      "Epoch 186 of 500, Train Loss: 0.008965\n",
      "Epoch 187 of 500, Train Loss: 0.008993\n",
      "Epoch 188 of 500, Train Loss: 0.008961\n",
      "Epoch 189 of 500, Train Loss: 0.008943\n",
      "Epoch 190 of 500, Train Loss: 0.008915\n",
      "Epoch 191 of 500, Train Loss: 0.008866\n",
      "Epoch 192 of 500, Train Loss: 0.008828\n",
      "Epoch 193 of 500, Train Loss: 0.008786\n",
      "Epoch 194 of 500, Train Loss: 0.008753\n",
      "Epoch 195 of 500, Train Loss: 0.008732\n",
      "Epoch 196 of 500, Train Loss: 0.008714\n",
      "Epoch 197 of 500, Train Loss: 0.008701\n",
      "Epoch 198 of 500, Train Loss: 0.008695\n",
      "Epoch 199 of 500, Train Loss: 0.008682\n",
      "Epoch 200 of 500, Train Loss: 0.008682\n",
      "Epoch 201 of 500, Train Loss: 0.008676\n",
      "Epoch 202 of 500, Train Loss: 0.008686\n",
      "Epoch 203 of 500, Train Loss: 0.008697\n",
      "Epoch 204 of 500, Train Loss: 0.008714\n",
      "Epoch 205 of 500, Train Loss: 0.008737\n",
      "Epoch 206 of 500, Train Loss: 0.008748\n",
      "Epoch 207 of 500, Train Loss: 0.008767\n",
      "Epoch 208 of 500, Train Loss: 0.008766\n",
      "Epoch 209 of 500, Train Loss: 0.008775\n",
      "Epoch 210 of 500, Train Loss: 0.008768\n",
      "Epoch 211 of 500, Train Loss: 0.008771\n",
      "Epoch 212 of 500, Train Loss: 0.008764\n",
      "Epoch 213 of 500, Train Loss: 0.008760\n",
      "Epoch 214 of 500, Train Loss: 0.008758\n",
      "Epoch 215 of 500, Train Loss: 0.008752\n",
      "Epoch 216 of 500, Train Loss: 0.008756\n",
      "Epoch 217 of 500, Train Loss: 0.008750\n",
      "Epoch 218 of 500, Train Loss: 0.008753\n",
      "Epoch 219 of 500, Train Loss: 0.008740\n",
      "Epoch 220 of 500, Train Loss: 0.008747\n",
      "Epoch 221 of 500, Train Loss: 0.008730\n",
      "Epoch 222 of 500, Train Loss: 0.008743\n",
      "Epoch 223 of 500, Train Loss: 0.008724\n",
      "Epoch 224 of 500, Train Loss: 0.008737\n",
      "Epoch 225 of 500, Train Loss: 0.008713\n",
      "Epoch 226 of 500, Train Loss: 0.008725\n",
      "Epoch 227 of 500, Train Loss: 0.008701\n",
      "Epoch 228 of 500, Train Loss: 0.008713\n",
      "Epoch 229 of 500, Train Loss: 0.008691\n",
      "Epoch 230 of 500, Train Loss: 0.008704\n",
      "Epoch 231 of 500, Train Loss: 0.008684\n",
      "Epoch 232 of 500, Train Loss: 0.008699\n",
      "Epoch 233 of 500, Train Loss: 0.008681\n",
      "Epoch 234 of 500, Train Loss: 0.008698\n",
      "Epoch 235 of 500, Train Loss: 0.008686\n",
      "Epoch 236 of 500, Train Loss: 0.008702\n",
      "Epoch 237 of 500, Train Loss: 0.008693\n",
      "Epoch 238 of 500, Train Loss: 0.008708\n",
      "Epoch 239 of 500, Train Loss: 0.008697\n",
      "Epoch 240 of 500, Train Loss: 0.008710\n",
      "Epoch 241 of 500, Train Loss: 0.008702\n",
      "Epoch 242 of 500, Train Loss: 0.008712\n",
      "Epoch 243 of 500, Train Loss: 0.008701\n",
      "Epoch 244 of 500, Train Loss: 0.008704\n",
      "Epoch 245 of 500, Train Loss: 0.008698\n",
      "Epoch 246 of 500, Train Loss: 0.008694\n",
      "Epoch 247 of 500, Train Loss: 0.008692\n",
      "Epoch 248 of 500, Train Loss: 0.008683\n",
      "Epoch 249 of 500, Train Loss: 0.008688\n",
      "Epoch 250 of 500, Train Loss: 0.008673\n",
      "Epoch 251 of 500, Train Loss: 0.008679\n",
      "Epoch 252 of 500, Train Loss: 0.008662\n",
      "Epoch 253 of 500, Train Loss: 0.008668\n",
      "Epoch 254 of 500, Train Loss: 0.008656\n",
      "Epoch 255 of 500, Train Loss: 0.008657\n",
      "Epoch 256 of 500, Train Loss: 0.008651\n",
      "Epoch 257 of 500, Train Loss: 0.008645\n",
      "Epoch 258 of 500, Train Loss: 0.008646\n",
      "Epoch 259 of 500, Train Loss: 0.008645\n",
      "Epoch 260 of 500, Train Loss: 0.008647\n",
      "Epoch 261 of 500, Train Loss: 0.008649\n",
      "Epoch 262 of 500, Train Loss: 0.008651\n",
      "Epoch 263 of 500, Train Loss: 0.008649\n",
      "Epoch 264 of 500, Train Loss: 0.008652\n",
      "Epoch 265 of 500, Train Loss: 0.008645\n",
      "Epoch 266 of 500, Train Loss: 0.008652\n",
      "Epoch 267 of 500, Train Loss: 0.008645\n",
      "Epoch 268 of 500, Train Loss: 0.008653\n",
      "Epoch 269 of 500, Train Loss: 0.008644\n",
      "Epoch 270 of 500, Train Loss: 0.008650\n",
      "Epoch 271 of 500, Train Loss: 0.008638\n",
      "Epoch 272 of 500, Train Loss: 0.008643\n",
      "Epoch 273 of 500, Train Loss: 0.008631\n",
      "Epoch 274 of 500, Train Loss: 0.008637\n",
      "Epoch 275 of 500, Train Loss: 0.008627\n",
      "Epoch 276 of 500, Train Loss: 0.008633\n",
      "Epoch 277 of 500, Train Loss: 0.008626\n",
      "Epoch 278 of 500, Train Loss: 0.008632\n",
      "Epoch 279 of 500, Train Loss: 0.008628\n",
      "Epoch 280 of 500, Train Loss: 0.008633\n",
      "Epoch 281 of 500, Train Loss: 0.008632\n",
      "Epoch 282 of 500, Train Loss: 0.008635\n",
      "Epoch 283 of 500, Train Loss: 0.008636\n",
      "Epoch 284 of 500, Train Loss: 0.008636\n",
      "Epoch 285 of 500, Train Loss: 0.008634\n",
      "Epoch 286 of 500, Train Loss: 0.008633\n",
      "Epoch 287 of 500, Train Loss: 0.008624\n",
      "Epoch 288 of 500, Train Loss: 0.008623\n",
      "Epoch 289 of 500, Train Loss: 0.008611\n",
      "Epoch 290 of 500, Train Loss: 0.008613\n",
      "Epoch 291 of 500, Train Loss: 0.008603\n",
      "Epoch 292 of 500, Train Loss: 0.008607\n",
      "Epoch 293 of 500, Train Loss: 0.008597\n",
      "Epoch 294 of 500, Train Loss: 0.008602\n",
      "Epoch 295 of 500, Train Loss: 0.008592\n",
      "Epoch 296 of 500, Train Loss: 0.008596\n",
      "Epoch 297 of 500, Train Loss: 0.008587\n",
      "Epoch 298 of 500, Train Loss: 0.008595\n",
      "Epoch 299 of 500, Train Loss: 0.008586\n",
      "Epoch 300 of 500, Train Loss: 0.008597\n",
      "Epoch 301 of 500, Train Loss: 0.008585\n",
      "Epoch 302 of 500, Train Loss: 0.008596\n",
      "Epoch 303 of 500, Train Loss: 0.008585\n",
      "Epoch 304 of 500, Train Loss: 0.008600\n",
      "Epoch 305 of 500, Train Loss: 0.008588\n",
      "Epoch 306 of 500, Train Loss: 0.008607\n",
      "Epoch 307 of 500, Train Loss: 0.008592\n",
      "Epoch 308 of 500, Train Loss: 0.008611\n",
      "Epoch 309 of 500, Train Loss: 0.008594\n",
      "Epoch 310 of 500, Train Loss: 0.008614\n",
      "Epoch 311 of 500, Train Loss: 0.008601\n",
      "Epoch 312 of 500, Train Loss: 0.008621\n",
      "Epoch 313 of 500, Train Loss: 0.008607\n",
      "Epoch 314 of 500, Train Loss: 0.008623\n",
      "Epoch 315 of 500, Train Loss: 0.008607\n",
      "Epoch 316 of 500, Train Loss: 0.008614\n",
      "Epoch 317 of 500, Train Loss: 0.008600\n",
      "Epoch 318 of 500, Train Loss: 0.008605\n",
      "Epoch 319 of 500, Train Loss: 0.008593\n",
      "Epoch 320 of 500, Train Loss: 0.008601\n",
      "Epoch 321 of 500, Train Loss: 0.008586\n",
      "Epoch 322 of 500, Train Loss: 0.008594\n",
      "Epoch 323 of 500, Train Loss: 0.008581\n",
      "Epoch 324 of 500, Train Loss: 0.008589\n",
      "Epoch 325 of 500, Train Loss: 0.008578\n",
      "Epoch 326 of 500, Train Loss: 0.008590\n",
      "Epoch 327 of 500, Train Loss: 0.008579\n",
      "Epoch 328 of 500, Train Loss: 0.008588\n",
      "Epoch 329 of 500, Train Loss: 0.008582\n",
      "Epoch 330 of 500, Train Loss: 0.008591\n",
      "Epoch 331 of 500, Train Loss: 0.008593\n",
      "Epoch 332 of 500, Train Loss: 0.008605\n",
      "Epoch 333 of 500, Train Loss: 0.008611\n",
      "Epoch 334 of 500, Train Loss: 0.008618\n",
      "Epoch 335 of 500, Train Loss: 0.008623\n",
      "Epoch 336 of 500, Train Loss: 0.008630\n",
      "Epoch 337 of 500, Train Loss: 0.008637\n",
      "Epoch 338 of 500, Train Loss: 0.008642\n",
      "Epoch 339 of 500, Train Loss: 0.008646\n",
      "Epoch 340 of 500, Train Loss: 0.008645\n",
      "Epoch 341 of 500, Train Loss: 0.008650\n",
      "Epoch 342 of 500, Train Loss: 0.008648\n",
      "Epoch 343 of 500, Train Loss: 0.008669\n",
      "Epoch 344 of 500, Train Loss: 0.008663\n",
      "Epoch 345 of 500, Train Loss: 0.008692\n",
      "Epoch 346 of 500, Train Loss: 0.008673\n",
      "Epoch 347 of 500, Train Loss: 0.008702\n",
      "Epoch 348 of 500, Train Loss: 0.008669\n",
      "Epoch 349 of 500, Train Loss: 0.008703\n",
      "Epoch 350 of 500, Train Loss: 0.008672\n",
      "Epoch 351 of 500, Train Loss: 0.008695\n",
      "Epoch 352 of 500, Train Loss: 0.008668\n",
      "Epoch 353 of 500, Train Loss: 0.008665\n",
      "Epoch 354 of 500, Train Loss: 0.008643\n",
      "Epoch 355 of 500, Train Loss: 0.008650\n",
      "Epoch 356 of 500, Train Loss: 0.008634\n",
      "Epoch 357 of 500, Train Loss: 0.008659\n",
      "Epoch 358 of 500, Train Loss: 0.008640\n",
      "Epoch 359 of 500, Train Loss: 0.008655\n",
      "Epoch 360 of 500, Train Loss: 0.008632\n",
      "Epoch 361 of 500, Train Loss: 0.008642\n",
      "Epoch 362 of 500, Train Loss: 0.008624\n",
      "Epoch 363 of 500, Train Loss: 0.008639\n",
      "Epoch 364 of 500, Train Loss: 0.008622\n",
      "Epoch 365 of 500, Train Loss: 0.008627\n",
      "Epoch 366 of 500, Train Loss: 0.008610\n",
      "Epoch 367 of 500, Train Loss: 0.008611\n",
      "Epoch 368 of 500, Train Loss: 0.008600\n",
      "Epoch 369 of 500, Train Loss: 0.008613\n",
      "Epoch 370 of 500, Train Loss: 0.008604\n",
      "Epoch 371 of 500, Train Loss: 0.008612\n",
      "Epoch 372 of 500, Train Loss: 0.008594\n",
      "Epoch 373 of 500, Train Loss: 0.008599\n",
      "Epoch 374 of 500, Train Loss: 0.008579\n",
      "Epoch 375 of 500, Train Loss: 0.008596\n",
      "Epoch 376 of 500, Train Loss: 0.008575\n",
      "Epoch 377 of 500, Train Loss: 0.008591\n",
      "Epoch 378 of 500, Train Loss: 0.008571\n",
      "Epoch 379 of 500, Train Loss: 0.008587\n",
      "Epoch 380 of 500, Train Loss: 0.008572\n",
      "Epoch 381 of 500, Train Loss: 0.008597\n",
      "Epoch 382 of 500, Train Loss: 0.008587\n",
      "Epoch 383 of 500, Train Loss: 0.008617\n",
      "Epoch 384 of 500, Train Loss: 0.008609\n",
      "Epoch 385 of 500, Train Loss: 0.008633\n",
      "Epoch 386 of 500, Train Loss: 0.008628\n",
      "Epoch 387 of 500, Train Loss: 0.008651\n",
      "Epoch 388 of 500, Train Loss: 0.008653\n",
      "Epoch 389 of 500, Train Loss: 0.008667\n",
      "Epoch 390 of 500, Train Loss: 0.008676\n",
      "Epoch 391 of 500, Train Loss: 0.008662\n",
      "Epoch 392 of 500, Train Loss: 0.008671\n",
      "Epoch 393 of 500, Train Loss: 0.008652\n",
      "Epoch 394 of 500, Train Loss: 0.008663\n",
      "Epoch 395 of 500, Train Loss: 0.008659\n",
      "Epoch 396 of 500, Train Loss: 0.008669\n",
      "Epoch 397 of 500, Train Loss: 0.008654\n",
      "Epoch 398 of 500, Train Loss: 0.008659\n",
      "Epoch 399 of 500, Train Loss: 0.008641\n",
      "Epoch 400 of 500, Train Loss: 0.008646\n",
      "Epoch 401 of 500, Train Loss: 0.008644\n",
      "Epoch 402 of 500, Train Loss: 0.008645\n",
      "Epoch 403 of 500, Train Loss: 0.008635\n",
      "Epoch 404 of 500, Train Loss: 0.008633\n",
      "Epoch 405 of 500, Train Loss: 0.008624\n",
      "Epoch 406 of 500, Train Loss: 0.008625\n",
      "Epoch 407 of 500, Train Loss: 0.008640\n",
      "Epoch 408 of 500, Train Loss: 0.008642\n",
      "Epoch 409 of 500, Train Loss: 0.008651\n",
      "Epoch 410 of 500, Train Loss: 0.008640\n",
      "Epoch 411 of 500, Train Loss: 0.008632\n",
      "Epoch 412 of 500, Train Loss: 0.008625\n",
      "Epoch 413 of 500, Train Loss: 0.008634\n",
      "Epoch 414 of 500, Train Loss: 0.008639\n",
      "Epoch 415 of 500, Train Loss: 0.008658\n",
      "Epoch 416 of 500, Train Loss: 0.008650\n",
      "Epoch 417 of 500, Train Loss: 0.008649\n",
      "Epoch 418 of 500, Train Loss: 0.008635\n",
      "Epoch 419 of 500, Train Loss: 0.008640\n",
      "Epoch 420 of 500, Train Loss: 0.008640\n",
      "Epoch 421 of 500, Train Loss: 0.008675\n",
      "Epoch 422 of 500, Train Loss: 0.008674\n",
      "Epoch 423 of 500, Train Loss: 0.008695\n",
      "Epoch 424 of 500, Train Loss: 0.008673\n",
      "Epoch 425 of 500, Train Loss: 0.008679\n",
      "Epoch 426 of 500, Train Loss: 0.008655\n",
      "Epoch 427 of 500, Train Loss: 0.008689\n",
      "Epoch 428 of 500, Train Loss: 0.008676\n",
      "Epoch 429 of 500, Train Loss: 0.008697\n",
      "Epoch 430 of 500, Train Loss: 0.008677\n",
      "Epoch 431 of 500, Train Loss: 0.008678\n",
      "Epoch 432 of 500, Train Loss: 0.008667\n",
      "Epoch 433 of 500, Train Loss: 0.008696\n",
      "Epoch 434 of 500, Train Loss: 0.008688\n",
      "Epoch 435 of 500, Train Loss: 0.008711\n",
      "Epoch 436 of 500, Train Loss: 0.008677\n",
      "Epoch 437 of 500, Train Loss: 0.008691\n",
      "Epoch 438 of 500, Train Loss: 0.008685\n",
      "Epoch 439 of 500, Train Loss: 0.008735\n",
      "Epoch 440 of 500, Train Loss: 0.008733\n",
      "Epoch 441 of 500, Train Loss: 0.008774\n",
      "Epoch 442 of 500, Train Loss: 0.008735\n",
      "Epoch 443 of 500, Train Loss: 0.008758\n",
      "Epoch 444 of 500, Train Loss: 0.008741\n",
      "Epoch 445 of 500, Train Loss: 0.008790\n",
      "Epoch 446 of 500, Train Loss: 0.008792\n",
      "Epoch 447 of 500, Train Loss: 0.008822\n",
      "Epoch 448 of 500, Train Loss: 0.008799\n",
      "Epoch 449 of 500, Train Loss: 0.008771\n",
      "Epoch 450 of 500, Train Loss: 0.008752\n",
      "Epoch 451 of 500, Train Loss: 0.008740\n",
      "Epoch 452 of 500, Train Loss: 0.008741\n",
      "Epoch 453 of 500, Train Loss: 0.008739\n",
      "Epoch 454 of 500, Train Loss: 0.008743\n",
      "Epoch 455 of 500, Train Loss: 0.008735\n",
      "Epoch 456 of 500, Train Loss: 0.008729\n",
      "Epoch 457 of 500, Train Loss: 0.008720\n",
      "Epoch 458 of 500, Train Loss: 0.008715\n",
      "Epoch 459 of 500, Train Loss: 0.008709\n",
      "Epoch 460 of 500, Train Loss: 0.008712\n",
      "Epoch 461 of 500, Train Loss: 0.008706\n",
      "Epoch 462 of 500, Train Loss: 0.008712\n",
      "Epoch 463 of 500, Train Loss: 0.008703\n",
      "Epoch 464 of 500, Train Loss: 0.008703\n",
      "Epoch 465 of 500, Train Loss: 0.008696\n",
      "Epoch 466 of 500, Train Loss: 0.008689\n",
      "Epoch 467 of 500, Train Loss: 0.008681\n",
      "Epoch 468 of 500, Train Loss: 0.008670\n",
      "Epoch 469 of 500, Train Loss: 0.008662\n",
      "Epoch 470 of 500, Train Loss: 0.008650\n",
      "Epoch 471 of 500, Train Loss: 0.008646\n",
      "Epoch 472 of 500, Train Loss: 0.008640\n",
      "Epoch 473 of 500, Train Loss: 0.008646\n",
      "Epoch 474 of 500, Train Loss: 0.008648\n",
      "Epoch 475 of 500, Train Loss: 0.008660\n",
      "Epoch 476 of 500, Train Loss: 0.008666\n",
      "Epoch 477 of 500, Train Loss: 0.008682\n",
      "Epoch 478 of 500, Train Loss: 0.008684\n",
      "Epoch 479 of 500, Train Loss: 0.008705\n",
      "Epoch 480 of 500, Train Loss: 0.008698\n",
      "Epoch 481 of 500, Train Loss: 0.008722\n",
      "Epoch 482 of 500, Train Loss: 0.008701\n",
      "Epoch 483 of 500, Train Loss: 0.008721\n",
      "Epoch 484 of 500, Train Loss: 0.008688\n",
      "Epoch 485 of 500, Train Loss: 0.008702\n",
      "Epoch 486 of 500, Train Loss: 0.008671\n",
      "Epoch 487 of 500, Train Loss: 0.008678\n",
      "Epoch 488 of 500, Train Loss: 0.008656\n",
      "Epoch 489 of 500, Train Loss: 0.008661\n",
      "Epoch 490 of 500, Train Loss: 0.008648\n",
      "Epoch 491 of 500, Train Loss: 0.008652\n",
      "Epoch 492 of 500, Train Loss: 0.008644\n",
      "Epoch 493 of 500, Train Loss: 0.008643\n",
      "Epoch 494 of 500, Train Loss: 0.008640\n",
      "Epoch 495 of 500, Train Loss: 0.008635\n",
      "Epoch 496 of 500, Train Loss: 0.008636\n",
      "Epoch 497 of 500, Train Loss: 0.008629\n",
      "Epoch 498 of 500, Train Loss: 0.008624\n",
      "Epoch 499 of 500, Train Loss: 0.008617\n",
      "Epoch 500 of 500, Train Loss: 0.008605\n",
      "latent train shape:  (16395, 90)\n",
      "M: 90, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 5\n",
      "Training the subspace: 0 / 90\n",
      "Training the subspace: 1 / 90\n",
      "Training the subspace: 2 / 90\n",
      "Training the subspace: 3 / 90\n",
      "Training the subspace: 4 / 90\n",
      "Training the subspace: 5 / 90\n",
      "Training the subspace: 6 / 90\n",
      "Training the subspace: 7 / 90\n",
      "Training the subspace: 8 / 90\n",
      "Training the subspace: 9 / 90\n",
      "Training the subspace: 10 / 90\n",
      "Training the subspace: 11 / 90\n",
      "Training the subspace: 12 / 90\n",
      "Training the subspace: 13 / 90\n",
      "Training the subspace: 14 / 90\n",
      "Training the subspace: 15 / 90\n",
      "Training the subspace: 16 / 90\n",
      "Training the subspace: 17 / 90\n",
      "Training the subspace: 18 / 90\n",
      "Training the subspace: 19 / 90\n",
      "Training the subspace: 20 / 90\n",
      "Training the subspace: 21 / 90\n",
      "Training the subspace: 22 / 90\n",
      "Training the subspace: 23 / 90\n",
      "Training the subspace: 24 / 90\n",
      "Training the subspace: 25 / 90\n",
      "Training the subspace: 26 / 90\n",
      "Training the subspace: 27 / 90\n",
      "Training the subspace: 28 / 90\n",
      "Training the subspace: 29 / 90\n",
      "Training the subspace: 30 / 90\n",
      "Training the subspace: 31 / 90\n",
      "Training the subspace: 32 / 90\n",
      "Training the subspace: 33 / 90\n",
      "Training the subspace: 34 / 90\n",
      "Training the subspace: 35 / 90\n",
      "Training the subspace: 36 / 90\n",
      "Training the subspace: 37 / 90\n",
      "Training the subspace: 38 / 90\n",
      "Training the subspace: 39 / 90\n",
      "Training the subspace: 40 / 90\n",
      "Training the subspace: 41 / 90\n",
      "Training the subspace: 42 / 90\n",
      "Training the subspace: 43 / 90\n",
      "Training the subspace: 44 / 90\n",
      "Training the subspace: 45 / 90\n",
      "Training the subspace: 46 / 90\n",
      "Training the subspace: 47 / 90\n",
      "Training the subspace: 48 / 90\n",
      "Training the subspace: 49 / 90\n",
      "Training the subspace: 50 / 90\n",
      "Training the subspace: 51 / 90\n",
      "Training the subspace: 52 / 90\n",
      "Training the subspace: 53 / 90\n",
      "Training the subspace: 54 / 90\n",
      "Training the subspace: 55 / 90\n",
      "Training the subspace: 56 / 90\n",
      "Training the subspace: 57 / 90\n",
      "Training the subspace: 58 / 90\n",
      "Training the subspace: 59 / 90\n",
      "Training the subspace: 60 / 90\n",
      "Training the subspace: 61 / 90\n",
      "Training the subspace: 62 / 90\n",
      "Training the subspace: 63 / 90\n",
      "Training the subspace: 64 / 90\n",
      "Training the subspace: 65 / 90\n",
      "Training the subspace: 66 / 90\n",
      "Training the subspace: 67 / 90\n",
      "Training the subspace: 68 / 90\n",
      "Training the subspace: 69 / 90\n",
      "Training the subspace: 70 / 90\n",
      "Training the subspace: 71 / 90\n",
      "Training the subspace: 72 / 90\n",
      "Training the subspace: 73 / 90\n",
      "Training the subspace: 74 / 90\n",
      "Training the subspace: 75 / 90\n",
      "Training the subspace: 76 / 90\n",
      "Training the subspace: 77 / 90\n",
      "Training the subspace: 78 / 90\n",
      "Training the subspace: 79 / 90\n",
      "Training the subspace: 80 / 90\n",
      "Training the subspace: 81 / 90\n",
      "Training the subspace: 82 / 90\n",
      "Training the subspace: 83 / 90\n",
      "Training the subspace: 84 / 90\n",
      "Training the subspace: 85 / 90\n",
      "Training the subspace: 86 / 90\n",
      "Training the subspace: 87 / 90\n",
      "Training the subspace: 88 / 90\n",
      "Training the subspace: 89 / 90\n",
      "Encoding the subspace: 0 / 90\n",
      "Encoding the subspace: 1 / 90\n",
      "Encoding the subspace: 2 / 90\n",
      "Encoding the subspace: 3 / 90\n",
      "Encoding the subspace: 4 / 90\n",
      "Encoding the subspace: 5 / 90\n",
      "Encoding the subspace: 6 / 90\n",
      "Encoding the subspace: 7 / 90\n",
      "Encoding the subspace: 8 / 90\n",
      "Encoding the subspace: 9 / 90\n",
      "Encoding the subspace: 10 / 90\n",
      "Encoding the subspace: 11 / 90\n",
      "Encoding the subspace: 12 / 90\n",
      "Encoding the subspace: 13 / 90\n",
      "Encoding the subspace: 14 / 90\n",
      "Encoding the subspace: 15 / 90\n",
      "Encoding the subspace: 16 / 90\n",
      "Encoding the subspace: 17 / 90\n",
      "Encoding the subspace: 18 / 90\n",
      "Encoding the subspace: 19 / 90\n",
      "Encoding the subspace: 20 / 90\n",
      "Encoding the subspace: 21 / 90\n",
      "Encoding the subspace: 22 / 90\n",
      "Encoding the subspace: 23 / 90\n",
      "Encoding the subspace: 24 / 90\n",
      "Encoding the subspace: 25 / 90\n",
      "Encoding the subspace: 26 / 90\n",
      "Encoding the subspace: 27 / 90\n",
      "Encoding the subspace: 28 / 90\n",
      "Encoding the subspace: 29 / 90\n",
      "Encoding the subspace: 30 / 90\n",
      "Encoding the subspace: 31 / 90\n",
      "Encoding the subspace: 32 / 90\n",
      "Encoding the subspace: 33 / 90\n",
      "Encoding the subspace: 34 / 90\n",
      "Encoding the subspace: 35 / 90\n",
      "Encoding the subspace: 36 / 90\n",
      "Encoding the subspace: 37 / 90\n",
      "Encoding the subspace: 38 / 90\n",
      "Encoding the subspace: 39 / 90\n",
      "Encoding the subspace: 40 / 90\n",
      "Encoding the subspace: 41 / 90\n",
      "Encoding the subspace: 42 / 90\n",
      "Encoding the subspace: 43 / 90\n",
      "Encoding the subspace: 44 / 90\n",
      "Encoding the subspace: 45 / 90\n",
      "Encoding the subspace: 46 / 90\n",
      "Encoding the subspace: 47 / 90\n",
      "Encoding the subspace: 48 / 90\n",
      "Encoding the subspace: 49 / 90\n",
      "Encoding the subspace: 50 / 90\n",
      "Encoding the subspace: 51 / 90\n",
      "Encoding the subspace: 52 / 90\n",
      "Encoding the subspace: 53 / 90\n",
      "Encoding the subspace: 54 / 90\n",
      "Encoding the subspace: 55 / 90\n",
      "Encoding the subspace: 56 / 90\n",
      "Encoding the subspace: 57 / 90\n",
      "Encoding the subspace: 58 / 90\n",
      "Encoding the subspace: 59 / 90\n",
      "Encoding the subspace: 60 / 90\n",
      "Encoding the subspace: 61 / 90\n",
      "Encoding the subspace: 62 / 90\n",
      "Encoding the subspace: 63 / 90\n",
      "Encoding the subspace: 64 / 90\n",
      "Encoding the subspace: 65 / 90\n",
      "Encoding the subspace: 66 / 90\n",
      "Encoding the subspace: 67 / 90\n",
      "Encoding the subspace: 68 / 90\n",
      "Encoding the subspace: 69 / 90\n",
      "Encoding the subspace: 70 / 90\n",
      "Encoding the subspace: 71 / 90\n",
      "Encoding the subspace: 72 / 90\n",
      "Encoding the subspace: 73 / 90\n",
      "Encoding the subspace: 74 / 90\n",
      "Encoding the subspace: 75 / 90\n",
      "Encoding the subspace: 76 / 90\n",
      "Encoding the subspace: 77 / 90\n",
      "Encoding the subspace: 78 / 90\n",
      "Encoding the subspace: 79 / 90\n",
      "Encoding the subspace: 80 / 90\n",
      "Encoding the subspace: 81 / 90\n",
      "Encoding the subspace: 82 / 90\n",
      "Encoding the subspace: 83 / 90\n",
      "Encoding the subspace: 84 / 90\n",
      "Encoding the subspace: 85 / 90\n",
      "Encoding the subspace: 86 / 90\n",
      "Encoding the subspace: 87 / 90\n",
      "Encoding the subspace: 88 / 90\n",
      "Encoding the subspace: 89 / 90\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=120, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.269082\n",
      "best loss:  0.2690822406448135\n",
      "Epoch 2 of 500, Train Loss: 0.050834\n",
      "best loss:  0.050833529394296854\n",
      "Epoch 3 of 500, Train Loss: 0.041283\n",
      "best loss:  0.04128264594875464\n",
      "Epoch 4 of 500, Train Loss: 0.032432\n",
      "best loss:  0.03243201181550633\n",
      "Epoch 5 of 500, Train Loss: 0.027326\n",
      "best loss:  0.02732603423398479\n",
      "Epoch 6 of 500, Train Loss: 0.024304\n",
      "best loss:  0.024303743245744313\n",
      "Epoch 7 of 500, Train Loss: 0.022073\n",
      "best loss:  0.02207308967395635\n",
      "Epoch 8 of 500, Train Loss: 0.020259\n",
      "best loss:  0.020258850633264193\n",
      "Epoch 9 of 500, Train Loss: 0.018818\n",
      "best loss:  0.018818417134019726\n",
      "Epoch 10 of 500, Train Loss: 0.017646\n",
      "best loss:  0.017646406451646283\n",
      "Epoch 11 of 500, Train Loss: 0.016621\n",
      "best loss:  0.016621311132926062\n",
      "Epoch 12 of 500, Train Loss: 0.015689\n",
      "best loss:  0.015689145290841484\n",
      "Epoch 13 of 500, Train Loss: 0.014864\n",
      "best loss:  0.014863744675219708\n",
      "Epoch 14 of 500, Train Loss: 0.014142\n",
      "best loss:  0.014141792297911968\n",
      "Epoch 15 of 500, Train Loss: 0.013500\n",
      "best loss:  0.01349964664997208\n",
      "Epoch 16 of 500, Train Loss: 0.012913\n",
      "best loss:  0.01291256800723486\n",
      "Epoch 17 of 500, Train Loss: 0.012416\n",
      "best loss:  0.012415532631413333\n",
      "Epoch 18 of 500, Train Loss: 0.011967\n",
      "best loss:  0.011967221616998373\n",
      "Epoch 19 of 500, Train Loss: 0.011578\n",
      "best loss:  0.011578398954092576\n",
      "Epoch 20 of 500, Train Loss: 0.011218\n",
      "best loss:  0.011218158329896586\n",
      "Epoch 21 of 500, Train Loss: 0.010909\n",
      "best loss:  0.0109089970245304\n",
      "Epoch 22 of 500, Train Loss: 0.010614\n",
      "best loss:  0.010613880445548991\n",
      "Epoch 23 of 500, Train Loss: 0.010342\n",
      "best loss:  0.010342327715890382\n",
      "Epoch 24 of 500, Train Loss: 0.010110\n",
      "best loss:  0.010110349289020668\n",
      "Epoch 25 of 500, Train Loss: 0.009885\n",
      "best loss:  0.009885038948588998\n",
      "Epoch 26 of 500, Train Loss: 0.009673\n",
      "best loss:  0.00967329039347589\n",
      "Epoch 27 of 500, Train Loss: 0.009482\n",
      "best loss:  0.00948248256147702\n",
      "Epoch 28 of 500, Train Loss: 0.009301\n",
      "best loss:  0.0093012893826939\n",
      "Epoch 29 of 500, Train Loss: 0.009131\n",
      "best loss:  0.009130616312148952\n",
      "Epoch 30 of 500, Train Loss: 0.008992\n",
      "best loss:  0.008991868125636657\n",
      "Epoch 31 of 500, Train Loss: 0.008877\n",
      "best loss:  0.00887747847778784\n",
      "Epoch 32 of 500, Train Loss: 0.008745\n",
      "best loss:  0.008745329282399945\n",
      "Epoch 33 of 500, Train Loss: 0.008654\n",
      "best loss:  0.008654122560193885\n",
      "Epoch 34 of 500, Train Loss: 0.008527\n",
      "best loss:  0.008526906450418936\n",
      "Epoch 35 of 500, Train Loss: 0.008443\n",
      "best loss:  0.008443038659409418\n",
      "Epoch 36 of 500, Train Loss: 0.008342\n",
      "best loss:  0.008342474357219586\n",
      "Epoch 37 of 500, Train Loss: 0.008253\n",
      "best loss:  0.008252651052429456\n",
      "Epoch 38 of 500, Train Loss: 0.008198\n",
      "best loss:  0.008197607266443219\n",
      "Epoch 39 of 500, Train Loss: 0.008117\n",
      "best loss:  0.008117133595614312\n",
      "Epoch 40 of 500, Train Loss: 0.008062\n",
      "best loss:  0.008062445657086819\n",
      "Epoch 41 of 500, Train Loss: 0.008013\n",
      "best loss:  0.008013321739795631\n",
      "Epoch 42 of 500, Train Loss: 0.007964\n",
      "best loss:  0.007964178962319865\n",
      "Epoch 43 of 500, Train Loss: 0.007931\n",
      "best loss:  0.00793117427473794\n",
      "Epoch 44 of 500, Train Loss: 0.007872\n",
      "best loss:  0.007872450139392492\n",
      "Epoch 45 of 500, Train Loss: 0.007835\n",
      "best loss:  0.007834970502565477\n",
      "Epoch 46 of 500, Train Loss: 0.007782\n",
      "best loss:  0.007781658790551682\n",
      "Epoch 47 of 500, Train Loss: 0.007754\n",
      "best loss:  0.007754338344233291\n",
      "Epoch 48 of 500, Train Loss: 0.007725\n",
      "best loss:  0.007725039109079593\n",
      "Epoch 49 of 500, Train Loss: 0.007685\n",
      "best loss:  0.007684780507891749\n",
      "Epoch 50 of 500, Train Loss: 0.007665\n",
      "best loss:  0.007664534479335434\n",
      "Epoch 51 of 500, Train Loss: 0.007635\n",
      "best loss:  0.007634730469263989\n",
      "Epoch 52 of 500, Train Loss: 0.007615\n",
      "best loss:  0.007614997012744651\n",
      "Epoch 53 of 500, Train Loss: 0.007586\n",
      "best loss:  0.007586281986112071\n",
      "Epoch 54 of 500, Train Loss: 0.007560\n",
      "best loss:  0.007560164472963589\n",
      "Epoch 55 of 500, Train Loss: 0.007532\n",
      "best loss:  0.0075323604007365265\n",
      "Epoch 56 of 500, Train Loss: 0.007518\n",
      "best loss:  0.007518348325578164\n",
      "Epoch 57 of 500, Train Loss: 0.007490\n",
      "best loss:  0.007490131104059965\n",
      "Epoch 58 of 500, Train Loss: 0.007475\n",
      "best loss:  0.007475332820732977\n",
      "Epoch 59 of 500, Train Loss: 0.007455\n",
      "best loss:  0.007455330338331784\n",
      "Epoch 60 of 500, Train Loss: 0.007444\n",
      "best loss:  0.0074439034533286255\n",
      "Epoch 61 of 500, Train Loss: 0.007414\n",
      "best loss:  0.007414418059245993\n",
      "Epoch 62 of 500, Train Loss: 0.007395\n",
      "best loss:  0.00739506043650773\n",
      "Epoch 63 of 500, Train Loss: 0.007369\n",
      "best loss:  0.00736860088476561\n",
      "Epoch 64 of 500, Train Loss: 0.007349\n",
      "best loss:  0.007349314109887928\n",
      "Epoch 65 of 500, Train Loss: 0.007335\n",
      "best loss:  0.007335328881997911\n",
      "Epoch 66 of 500, Train Loss: 0.007332\n",
      "best loss:  0.007332408571500302\n",
      "Epoch 67 of 500, Train Loss: 0.007318\n",
      "best loss:  0.007317734268347754\n",
      "Epoch 68 of 500, Train Loss: 0.007307\n",
      "best loss:  0.0073065930064190035\n",
      "Epoch 69 of 500, Train Loss: 0.007277\n",
      "best loss:  0.007276948353130364\n",
      "Epoch 70 of 500, Train Loss: 0.007264\n",
      "best loss:  0.007263867577759896\n",
      "Epoch 71 of 500, Train Loss: 0.007244\n",
      "best loss:  0.007243829896080654\n",
      "Epoch 72 of 500, Train Loss: 0.007238\n",
      "best loss:  0.007238059403585061\n",
      "Epoch 73 of 500, Train Loss: 0.007222\n",
      "best loss:  0.00722179205906794\n",
      "Epoch 74 of 500, Train Loss: 0.007222\n",
      "Epoch 75 of 500, Train Loss: 0.007213\n",
      "best loss:  0.007213491103462983\n",
      "Epoch 76 of 500, Train Loss: 0.007217\n",
      "Epoch 77 of 500, Train Loss: 0.007204\n",
      "best loss:  0.007203606563546269\n",
      "Epoch 78 of 500, Train Loss: 0.007212\n",
      "Epoch 79 of 500, Train Loss: 0.007204\n",
      "Epoch 80 of 500, Train Loss: 0.007220\n",
      "Epoch 81 of 500, Train Loss: 0.007215\n",
      "Epoch 82 of 500, Train Loss: 0.007237\n",
      "Epoch 83 of 500, Train Loss: 0.007231\n",
      "Epoch 84 of 500, Train Loss: 0.007250\n",
      "Epoch 85 of 500, Train Loss: 0.007247\n",
      "Epoch 86 of 500, Train Loss: 0.007251\n",
      "Epoch 87 of 500, Train Loss: 0.007246\n",
      "Epoch 88 of 500, Train Loss: 0.007244\n",
      "Epoch 89 of 500, Train Loss: 0.007237\n",
      "Epoch 90 of 500, Train Loss: 0.007237\n",
      "Epoch 91 of 500, Train Loss: 0.007210\n",
      "Epoch 92 of 500, Train Loss: 0.007233\n",
      "Epoch 93 of 500, Train Loss: 0.007217\n",
      "Epoch 94 of 500, Train Loss: 0.007274\n",
      "Epoch 95 of 500, Train Loss: 0.007272\n",
      "Epoch 96 of 500, Train Loss: 0.007360\n",
      "Epoch 97 of 500, Train Loss: 0.007377\n",
      "Epoch 98 of 500, Train Loss: 0.007450\n",
      "Epoch 99 of 500, Train Loss: 0.007505\n",
      "Epoch 100 of 500, Train Loss: 0.007516\n",
      "Epoch 101 of 500, Train Loss: 0.007677\n",
      "Epoch 102 of 500, Train Loss: 0.007674\n",
      "Epoch 103 of 500, Train Loss: 0.007923\n",
      "Epoch 104 of 500, Train Loss: 0.007969\n",
      "Epoch 105 of 500, Train Loss: 0.008157\n",
      "Epoch 106 of 500, Train Loss: 0.008208\n",
      "Epoch 107 of 500, Train Loss: 0.008086\n",
      "Epoch 108 of 500, Train Loss: 0.007995\n",
      "Epoch 109 of 500, Train Loss: 0.007816\n",
      "Epoch 110 of 500, Train Loss: 0.007782\n",
      "Epoch 111 of 500, Train Loss: 0.007676\n",
      "Epoch 112 of 500, Train Loss: 0.007653\n",
      "Epoch 113 of 500, Train Loss: 0.007624\n",
      "Epoch 114 of 500, Train Loss: 0.007622\n",
      "Epoch 115 of 500, Train Loss: 0.007615\n",
      "Epoch 116 of 500, Train Loss: 0.007614\n",
      "Epoch 117 of 500, Train Loss: 0.007666\n",
      "Epoch 118 of 500, Train Loss: 0.007704\n",
      "Epoch 119 of 500, Train Loss: 0.007747\n",
      "Epoch 120 of 500, Train Loss: 0.007715\n",
      "Epoch 121 of 500, Train Loss: 0.007731\n",
      "Epoch 122 of 500, Train Loss: 0.007633\n",
      "Epoch 123 of 500, Train Loss: 0.007638\n",
      "Epoch 124 of 500, Train Loss: 0.007569\n",
      "Epoch 125 of 500, Train Loss: 0.007637\n",
      "Epoch 126 of 500, Train Loss: 0.007605\n",
      "Epoch 127 of 500, Train Loss: 0.007714\n",
      "Epoch 128 of 500, Train Loss: 0.007687\n",
      "Epoch 129 of 500, Train Loss: 0.007796\n",
      "Epoch 130 of 500, Train Loss: 0.007747\n",
      "Epoch 131 of 500, Train Loss: 0.007816\n",
      "Epoch 132 of 500, Train Loss: 0.007712\n",
      "Epoch 133 of 500, Train Loss: 0.007693\n",
      "Epoch 134 of 500, Train Loss: 0.007599\n",
      "Epoch 135 of 500, Train Loss: 0.007613\n",
      "Epoch 136 of 500, Train Loss: 0.007558\n",
      "Epoch 137 of 500, Train Loss: 0.007593\n",
      "Epoch 138 of 500, Train Loss: 0.007549\n",
      "Epoch 139 of 500, Train Loss: 0.007593\n",
      "Epoch 140 of 500, Train Loss: 0.007557\n",
      "Epoch 141 of 500, Train Loss: 0.007597\n",
      "Epoch 142 of 500, Train Loss: 0.007549\n",
      "Epoch 143 of 500, Train Loss: 0.007586\n",
      "Epoch 144 of 500, Train Loss: 0.007529\n",
      "Epoch 145 of 500, Train Loss: 0.007554\n",
      "Epoch 146 of 500, Train Loss: 0.007494\n",
      "Epoch 147 of 500, Train Loss: 0.007515\n",
      "Epoch 148 of 500, Train Loss: 0.007467\n",
      "Epoch 149 of 500, Train Loss: 0.007493\n",
      "Epoch 150 of 500, Train Loss: 0.007457\n",
      "Epoch 151 of 500, Train Loss: 0.007493\n",
      "Epoch 152 of 500, Train Loss: 0.007466\n",
      "Epoch 153 of 500, Train Loss: 0.007509\n",
      "Epoch 154 of 500, Train Loss: 0.007484\n",
      "Epoch 155 of 500, Train Loss: 0.007529\n",
      "Epoch 156 of 500, Train Loss: 0.007482\n",
      "Epoch 157 of 500, Train Loss: 0.007524\n",
      "Epoch 158 of 500, Train Loss: 0.007454\n",
      "Epoch 159 of 500, Train Loss: 0.007487\n",
      "Epoch 160 of 500, Train Loss: 0.007435\n",
      "Epoch 161 of 500, Train Loss: 0.007460\n",
      "Epoch 162 of 500, Train Loss: 0.007429\n",
      "Epoch 163 of 500, Train Loss: 0.007451\n",
      "Epoch 164 of 500, Train Loss: 0.007429\n",
      "Epoch 165 of 500, Train Loss: 0.007451\n",
      "Epoch 166 of 500, Train Loss: 0.007422\n",
      "Epoch 167 of 500, Train Loss: 0.007436\n",
      "Epoch 168 of 500, Train Loss: 0.007396\n",
      "Epoch 169 of 500, Train Loss: 0.007401\n",
      "Epoch 170 of 500, Train Loss: 0.007366\n",
      "Epoch 171 of 500, Train Loss: 0.007373\n",
      "Epoch 172 of 500, Train Loss: 0.007350\n",
      "Epoch 173 of 500, Train Loss: 0.007370\n",
      "Epoch 174 of 500, Train Loss: 0.007348\n",
      "Epoch 175 of 500, Train Loss: 0.007374\n",
      "Epoch 176 of 500, Train Loss: 0.007347\n",
      "Epoch 177 of 500, Train Loss: 0.007369\n",
      "Epoch 178 of 500, Train Loss: 0.007348\n",
      "Epoch 179 of 500, Train Loss: 0.007367\n",
      "Epoch 180 of 500, Train Loss: 0.007354\n",
      "Epoch 181 of 500, Train Loss: 0.007380\n",
      "Epoch 182 of 500, Train Loss: 0.007377\n",
      "Epoch 183 of 500, Train Loss: 0.007405\n",
      "Epoch 184 of 500, Train Loss: 0.007403\n",
      "Epoch 185 of 500, Train Loss: 0.007415\n",
      "Epoch 186 of 500, Train Loss: 0.007398\n",
      "Epoch 187 of 500, Train Loss: 0.007399\n",
      "Epoch 188 of 500, Train Loss: 0.007382\n",
      "Epoch 189 of 500, Train Loss: 0.007390\n",
      "Epoch 190 of 500, Train Loss: 0.007379\n",
      "Epoch 191 of 500, Train Loss: 0.007392\n",
      "Epoch 192 of 500, Train Loss: 0.007380\n",
      "Epoch 193 of 500, Train Loss: 0.007382\n",
      "Epoch 194 of 500, Train Loss: 0.007371\n",
      "Epoch 195 of 500, Train Loss: 0.007361\n",
      "Epoch 196 of 500, Train Loss: 0.007361\n",
      "Epoch 197 of 500, Train Loss: 0.007354\n",
      "Epoch 198 of 500, Train Loss: 0.007367\n",
      "Epoch 199 of 500, Train Loss: 0.007361\n",
      "Epoch 200 of 500, Train Loss: 0.007369\n",
      "Epoch 201 of 500, Train Loss: 0.007364\n",
      "Epoch 202 of 500, Train Loss: 0.007373\n",
      "Epoch 203 of 500, Train Loss: 0.007378\n",
      "Epoch 204 of 500, Train Loss: 0.007403\n",
      "Epoch 205 of 500, Train Loss: 0.007408\n",
      "Epoch 206 of 500, Train Loss: 0.007431\n",
      "Epoch 207 of 500, Train Loss: 0.007420\n",
      "Epoch 208 of 500, Train Loss: 0.007431\n",
      "Epoch 209 of 500, Train Loss: 0.007428\n",
      "Epoch 210 of 500, Train Loss: 0.007442\n",
      "Epoch 211 of 500, Train Loss: 0.007444\n",
      "Epoch 212 of 500, Train Loss: 0.007440\n",
      "Epoch 213 of 500, Train Loss: 0.007418\n",
      "Epoch 214 of 500, Train Loss: 0.007385\n",
      "Epoch 215 of 500, Train Loss: 0.007382\n",
      "Epoch 216 of 500, Train Loss: 0.007385\n",
      "Epoch 217 of 500, Train Loss: 0.007406\n",
      "Epoch 218 of 500, Train Loss: 0.007408\n",
      "Epoch 219 of 500, Train Loss: 0.007394\n",
      "Epoch 220 of 500, Train Loss: 0.007364\n",
      "Epoch 221 of 500, Train Loss: 0.007367\n",
      "Epoch 222 of 500, Train Loss: 0.007358\n",
      "Epoch 223 of 500, Train Loss: 0.007361\n",
      "Epoch 224 of 500, Train Loss: 0.007348\n",
      "Epoch 225 of 500, Train Loss: 0.007359\n",
      "Epoch 226 of 500, Train Loss: 0.007353\n",
      "Epoch 227 of 500, Train Loss: 0.007353\n",
      "Epoch 228 of 500, Train Loss: 0.007319\n",
      "Epoch 229 of 500, Train Loss: 0.007345\n",
      "Epoch 230 of 500, Train Loss: 0.007334\n",
      "Epoch 231 of 500, Train Loss: 0.007355\n",
      "Epoch 232 of 500, Train Loss: 0.007337\n",
      "Epoch 233 of 500, Train Loss: 0.007347\n",
      "Epoch 234 of 500, Train Loss: 0.007334\n",
      "Epoch 235 of 500, Train Loss: 0.007369\n",
      "Epoch 236 of 500, Train Loss: 0.007391\n",
      "Epoch 237 of 500, Train Loss: 0.007407\n",
      "Epoch 238 of 500, Train Loss: 0.007427\n",
      "Epoch 239 of 500, Train Loss: 0.007425\n",
      "Epoch 240 of 500, Train Loss: 0.007423\n",
      "Epoch 241 of 500, Train Loss: 0.007434\n",
      "Epoch 242 of 500, Train Loss: 0.007446\n",
      "Epoch 243 of 500, Train Loss: 0.007438\n",
      "Epoch 244 of 500, Train Loss: 0.007445\n",
      "Epoch 245 of 500, Train Loss: 0.007454\n",
      "Epoch 246 of 500, Train Loss: 0.007460\n",
      "Epoch 247 of 500, Train Loss: 0.007502\n",
      "Epoch 248 of 500, Train Loss: 0.007474\n",
      "Epoch 249 of 500, Train Loss: 0.007485\n",
      "Epoch 250 of 500, Train Loss: 0.007514\n",
      "Epoch 251 of 500, Train Loss: 0.007574\n",
      "Epoch 252 of 500, Train Loss: 0.007557\n",
      "Epoch 253 of 500, Train Loss: 0.007595\n",
      "Epoch 254 of 500, Train Loss: 0.007609\n",
      "Epoch 255 of 500, Train Loss: 0.007582\n",
      "Epoch 256 of 500, Train Loss: 0.007609\n",
      "Epoch 257 of 500, Train Loss: 0.007613\n",
      "Epoch 258 of 500, Train Loss: 0.007628\n",
      "Epoch 259 of 500, Train Loss: 0.007611\n",
      "Epoch 260 of 500, Train Loss: 0.007575\n",
      "Epoch 261 of 500, Train Loss: 0.007480\n",
      "Epoch 262 of 500, Train Loss: 0.007499\n",
      "Epoch 263 of 500, Train Loss: 0.007452\n",
      "Epoch 264 of 500, Train Loss: 0.007482\n",
      "Epoch 265 of 500, Train Loss: 0.007462\n",
      "Epoch 266 of 500, Train Loss: 0.007490\n",
      "Epoch 267 of 500, Train Loss: 0.007466\n",
      "Epoch 268 of 500, Train Loss: 0.007490\n",
      "Epoch 269 of 500, Train Loss: 0.007463\n",
      "Epoch 270 of 500, Train Loss: 0.007445\n",
      "Epoch 271 of 500, Train Loss: 0.007430\n",
      "Epoch 272 of 500, Train Loss: 0.007437\n",
      "Epoch 273 of 500, Train Loss: 0.007436\n",
      "Epoch 274 of 500, Train Loss: 0.007453\n",
      "Epoch 275 of 500, Train Loss: 0.007445\n",
      "Epoch 276 of 500, Train Loss: 0.007436\n",
      "Epoch 277 of 500, Train Loss: 0.007425\n",
      "Epoch 278 of 500, Train Loss: 0.007405\n",
      "Epoch 279 of 500, Train Loss: 0.007395\n",
      "Epoch 280 of 500, Train Loss: 0.007377\n",
      "Epoch 281 of 500, Train Loss: 0.007371\n",
      "Epoch 282 of 500, Train Loss: 0.007394\n",
      "Epoch 283 of 500, Train Loss: 0.007424\n",
      "Epoch 284 of 500, Train Loss: 0.007479\n",
      "Epoch 285 of 500, Train Loss: 0.007499\n",
      "Epoch 286 of 500, Train Loss: 0.007554\n",
      "Epoch 287 of 500, Train Loss: 0.007526\n",
      "Epoch 288 of 500, Train Loss: 0.007531\n",
      "Epoch 289 of 500, Train Loss: 0.007542\n",
      "Epoch 290 of 500, Train Loss: 0.007564\n",
      "Epoch 291 of 500, Train Loss: 0.007547\n",
      "Epoch 292 of 500, Train Loss: 0.007547\n",
      "Epoch 293 of 500, Train Loss: 0.007550\n",
      "Epoch 294 of 500, Train Loss: 0.007551\n",
      "Epoch 295 of 500, Train Loss: 0.007486\n",
      "Epoch 296 of 500, Train Loss: 0.007467\n",
      "Epoch 297 of 500, Train Loss: 0.007446\n",
      "Epoch 298 of 500, Train Loss: 0.007463\n",
      "Epoch 299 of 500, Train Loss: 0.007463\n",
      "Epoch 300 of 500, Train Loss: 0.007462\n",
      "Epoch 301 of 500, Train Loss: 0.007456\n",
      "Epoch 302 of 500, Train Loss: 0.007467\n",
      "Epoch 303 of 500, Train Loss: 0.007439\n",
      "Epoch 304 of 500, Train Loss: 0.007460\n",
      "Epoch 305 of 500, Train Loss: 0.007445\n",
      "Epoch 306 of 500, Train Loss: 0.007441\n",
      "Epoch 307 of 500, Train Loss: 0.007439\n",
      "Epoch 308 of 500, Train Loss: 0.007448\n",
      "Epoch 309 of 500, Train Loss: 0.007457\n",
      "Epoch 310 of 500, Train Loss: 0.007455\n",
      "Epoch 311 of 500, Train Loss: 0.007484\n",
      "Epoch 312 of 500, Train Loss: 0.007497\n",
      "Epoch 313 of 500, Train Loss: 0.007497\n",
      "Epoch 314 of 500, Train Loss: 0.007501\n",
      "Epoch 315 of 500, Train Loss: 0.007475\n",
      "Epoch 316 of 500, Train Loss: 0.007460\n",
      "Epoch 317 of 500, Train Loss: 0.007445\n",
      "Epoch 318 of 500, Train Loss: 0.007445\n",
      "Epoch 319 of 500, Train Loss: 0.007437\n",
      "Epoch 320 of 500, Train Loss: 0.007453\n",
      "Epoch 321 of 500, Train Loss: 0.007442\n",
      "Epoch 322 of 500, Train Loss: 0.007452\n",
      "Epoch 323 of 500, Train Loss: 0.007449\n",
      "Epoch 324 of 500, Train Loss: 0.007469\n",
      "Epoch 325 of 500, Train Loss: 0.007488\n",
      "Epoch 326 of 500, Train Loss: 0.007499\n",
      "Epoch 327 of 500, Train Loss: 0.007469\n",
      "Epoch 328 of 500, Train Loss: 0.007441\n",
      "Epoch 329 of 500, Train Loss: 0.007425\n",
      "Epoch 330 of 500, Train Loss: 0.007418\n",
      "Epoch 331 of 500, Train Loss: 0.007413\n",
      "Epoch 332 of 500, Train Loss: 0.007420\n",
      "Epoch 333 of 500, Train Loss: 0.007411\n",
      "Epoch 334 of 500, Train Loss: 0.007420\n",
      "Epoch 335 of 500, Train Loss: 0.007428\n",
      "Epoch 336 of 500, Train Loss: 0.007472\n",
      "Epoch 337 of 500, Train Loss: 0.007471\n",
      "Epoch 338 of 500, Train Loss: 0.007472\n",
      "Epoch 339 of 500, Train Loss: 0.007469\n",
      "Epoch 340 of 500, Train Loss: 0.007457\n",
      "Epoch 341 of 500, Train Loss: 0.007432\n",
      "Epoch 342 of 500, Train Loss: 0.007455\n",
      "Epoch 343 of 500, Train Loss: 0.007421\n",
      "Epoch 344 of 500, Train Loss: 0.007424\n",
      "Epoch 345 of 500, Train Loss: 0.007409\n",
      "Epoch 346 of 500, Train Loss: 0.007430\n",
      "Epoch 347 of 500, Train Loss: 0.007423\n",
      "Epoch 348 of 500, Train Loss: 0.007427\n",
      "Epoch 349 of 500, Train Loss: 0.007398\n",
      "Epoch 350 of 500, Train Loss: 0.007411\n",
      "Epoch 351 of 500, Train Loss: 0.007393\n",
      "Epoch 352 of 500, Train Loss: 0.007429\n",
      "Epoch 353 of 500, Train Loss: 0.007393\n",
      "Epoch 354 of 500, Train Loss: 0.007407\n",
      "Epoch 355 of 500, Train Loss: 0.007378\n",
      "Epoch 356 of 500, Train Loss: 0.007390\n",
      "Epoch 357 of 500, Train Loss: 0.007366\n",
      "Epoch 358 of 500, Train Loss: 0.007379\n",
      "Epoch 359 of 500, Train Loss: 0.007367\n",
      "Epoch 360 of 500, Train Loss: 0.007368\n",
      "Epoch 361 of 500, Train Loss: 0.007380\n",
      "Epoch 362 of 500, Train Loss: 0.007402\n",
      "Epoch 363 of 500, Train Loss: 0.007398\n",
      "Epoch 364 of 500, Train Loss: 0.007417\n",
      "Epoch 365 of 500, Train Loss: 0.007402\n",
      "Epoch 366 of 500, Train Loss: 0.007410\n",
      "Epoch 367 of 500, Train Loss: 0.007406\n",
      "Epoch 368 of 500, Train Loss: 0.007409\n",
      "Epoch 369 of 500, Train Loss: 0.007397\n",
      "Epoch 370 of 500, Train Loss: 0.007416\n",
      "Epoch 371 of 500, Train Loss: 0.007393\n",
      "Epoch 372 of 500, Train Loss: 0.007408\n",
      "Epoch 373 of 500, Train Loss: 0.007416\n",
      "Epoch 374 of 500, Train Loss: 0.007427\n",
      "Epoch 375 of 500, Train Loss: 0.007398\n",
      "Epoch 376 of 500, Train Loss: 0.007428\n",
      "Epoch 377 of 500, Train Loss: 0.007399\n",
      "Epoch 378 of 500, Train Loss: 0.007394\n",
      "Epoch 379 of 500, Train Loss: 0.007394\n",
      "Epoch 380 of 500, Train Loss: 0.007427\n",
      "Epoch 381 of 500, Train Loss: 0.007413\n",
      "Epoch 382 of 500, Train Loss: 0.007408\n",
      "Epoch 383 of 500, Train Loss: 0.007401\n",
      "Epoch 384 of 500, Train Loss: 0.007410\n",
      "Epoch 385 of 500, Train Loss: 0.007392\n",
      "Epoch 386 of 500, Train Loss: 0.007396\n",
      "Epoch 387 of 500, Train Loss: 0.007375\n",
      "Epoch 388 of 500, Train Loss: 0.007385\n",
      "Epoch 389 of 500, Train Loss: 0.007364\n",
      "Epoch 390 of 500, Train Loss: 0.007386\n",
      "Epoch 391 of 500, Train Loss: 0.007407\n",
      "Epoch 392 of 500, Train Loss: 0.007414\n",
      "Epoch 393 of 500, Train Loss: 0.007418\n",
      "Epoch 394 of 500, Train Loss: 0.007405\n",
      "Epoch 395 of 500, Train Loss: 0.007406\n",
      "Epoch 396 of 500, Train Loss: 0.007418\n",
      "Epoch 397 of 500, Train Loss: 0.007427\n",
      "Epoch 398 of 500, Train Loss: 0.007444\n",
      "Epoch 399 of 500, Train Loss: 0.007451\n",
      "Epoch 400 of 500, Train Loss: 0.007406\n",
      "Epoch 401 of 500, Train Loss: 0.007384\n",
      "Epoch 402 of 500, Train Loss: 0.007384\n",
      "Epoch 403 of 500, Train Loss: 0.007381\n",
      "Epoch 404 of 500, Train Loss: 0.007400\n",
      "Epoch 405 of 500, Train Loss: 0.007389\n",
      "Epoch 406 of 500, Train Loss: 0.007351\n",
      "Epoch 407 of 500, Train Loss: 0.007340\n",
      "Epoch 408 of 500, Train Loss: 0.007350\n",
      "Epoch 409 of 500, Train Loss: 0.007368\n",
      "Epoch 410 of 500, Train Loss: 0.007373\n",
      "Epoch 411 of 500, Train Loss: 0.007372\n",
      "Epoch 412 of 500, Train Loss: 0.007369\n",
      "Epoch 413 of 500, Train Loss: 0.007349\n",
      "Epoch 414 of 500, Train Loss: 0.007334\n",
      "Epoch 415 of 500, Train Loss: 0.007350\n",
      "Epoch 416 of 500, Train Loss: 0.007356\n",
      "Epoch 417 of 500, Train Loss: 0.007356\n",
      "Epoch 418 of 500, Train Loss: 0.007349\n",
      "Epoch 419 of 500, Train Loss: 0.007333\n",
      "Epoch 420 of 500, Train Loss: 0.007352\n",
      "Epoch 421 of 500, Train Loss: 0.007374\n",
      "Epoch 422 of 500, Train Loss: 0.007399\n",
      "Epoch 423 of 500, Train Loss: 0.007380\n",
      "Epoch 424 of 500, Train Loss: 0.007367\n",
      "Epoch 425 of 500, Train Loss: 0.007372\n",
      "Epoch 426 of 500, Train Loss: 0.007393\n",
      "Epoch 427 of 500, Train Loss: 0.007409\n",
      "Epoch 428 of 500, Train Loss: 0.007427\n",
      "Epoch 429 of 500, Train Loss: 0.007393\n",
      "Epoch 430 of 500, Train Loss: 0.007369\n",
      "Epoch 431 of 500, Train Loss: 0.007403\n",
      "Epoch 432 of 500, Train Loss: 0.007389\n",
      "Epoch 433 of 500, Train Loss: 0.007370\n",
      "Epoch 434 of 500, Train Loss: 0.007386\n",
      "Epoch 435 of 500, Train Loss: 0.007384\n",
      "Epoch 436 of 500, Train Loss: 0.007389\n",
      "Epoch 437 of 500, Train Loss: 0.007427\n",
      "Epoch 438 of 500, Train Loss: 0.007443\n",
      "Epoch 439 of 500, Train Loss: 0.007420\n",
      "Epoch 440 of 500, Train Loss: 0.007409\n",
      "Epoch 441 of 500, Train Loss: 0.007384\n",
      "Epoch 442 of 500, Train Loss: 0.007356\n",
      "Epoch 443 of 500, Train Loss: 0.007379\n",
      "Epoch 444 of 500, Train Loss: 0.007368\n",
      "Epoch 445 of 500, Train Loss: 0.007358\n",
      "Epoch 446 of 500, Train Loss: 0.007354\n",
      "Epoch 447 of 500, Train Loss: 0.007368\n",
      "Epoch 448 of 500, Train Loss: 0.007354\n",
      "Epoch 449 of 500, Train Loss: 0.007382\n",
      "Epoch 450 of 500, Train Loss: 0.007364\n",
      "Epoch 451 of 500, Train Loss: 0.007343\n",
      "Epoch 452 of 500, Train Loss: 0.007314\n",
      "Epoch 453 of 500, Train Loss: 0.007314\n",
      "Epoch 454 of 500, Train Loss: 0.007321\n",
      "Epoch 455 of 500, Train Loss: 0.007330\n",
      "Epoch 456 of 500, Train Loss: 0.007325\n",
      "Epoch 457 of 500, Train Loss: 0.007295\n",
      "Epoch 458 of 500, Train Loss: 0.007307\n",
      "Epoch 459 of 500, Train Loss: 0.007323\n",
      "Epoch 460 of 500, Train Loss: 0.007328\n",
      "Epoch 461 of 500, Train Loss: 0.007339\n",
      "Epoch 462 of 500, Train Loss: 0.007332\n",
      "Epoch 463 of 500, Train Loss: 0.007332\n",
      "Epoch 464 of 500, Train Loss: 0.007333\n",
      "Epoch 465 of 500, Train Loss: 0.007349\n",
      "Epoch 466 of 500, Train Loss: 0.007357\n",
      "Epoch 467 of 500, Train Loss: 0.007390\n",
      "Epoch 468 of 500, Train Loss: 0.007451\n",
      "Epoch 469 of 500, Train Loss: 0.007468\n",
      "Epoch 470 of 500, Train Loss: 0.007399\n",
      "Epoch 471 of 500, Train Loss: 0.007363\n",
      "Epoch 472 of 500, Train Loss: 0.007329\n",
      "Epoch 473 of 500, Train Loss: 0.007329\n",
      "Epoch 474 of 500, Train Loss: 0.007341\n",
      "Epoch 475 of 500, Train Loss: 0.007328\n",
      "Epoch 476 of 500, Train Loss: 0.007321\n",
      "Epoch 477 of 500, Train Loss: 0.007329\n",
      "Epoch 478 of 500, Train Loss: 0.007347\n",
      "Epoch 479 of 500, Train Loss: 0.007344\n",
      "Epoch 480 of 500, Train Loss: 0.007387\n",
      "Epoch 481 of 500, Train Loss: 0.007419\n",
      "Epoch 482 of 500, Train Loss: 0.007411\n",
      "Epoch 483 of 500, Train Loss: 0.007401\n",
      "Epoch 484 of 500, Train Loss: 0.007379\n",
      "Epoch 485 of 500, Train Loss: 0.007361\n",
      "Epoch 486 of 500, Train Loss: 0.007362\n",
      "Epoch 487 of 500, Train Loss: 0.007348\n",
      "Epoch 488 of 500, Train Loss: 0.007336\n",
      "Epoch 489 of 500, Train Loss: 0.007351\n",
      "Epoch 490 of 500, Train Loss: 0.007346\n",
      "Epoch 491 of 500, Train Loss: 0.007341\n",
      "Epoch 492 of 500, Train Loss: 0.007340\n",
      "Epoch 493 of 500, Train Loss: 0.007373\n",
      "Epoch 494 of 500, Train Loss: 0.007370\n",
      "Epoch 495 of 500, Train Loss: 0.007400\n",
      "Epoch 496 of 500, Train Loss: 0.007411\n",
      "Epoch 497 of 500, Train Loss: 0.007390\n",
      "Epoch 498 of 500, Train Loss: 0.007357\n",
      "Epoch 499 of 500, Train Loss: 0.007358\n",
      "Epoch 500 of 500, Train Loss: 0.007366\n",
      "latent train shape:  (16395, 120)\n",
      "M: 120, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 60\n",
      "Training the subspace: 0 / 120\n",
      "Training the subspace: 1 / 120\n",
      "Training the subspace: 2 / 120\n",
      "Training the subspace: 3 / 120\n",
      "Training the subspace: 4 / 120\n",
      "Training the subspace: 5 / 120\n",
      "Training the subspace: 6 / 120\n",
      "Training the subspace: 7 / 120\n",
      "Training the subspace: 8 / 120\n",
      "Training the subspace: 9 / 120\n",
      "Training the subspace: 10 / 120\n",
      "Training the subspace: 11 / 120\n",
      "Training the subspace: 12 / 120\n",
      "Training the subspace: 13 / 120\n",
      "Training the subspace: 14 / 120\n",
      "Training the subspace: 15 / 120\n",
      "Training the subspace: 16 / 120\n",
      "Training the subspace: 17 / 120\n",
      "Training the subspace: 18 / 120\n",
      "Training the subspace: 19 / 120\n",
      "Training the subspace: 20 / 120\n",
      "Training the subspace: 21 / 120\n",
      "Training the subspace: 22 / 120\n",
      "Training the subspace: 23 / 120\n",
      "Training the subspace: 24 / 120\n",
      "Training the subspace: 25 / 120\n",
      "Training the subspace: 26 / 120\n",
      "Training the subspace: 27 / 120\n",
      "Training the subspace: 28 / 120\n",
      "Training the subspace: 29 / 120\n",
      "Training the subspace: 30 / 120\n",
      "Training the subspace: 31 / 120\n",
      "Training the subspace: 32 / 120\n",
      "Training the subspace: 33 / 120\n",
      "Training the subspace: 34 / 120\n",
      "Training the subspace: 35 / 120\n",
      "Training the subspace: 36 / 120\n",
      "Training the subspace: 37 / 120\n",
      "Training the subspace: 38 / 120\n",
      "Training the subspace: 39 / 120\n",
      "Training the subspace: 40 / 120\n",
      "Training the subspace: 41 / 120\n",
      "Training the subspace: 42 / 120\n",
      "Training the subspace: 43 / 120\n",
      "Training the subspace: 44 / 120\n",
      "Training the subspace: 45 / 120\n",
      "Training the subspace: 46 / 120\n",
      "Training the subspace: 47 / 120\n",
      "Training the subspace: 48 / 120\n",
      "Training the subspace: 49 / 120\n",
      "Training the subspace: 50 / 120\n",
      "Training the subspace: 51 / 120\n",
      "Training the subspace: 52 / 120\n",
      "Training the subspace: 53 / 120\n",
      "Training the subspace: 54 / 120\n",
      "Training the subspace: 55 / 120\n",
      "Training the subspace: 56 / 120\n",
      "Training the subspace: 57 / 120\n",
      "Training the subspace: 58 / 120\n",
      "Training the subspace: 59 / 120\n",
      "Training the subspace: 60 / 120\n",
      "Training the subspace: 61 / 120\n",
      "Training the subspace: 62 / 120\n",
      "Training the subspace: 63 / 120\n",
      "Training the subspace: 64 / 120\n",
      "Training the subspace: 65 / 120\n",
      "Training the subspace: 66 / 120\n",
      "Training the subspace: 67 / 120\n",
      "Training the subspace: 68 / 120\n",
      "Training the subspace: 69 / 120\n",
      "Training the subspace: 70 / 120\n",
      "Training the subspace: 71 / 120\n",
      "Training the subspace: 72 / 120\n",
      "Training the subspace: 73 / 120\n",
      "Training the subspace: 74 / 120\n",
      "Training the subspace: 75 / 120\n",
      "Training the subspace: 76 / 120\n",
      "Training the subspace: 77 / 120\n",
      "Training the subspace: 78 / 120\n",
      "Training the subspace: 79 / 120\n",
      "Training the subspace: 80 / 120\n",
      "Training the subspace: 81 / 120\n",
      "Training the subspace: 82 / 120\n",
      "Training the subspace: 83 / 120\n",
      "Training the subspace: 84 / 120\n",
      "Training the subspace: 85 / 120\n",
      "Training the subspace: 86 / 120\n",
      "Training the subspace: 87 / 120\n",
      "Training the subspace: 88 / 120\n",
      "Training the subspace: 89 / 120\n",
      "Training the subspace: 90 / 120\n",
      "Training the subspace: 91 / 120\n",
      "Training the subspace: 92 / 120\n",
      "Training the subspace: 93 / 120\n",
      "Training the subspace: 94 / 120\n",
      "Training the subspace: 95 / 120\n",
      "Training the subspace: 96 / 120\n",
      "Training the subspace: 97 / 120\n",
      "Training the subspace: 98 / 120\n",
      "Training the subspace: 99 / 120\n",
      "Training the subspace: 100 / 120\n",
      "Training the subspace: 101 / 120\n",
      "Training the subspace: 102 / 120\n",
      "Training the subspace: 103 / 120\n",
      "Training the subspace: 104 / 120\n",
      "Training the subspace: 105 / 120\n",
      "Training the subspace: 106 / 120\n",
      "Training the subspace: 107 / 120\n",
      "Training the subspace: 108 / 120\n",
      "Training the subspace: 109 / 120\n",
      "Training the subspace: 110 / 120\n",
      "Training the subspace: 111 / 120\n",
      "Training the subspace: 112 / 120\n",
      "Training the subspace: 113 / 120\n",
      "Training the subspace: 114 / 120\n",
      "Training the subspace: 115 / 120\n",
      "Training the subspace: 116 / 120\n",
      "Training the subspace: 117 / 120\n",
      "Training the subspace: 118 / 120\n",
      "Training the subspace: 119 / 120\n",
      "Encoding the subspace: 0 / 120\n",
      "Encoding the subspace: 1 / 120\n",
      "Encoding the subspace: 2 / 120\n",
      "Encoding the subspace: 3 / 120\n",
      "Encoding the subspace: 4 / 120\n",
      "Encoding the subspace: 5 / 120\n",
      "Encoding the subspace: 6 / 120\n",
      "Encoding the subspace: 7 / 120\n",
      "Encoding the subspace: 8 / 120\n",
      "Encoding the subspace: 9 / 120\n",
      "Encoding the subspace: 10 / 120\n",
      "Encoding the subspace: 11 / 120\n",
      "Encoding the subspace: 12 / 120\n",
      "Encoding the subspace: 13 / 120\n",
      "Encoding the subspace: 14 / 120\n",
      "Encoding the subspace: 15 / 120\n",
      "Encoding the subspace: 16 / 120\n",
      "Encoding the subspace: 17 / 120\n",
      "Encoding the subspace: 18 / 120\n",
      "Encoding the subspace: 19 / 120\n",
      "Encoding the subspace: 20 / 120\n",
      "Encoding the subspace: 21 / 120\n",
      "Encoding the subspace: 22 / 120\n",
      "Encoding the subspace: 23 / 120\n",
      "Encoding the subspace: 24 / 120\n",
      "Encoding the subspace: 25 / 120\n",
      "Encoding the subspace: 26 / 120\n",
      "Encoding the subspace: 27 / 120\n",
      "Encoding the subspace: 28 / 120\n",
      "Encoding the subspace: 29 / 120\n",
      "Encoding the subspace: 30 / 120\n",
      "Encoding the subspace: 31 / 120\n",
      "Encoding the subspace: 32 / 120\n",
      "Encoding the subspace: 33 / 120\n",
      "Encoding the subspace: 34 / 120\n",
      "Encoding the subspace: 35 / 120\n",
      "Encoding the subspace: 36 / 120\n",
      "Encoding the subspace: 37 / 120\n",
      "Encoding the subspace: 38 / 120\n",
      "Encoding the subspace: 39 / 120\n",
      "Encoding the subspace: 40 / 120\n",
      "Encoding the subspace: 41 / 120\n",
      "Encoding the subspace: 42 / 120\n",
      "Encoding the subspace: 43 / 120\n",
      "Encoding the subspace: 44 / 120\n",
      "Encoding the subspace: 45 / 120\n",
      "Encoding the subspace: 46 / 120\n",
      "Encoding the subspace: 47 / 120\n",
      "Encoding the subspace: 48 / 120\n",
      "Encoding the subspace: 49 / 120\n",
      "Encoding the subspace: 50 / 120\n",
      "Encoding the subspace: 51 / 120\n",
      "Encoding the subspace: 52 / 120\n",
      "Encoding the subspace: 53 / 120\n",
      "Encoding the subspace: 54 / 120\n",
      "Encoding the subspace: 55 / 120\n",
      "Encoding the subspace: 56 / 120\n",
      "Encoding the subspace: 57 / 120\n",
      "Encoding the subspace: 58 / 120\n",
      "Encoding the subspace: 59 / 120\n",
      "Encoding the subspace: 60 / 120\n",
      "Encoding the subspace: 61 / 120\n",
      "Encoding the subspace: 62 / 120\n",
      "Encoding the subspace: 63 / 120\n",
      "Encoding the subspace: 64 / 120\n",
      "Encoding the subspace: 65 / 120\n",
      "Encoding the subspace: 66 / 120\n",
      "Encoding the subspace: 67 / 120\n",
      "Encoding the subspace: 68 / 120\n",
      "Encoding the subspace: 69 / 120\n",
      "Encoding the subspace: 70 / 120\n",
      "Encoding the subspace: 71 / 120\n",
      "Encoding the subspace: 72 / 120\n",
      "Encoding the subspace: 73 / 120\n",
      "Encoding the subspace: 74 / 120\n",
      "Encoding the subspace: 75 / 120\n",
      "Encoding the subspace: 76 / 120\n",
      "Encoding the subspace: 77 / 120\n",
      "Encoding the subspace: 78 / 120\n",
      "Encoding the subspace: 79 / 120\n",
      "Encoding the subspace: 80 / 120\n",
      "Encoding the subspace: 81 / 120\n",
      "Encoding the subspace: 82 / 120\n",
      "Encoding the subspace: 83 / 120\n",
      "Encoding the subspace: 84 / 120\n",
      "Encoding the subspace: 85 / 120\n",
      "Encoding the subspace: 86 / 120\n",
      "Encoding the subspace: 87 / 120\n",
      "Encoding the subspace: 88 / 120\n",
      "Encoding the subspace: 89 / 120\n",
      "Encoding the subspace: 90 / 120\n",
      "Encoding the subspace: 91 / 120\n",
      "Encoding the subspace: 92 / 120\n",
      "Encoding the subspace: 93 / 120\n",
      "Encoding the subspace: 94 / 120\n",
      "Encoding the subspace: 95 / 120\n",
      "Encoding the subspace: 96 / 120\n",
      "Encoding the subspace: 97 / 120\n",
      "Encoding the subspace: 98 / 120\n",
      "Encoding the subspace: 99 / 120\n",
      "Encoding the subspace: 100 / 120\n",
      "Encoding the subspace: 101 / 120\n",
      "Encoding the subspace: 102 / 120\n",
      "Encoding the subspace: 103 / 120\n",
      "Encoding the subspace: 104 / 120\n",
      "Encoding the subspace: 105 / 120\n",
      "Encoding the subspace: 106 / 120\n",
      "Encoding the subspace: 107 / 120\n",
      "Encoding the subspace: 108 / 120\n",
      "Encoding the subspace: 109 / 120\n",
      "Encoding the subspace: 110 / 120\n",
      "Encoding the subspace: 111 / 120\n",
      "Encoding the subspace: 112 / 120\n",
      "Encoding the subspace: 113 / 120\n",
      "Encoding the subspace: 114 / 120\n",
      "Encoding the subspace: 115 / 120\n",
      "Encoding the subspace: 116 / 120\n",
      "Encoding the subspace: 117 / 120\n",
      "Encoding the subspace: 118 / 120\n",
      "Encoding the subspace: 119 / 120\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=150, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.244043\n",
      "best loss:  0.24404299291673787\n",
      "Epoch 2 of 500, Train Loss: 0.045798\n",
      "best loss:  0.045797832302528964\n",
      "Epoch 3 of 500, Train Loss: 0.034616\n",
      "best loss:  0.03461649524889049\n",
      "Epoch 4 of 500, Train Loss: 0.027630\n",
      "best loss:  0.02762952234547282\n",
      "Epoch 5 of 500, Train Loss: 0.023829\n",
      "best loss:  0.023828934657012017\n",
      "Epoch 6 of 500, Train Loss: 0.021216\n",
      "best loss:  0.021215719727225343\n",
      "Epoch 7 of 500, Train Loss: 0.019250\n",
      "best loss:  0.019249798174561253\n",
      "Epoch 8 of 500, Train Loss: 0.017789\n",
      "best loss:  0.017788520518486395\n",
      "Epoch 9 of 500, Train Loss: 0.016575\n",
      "best loss:  0.016575321571156022\n",
      "Epoch 10 of 500, Train Loss: 0.015504\n",
      "best loss:  0.015504398743501078\n",
      "Epoch 11 of 500, Train Loss: 0.014561\n",
      "best loss:  0.014561334075523511\n",
      "Epoch 12 of 500, Train Loss: 0.013764\n",
      "best loss:  0.013764164407598577\n",
      "Epoch 13 of 500, Train Loss: 0.013058\n",
      "best loss:  0.013058392347248142\n",
      "Epoch 14 of 500, Train Loss: 0.012422\n",
      "best loss:  0.012421789481403855\n",
      "Epoch 15 of 500, Train Loss: 0.011862\n",
      "best loss:  0.011862366612776825\n",
      "Epoch 16 of 500, Train Loss: 0.011379\n",
      "best loss:  0.011379370339532463\n",
      "Epoch 17 of 500, Train Loss: 0.010956\n",
      "best loss:  0.010955837362332183\n",
      "Epoch 18 of 500, Train Loss: 0.010538\n",
      "best loss:  0.01053768118808241\n",
      "Epoch 19 of 500, Train Loss: 0.010210\n",
      "best loss:  0.010210083173984993\n",
      "Epoch 20 of 500, Train Loss: 0.009850\n",
      "best loss:  0.009850237392432367\n",
      "Epoch 21 of 500, Train Loss: 0.009543\n",
      "best loss:  0.009543043486264087\n",
      "Epoch 22 of 500, Train Loss: 0.009253\n",
      "best loss:  0.009252647486201292\n",
      "Epoch 23 of 500, Train Loss: 0.009024\n",
      "best loss:  0.009023745089310019\n",
      "Epoch 24 of 500, Train Loss: 0.008794\n",
      "best loss:  0.00879414661768499\n",
      "Epoch 25 of 500, Train Loss: 0.008585\n",
      "best loss:  0.008585019039383605\n",
      "Epoch 26 of 500, Train Loss: 0.008381\n",
      "best loss:  0.008380814876552678\n",
      "Epoch 27 of 500, Train Loss: 0.008199\n",
      "best loss:  0.00819915906068609\n",
      "Epoch 28 of 500, Train Loss: 0.008061\n",
      "best loss:  0.008061301965327833\n",
      "Epoch 29 of 500, Train Loss: 0.007923\n",
      "best loss:  0.007923245920358377\n",
      "Epoch 30 of 500, Train Loss: 0.007797\n",
      "best loss:  0.007797100173775107\n",
      "Epoch 31 of 500, Train Loss: 0.007653\n",
      "best loss:  0.007652885910794103\n",
      "Epoch 32 of 500, Train Loss: 0.007568\n",
      "best loss:  0.0075676565998425175\n",
      "Epoch 33 of 500, Train Loss: 0.007451\n",
      "best loss:  0.0074513446028180086\n",
      "Epoch 34 of 500, Train Loss: 0.007385\n",
      "best loss:  0.007385256945682588\n",
      "Epoch 35 of 500, Train Loss: 0.007300\n",
      "best loss:  0.007299895907184578\n",
      "Epoch 36 of 500, Train Loss: 0.007221\n",
      "best loss:  0.0072211447408081705\n",
      "Epoch 37 of 500, Train Loss: 0.007155\n",
      "best loss:  0.007154936933584959\n",
      "Epoch 38 of 500, Train Loss: 0.007117\n",
      "best loss:  0.007116743660795452\n",
      "Epoch 39 of 500, Train Loss: 0.007046\n",
      "best loss:  0.00704557805722796\n",
      "Epoch 40 of 500, Train Loss: 0.007023\n",
      "best loss:  0.007023148337702173\n",
      "Epoch 41 of 500, Train Loss: 0.006956\n",
      "best loss:  0.0069556464807738645\n",
      "Epoch 42 of 500, Train Loss: 0.006895\n",
      "best loss:  0.006895025425893796\n",
      "Epoch 43 of 500, Train Loss: 0.006849\n",
      "best loss:  0.0068486502410775385\n",
      "Epoch 44 of 500, Train Loss: 0.006808\n",
      "best loss:  0.006808202699401647\n",
      "Epoch 45 of 500, Train Loss: 0.006753\n",
      "best loss:  0.006752655372703123\n",
      "Epoch 46 of 500, Train Loss: 0.006731\n",
      "best loss:  0.006731299700710901\n",
      "Epoch 47 of 500, Train Loss: 0.006688\n",
      "best loss:  0.006687870229862436\n",
      "Epoch 48 of 500, Train Loss: 0.006677\n",
      "best loss:  0.006677369227612124\n",
      "Epoch 49 of 500, Train Loss: 0.006629\n",
      "best loss:  0.006629453567122904\n",
      "Epoch 50 of 500, Train Loss: 0.006641\n",
      "Epoch 51 of 500, Train Loss: 0.006612\n",
      "best loss:  0.006611724922211494\n",
      "Epoch 52 of 500, Train Loss: 0.006615\n",
      "Epoch 53 of 500, Train Loss: 0.006601\n",
      "best loss:  0.006601027061313556\n",
      "Epoch 54 of 500, Train Loss: 0.006604\n",
      "Epoch 55 of 500, Train Loss: 0.006587\n",
      "best loss:  0.006587498495761348\n",
      "Epoch 56 of 500, Train Loss: 0.006582\n",
      "best loss:  0.006582338441783281\n",
      "Epoch 57 of 500, Train Loss: 0.006517\n",
      "best loss:  0.00651684533112029\n",
      "Epoch 58 of 500, Train Loss: 0.006528\n",
      "Epoch 59 of 500, Train Loss: 0.006496\n",
      "best loss:  0.006496306449782226\n",
      "Epoch 60 of 500, Train Loss: 0.006518\n",
      "Epoch 61 of 500, Train Loss: 0.006516\n",
      "Epoch 62 of 500, Train Loss: 0.006564\n",
      "Epoch 63 of 500, Train Loss: 0.006625\n",
      "Epoch 64 of 500, Train Loss: 0.006696\n",
      "Epoch 65 of 500, Train Loss: 0.006784\n",
      "Epoch 66 of 500, Train Loss: 0.006836\n",
      "Epoch 67 of 500, Train Loss: 0.006922\n",
      "Epoch 68 of 500, Train Loss: 0.006932\n",
      "Epoch 69 of 500, Train Loss: 0.006958\n",
      "Epoch 70 of 500, Train Loss: 0.006948\n",
      "Epoch 71 of 500, Train Loss: 0.006895\n",
      "Epoch 72 of 500, Train Loss: 0.006883\n",
      "Epoch 73 of 500, Train Loss: 0.006764\n",
      "Epoch 74 of 500, Train Loss: 0.006698\n",
      "Epoch 75 of 500, Train Loss: 0.006584\n",
      "Epoch 76 of 500, Train Loss: 0.006527\n",
      "Epoch 77 of 500, Train Loss: 0.006433\n",
      "best loss:  0.006432751361610072\n",
      "Epoch 78 of 500, Train Loss: 0.006392\n",
      "best loss:  0.006391850271931843\n",
      "Epoch 79 of 500, Train Loss: 0.006328\n",
      "best loss:  0.006328045122633154\n",
      "Epoch 80 of 500, Train Loss: 0.006321\n",
      "best loss:  0.0063211048737550945\n",
      "Epoch 81 of 500, Train Loss: 0.006288\n",
      "best loss:  0.006287640454622108\n",
      "Epoch 82 of 500, Train Loss: 0.006314\n",
      "Epoch 83 of 500, Train Loss: 0.006307\n",
      "Epoch 84 of 500, Train Loss: 0.006344\n",
      "Epoch 85 of 500, Train Loss: 0.006358\n",
      "Epoch 86 of 500, Train Loss: 0.006403\n",
      "Epoch 87 of 500, Train Loss: 0.006419\n",
      "Epoch 88 of 500, Train Loss: 0.006433\n",
      "Epoch 89 of 500, Train Loss: 0.006465\n",
      "Epoch 90 of 500, Train Loss: 0.006427\n",
      "Epoch 91 of 500, Train Loss: 0.006435\n",
      "Epoch 92 of 500, Train Loss: 0.006412\n",
      "Epoch 93 of 500, Train Loss: 0.006464\n",
      "Epoch 94 of 500, Train Loss: 0.006498\n",
      "Epoch 95 of 500, Train Loss: 0.006631\n",
      "Epoch 96 of 500, Train Loss: 0.006665\n",
      "Epoch 97 of 500, Train Loss: 0.006687\n",
      "Epoch 98 of 500, Train Loss: 0.006608\n",
      "Epoch 99 of 500, Train Loss: 0.006463\n",
      "Epoch 100 of 500, Train Loss: 0.006405\n",
      "Epoch 101 of 500, Train Loss: 0.006430\n",
      "Epoch 102 of 500, Train Loss: 0.006514\n",
      "Epoch 103 of 500, Train Loss: 0.006537\n",
      "Epoch 104 of 500, Train Loss: 0.006628\n",
      "Epoch 105 of 500, Train Loss: 0.006745\n",
      "Epoch 106 of 500, Train Loss: 0.006944\n",
      "Epoch 107 of 500, Train Loss: 0.006912\n",
      "Epoch 108 of 500, Train Loss: 0.006897\n",
      "Epoch 109 of 500, Train Loss: 0.006876\n",
      "Epoch 110 of 500, Train Loss: 0.006888\n",
      "Epoch 111 of 500, Train Loss: 0.006789\n",
      "Epoch 112 of 500, Train Loss: 0.006627\n",
      "Epoch 113 of 500, Train Loss: 0.006664\n",
      "Epoch 114 of 500, Train Loss: 0.006592\n",
      "Epoch 115 of 500, Train Loss: 0.006678\n",
      "Epoch 116 of 500, Train Loss: 0.006632\n",
      "Epoch 117 of 500, Train Loss: 0.006757\n",
      "Epoch 118 of 500, Train Loss: 0.006742\n",
      "Epoch 119 of 500, Train Loss: 0.006955\n",
      "Epoch 120 of 500, Train Loss: 0.006736\n",
      "Epoch 121 of 500, Train Loss: 0.006763\n",
      "Epoch 122 of 500, Train Loss: 0.006598\n",
      "Epoch 123 of 500, Train Loss: 0.006656\n",
      "Epoch 124 of 500, Train Loss: 0.006532\n",
      "Epoch 125 of 500, Train Loss: 0.006584\n",
      "Epoch 126 of 500, Train Loss: 0.006446\n",
      "Epoch 127 of 500, Train Loss: 0.006497\n",
      "Epoch 128 of 500, Train Loss: 0.006482\n",
      "Epoch 129 of 500, Train Loss: 0.006629\n",
      "Epoch 130 of 500, Train Loss: 0.006553\n",
      "Epoch 131 of 500, Train Loss: 0.006664\n",
      "Epoch 132 of 500, Train Loss: 0.006570\n",
      "Epoch 133 of 500, Train Loss: 0.006643\n",
      "Epoch 134 of 500, Train Loss: 0.006570\n",
      "Epoch 135 of 500, Train Loss: 0.006608\n",
      "Epoch 136 of 500, Train Loss: 0.006532\n",
      "Epoch 137 of 500, Train Loss: 0.006578\n",
      "Epoch 138 of 500, Train Loss: 0.006519\n",
      "Epoch 139 of 500, Train Loss: 0.006570\n",
      "Epoch 140 of 500, Train Loss: 0.006540\n",
      "Epoch 141 of 500, Train Loss: 0.006587\n",
      "Epoch 142 of 500, Train Loss: 0.006552\n",
      "Epoch 143 of 500, Train Loss: 0.006562\n",
      "Epoch 144 of 500, Train Loss: 0.006564\n",
      "Epoch 145 of 500, Train Loss: 0.006637\n",
      "Epoch 146 of 500, Train Loss: 0.006641\n",
      "Epoch 147 of 500, Train Loss: 0.006620\n",
      "Epoch 148 of 500, Train Loss: 0.006571\n",
      "Epoch 149 of 500, Train Loss: 0.006584\n",
      "Epoch 150 of 500, Train Loss: 0.006583\n",
      "Epoch 151 of 500, Train Loss: 0.006638\n",
      "Epoch 152 of 500, Train Loss: 0.006627\n",
      "Epoch 153 of 500, Train Loss: 0.006587\n",
      "Epoch 154 of 500, Train Loss: 0.006539\n",
      "Epoch 155 of 500, Train Loss: 0.006559\n",
      "Epoch 156 of 500, Train Loss: 0.006557\n",
      "Epoch 157 of 500, Train Loss: 0.006624\n",
      "Epoch 158 of 500, Train Loss: 0.006646\n",
      "Epoch 159 of 500, Train Loss: 0.006590\n",
      "Epoch 160 of 500, Train Loss: 0.006558\n",
      "Epoch 161 of 500, Train Loss: 0.006512\n",
      "Epoch 162 of 500, Train Loss: 0.006531\n",
      "Epoch 163 of 500, Train Loss: 0.006553\n",
      "Epoch 164 of 500, Train Loss: 0.006596\n",
      "Epoch 165 of 500, Train Loss: 0.006605\n",
      "Epoch 166 of 500, Train Loss: 0.006604\n",
      "Epoch 167 of 500, Train Loss: 0.006598\n",
      "Epoch 168 of 500, Train Loss: 0.006566\n",
      "Epoch 169 of 500, Train Loss: 0.006543\n",
      "Epoch 170 of 500, Train Loss: 0.006534\n",
      "Epoch 171 of 500, Train Loss: 0.006540\n",
      "Epoch 172 of 500, Train Loss: 0.006491\n",
      "Epoch 173 of 500, Train Loss: 0.006464\n",
      "Epoch 174 of 500, Train Loss: 0.006433\n",
      "Epoch 175 of 500, Train Loss: 0.006449\n",
      "Epoch 176 of 500, Train Loss: 0.006469\n",
      "Epoch 177 of 500, Train Loss: 0.006537\n",
      "Epoch 178 of 500, Train Loss: 0.006541\n",
      "Epoch 179 of 500, Train Loss: 0.006579\n",
      "Epoch 180 of 500, Train Loss: 0.006533\n",
      "Epoch 181 of 500, Train Loss: 0.006536\n",
      "Epoch 182 of 500, Train Loss: 0.006536\n",
      "Epoch 183 of 500, Train Loss: 0.006557\n",
      "Epoch 184 of 500, Train Loss: 0.006544\n",
      "Epoch 185 of 500, Train Loss: 0.006586\n",
      "Epoch 186 of 500, Train Loss: 0.006548\n",
      "Epoch 187 of 500, Train Loss: 0.006544\n",
      "Epoch 188 of 500, Train Loss: 0.006533\n",
      "Epoch 189 of 500, Train Loss: 0.006599\n",
      "Epoch 190 of 500, Train Loss: 0.006532\n",
      "Epoch 191 of 500, Train Loss: 0.006589\n",
      "Epoch 192 of 500, Train Loss: 0.006519\n",
      "Epoch 193 of 500, Train Loss: 0.006547\n",
      "Epoch 194 of 500, Train Loss: 0.006432\n",
      "Epoch 195 of 500, Train Loss: 0.006433\n",
      "Epoch 196 of 500, Train Loss: 0.006384\n",
      "Epoch 197 of 500, Train Loss: 0.006410\n",
      "Epoch 198 of 500, Train Loss: 0.006359\n",
      "Epoch 199 of 500, Train Loss: 0.006355\n",
      "Epoch 200 of 500, Train Loss: 0.006331\n",
      "Epoch 201 of 500, Train Loss: 0.006361\n",
      "Epoch 202 of 500, Train Loss: 0.006361\n",
      "Epoch 203 of 500, Train Loss: 0.006387\n",
      "Epoch 204 of 500, Train Loss: 0.006380\n",
      "Epoch 205 of 500, Train Loss: 0.006377\n",
      "Epoch 206 of 500, Train Loss: 0.006354\n",
      "Epoch 207 of 500, Train Loss: 0.006359\n",
      "Epoch 208 of 500, Train Loss: 0.006339\n",
      "Epoch 209 of 500, Train Loss: 0.006364\n",
      "Epoch 210 of 500, Train Loss: 0.006353\n",
      "Epoch 211 of 500, Train Loss: 0.006353\n",
      "Epoch 212 of 500, Train Loss: 0.006336\n",
      "Epoch 213 of 500, Train Loss: 0.006328\n",
      "Epoch 214 of 500, Train Loss: 0.006308\n",
      "Epoch 215 of 500, Train Loss: 0.006335\n",
      "Epoch 216 of 500, Train Loss: 0.006337\n",
      "Epoch 217 of 500, Train Loss: 0.006354\n",
      "Epoch 218 of 500, Train Loss: 0.006336\n",
      "Epoch 219 of 500, Train Loss: 0.006319\n",
      "Epoch 220 of 500, Train Loss: 0.006328\n",
      "Epoch 221 of 500, Train Loss: 0.006394\n",
      "Epoch 222 of 500, Train Loss: 0.006409\n",
      "Epoch 223 of 500, Train Loss: 0.006460\n",
      "Epoch 224 of 500, Train Loss: 0.006430\n",
      "Epoch 225 of 500, Train Loss: 0.006421\n",
      "Epoch 226 of 500, Train Loss: 0.006410\n",
      "Epoch 227 of 500, Train Loss: 0.006421\n",
      "Epoch 228 of 500, Train Loss: 0.006378\n",
      "Epoch 229 of 500, Train Loss: 0.006377\n",
      "Epoch 230 of 500, Train Loss: 0.006360\n",
      "Epoch 231 of 500, Train Loss: 0.006352\n",
      "Epoch 232 of 500, Train Loss: 0.006348\n",
      "Epoch 233 of 500, Train Loss: 0.006360\n",
      "Epoch 234 of 500, Train Loss: 0.006358\n",
      "Epoch 235 of 500, Train Loss: 0.006350\n",
      "Epoch 236 of 500, Train Loss: 0.006358\n",
      "Epoch 237 of 500, Train Loss: 0.006377\n",
      "Epoch 238 of 500, Train Loss: 0.006406\n",
      "Epoch 239 of 500, Train Loss: 0.006440\n",
      "Epoch 240 of 500, Train Loss: 0.006408\n",
      "Epoch 241 of 500, Train Loss: 0.006435\n",
      "Epoch 242 of 500, Train Loss: 0.006454\n",
      "Epoch 243 of 500, Train Loss: 0.006440\n",
      "Epoch 244 of 500, Train Loss: 0.006408\n",
      "Epoch 245 of 500, Train Loss: 0.006427\n",
      "Epoch 246 of 500, Train Loss: 0.006424\n",
      "Epoch 247 of 500, Train Loss: 0.006415\n",
      "Epoch 248 of 500, Train Loss: 0.006421\n",
      "Epoch 249 of 500, Train Loss: 0.006401\n",
      "Epoch 250 of 500, Train Loss: 0.006404\n",
      "Epoch 251 of 500, Train Loss: 0.006458\n",
      "Epoch 252 of 500, Train Loss: 0.006465\n",
      "Epoch 253 of 500, Train Loss: 0.006429\n",
      "Epoch 254 of 500, Train Loss: 0.006392\n",
      "Epoch 255 of 500, Train Loss: 0.006411\n",
      "Epoch 256 of 500, Train Loss: 0.006399\n",
      "Epoch 257 of 500, Train Loss: 0.006425\n",
      "Epoch 258 of 500, Train Loss: 0.006445\n",
      "Epoch 259 of 500, Train Loss: 0.006472\n",
      "Epoch 260 of 500, Train Loss: 0.006482\n",
      "Epoch 261 of 500, Train Loss: 0.006528\n",
      "Epoch 262 of 500, Train Loss: 0.006581\n",
      "Epoch 263 of 500, Train Loss: 0.006616\n",
      "Epoch 264 of 500, Train Loss: 0.006649\n",
      "Epoch 265 of 500, Train Loss: 0.006647\n",
      "Epoch 266 of 500, Train Loss: 0.006723\n",
      "Epoch 267 of 500, Train Loss: 0.006711\n",
      "Epoch 268 of 500, Train Loss: 0.006780\n",
      "Epoch 269 of 500, Train Loss: 0.006654\n",
      "Epoch 270 of 500, Train Loss: 0.006630\n",
      "Epoch 271 of 500, Train Loss: 0.006547\n",
      "Epoch 272 of 500, Train Loss: 0.006531\n",
      "Epoch 273 of 500, Train Loss: 0.006537\n",
      "Epoch 274 of 500, Train Loss: 0.006509\n",
      "Epoch 275 of 500, Train Loss: 0.006515\n",
      "Epoch 276 of 500, Train Loss: 0.006524\n",
      "Epoch 277 of 500, Train Loss: 0.006554\n",
      "Epoch 278 of 500, Train Loss: 0.006576\n",
      "Epoch 279 of 500, Train Loss: 0.006595\n",
      "Epoch 280 of 500, Train Loss: 0.006546\n",
      "Epoch 281 of 500, Train Loss: 0.006504\n",
      "Epoch 282 of 500, Train Loss: 0.006521\n",
      "Epoch 283 of 500, Train Loss: 0.006526\n",
      "Epoch 284 of 500, Train Loss: 0.006510\n",
      "Epoch 285 of 500, Train Loss: 0.006495\n",
      "Epoch 286 of 500, Train Loss: 0.006516\n",
      "Epoch 287 of 500, Train Loss: 0.006488\n",
      "Epoch 288 of 500, Train Loss: 0.006495\n",
      "Epoch 289 of 500, Train Loss: 0.006474\n",
      "Epoch 290 of 500, Train Loss: 0.006475\n",
      "Epoch 291 of 500, Train Loss: 0.006486\n",
      "Epoch 292 of 500, Train Loss: 0.006518\n",
      "Epoch 293 of 500, Train Loss: 0.006560\n",
      "Epoch 294 of 500, Train Loss: 0.006538\n",
      "Epoch 295 of 500, Train Loss: 0.006544\n",
      "Epoch 296 of 500, Train Loss: 0.006577\n",
      "Epoch 297 of 500, Train Loss: 0.006548\n",
      "Epoch 298 of 500, Train Loss: 0.006537\n",
      "Epoch 299 of 500, Train Loss: 0.006499\n",
      "Epoch 300 of 500, Train Loss: 0.006428\n",
      "Epoch 301 of 500, Train Loss: 0.006433\n",
      "Epoch 302 of 500, Train Loss: 0.006433\n",
      "Epoch 303 of 500, Train Loss: 0.006476\n",
      "Epoch 304 of 500, Train Loss: 0.006494\n",
      "Epoch 305 of 500, Train Loss: 0.006509\n",
      "Epoch 306 of 500, Train Loss: 0.006464\n",
      "Epoch 307 of 500, Train Loss: 0.006482\n",
      "Epoch 308 of 500, Train Loss: 0.006506\n",
      "Epoch 309 of 500, Train Loss: 0.006442\n",
      "Epoch 310 of 500, Train Loss: 0.006454\n",
      "Epoch 311 of 500, Train Loss: 0.006404\n",
      "Epoch 312 of 500, Train Loss: 0.006420\n",
      "Epoch 313 of 500, Train Loss: 0.006395\n",
      "Epoch 314 of 500, Train Loss: 0.006439\n",
      "Epoch 315 of 500, Train Loss: 0.006462\n",
      "Epoch 316 of 500, Train Loss: 0.006488\n",
      "Epoch 317 of 500, Train Loss: 0.006474\n",
      "Epoch 318 of 500, Train Loss: 0.006500\n",
      "Epoch 319 of 500, Train Loss: 0.006501\n",
      "Epoch 320 of 500, Train Loss: 0.006504\n",
      "Epoch 321 of 500, Train Loss: 0.006487\n",
      "Epoch 322 of 500, Train Loss: 0.006493\n",
      "Epoch 323 of 500, Train Loss: 0.006435\n",
      "Epoch 324 of 500, Train Loss: 0.006465\n",
      "Epoch 325 of 500, Train Loss: 0.006405\n",
      "Epoch 326 of 500, Train Loss: 0.006403\n",
      "Epoch 327 of 500, Train Loss: 0.006358\n",
      "Epoch 328 of 500, Train Loss: 0.006367\n",
      "Epoch 329 of 500, Train Loss: 0.006339\n",
      "Epoch 330 of 500, Train Loss: 0.006328\n",
      "Epoch 331 of 500, Train Loss: 0.006318\n",
      "Epoch 332 of 500, Train Loss: 0.006313\n",
      "Epoch 333 of 500, Train Loss: 0.006318\n",
      "Epoch 334 of 500, Train Loss: 0.006347\n",
      "Epoch 335 of 500, Train Loss: 0.006352\n",
      "Epoch 336 of 500, Train Loss: 0.006375\n",
      "Epoch 337 of 500, Train Loss: 0.006360\n",
      "Epoch 338 of 500, Train Loss: 0.006365\n",
      "Epoch 339 of 500, Train Loss: 0.006390\n",
      "Epoch 340 of 500, Train Loss: 0.006425\n",
      "Epoch 341 of 500, Train Loss: 0.006473\n",
      "Epoch 342 of 500, Train Loss: 0.006519\n",
      "Epoch 343 of 500, Train Loss: 0.006557\n",
      "Epoch 344 of 500, Train Loss: 0.006566\n",
      "Epoch 345 of 500, Train Loss: 0.006570\n",
      "Epoch 346 of 500, Train Loss: 0.006553\n",
      "Epoch 347 of 500, Train Loss: 0.006531\n",
      "Epoch 348 of 500, Train Loss: 0.006508\n",
      "Epoch 349 of 500, Train Loss: 0.006430\n",
      "Epoch 350 of 500, Train Loss: 0.006395\n",
      "Epoch 351 of 500, Train Loss: 0.006395\n",
      "Epoch 352 of 500, Train Loss: 0.006403\n",
      "Epoch 353 of 500, Train Loss: 0.006382\n",
      "Epoch 354 of 500, Train Loss: 0.006365\n",
      "Epoch 355 of 500, Train Loss: 0.006388\n",
      "Epoch 356 of 500, Train Loss: 0.006391\n",
      "Epoch 357 of 500, Train Loss: 0.006387\n",
      "Epoch 358 of 500, Train Loss: 0.006361\n",
      "Epoch 359 of 500, Train Loss: 0.006365\n",
      "Epoch 360 of 500, Train Loss: 0.006391\n",
      "Epoch 361 of 500, Train Loss: 0.006387\n",
      "Epoch 362 of 500, Train Loss: 0.006390\n",
      "Epoch 363 of 500, Train Loss: 0.006420\n",
      "Epoch 364 of 500, Train Loss: 0.006422\n",
      "Epoch 365 of 500, Train Loss: 0.006455\n",
      "Epoch 366 of 500, Train Loss: 0.006430\n",
      "Epoch 367 of 500, Train Loss: 0.006436\n",
      "Epoch 368 of 500, Train Loss: 0.006429\n",
      "Epoch 369 of 500, Train Loss: 0.006415\n",
      "Epoch 370 of 500, Train Loss: 0.006394\n",
      "Epoch 371 of 500, Train Loss: 0.006411\n",
      "Epoch 372 of 500, Train Loss: 0.006444\n",
      "Epoch 373 of 500, Train Loss: 0.006429\n",
      "Epoch 374 of 500, Train Loss: 0.006425\n",
      "Epoch 375 of 500, Train Loss: 0.006416\n",
      "Epoch 376 of 500, Train Loss: 0.006432\n",
      "Epoch 377 of 500, Train Loss: 0.006416\n",
      "Epoch 378 of 500, Train Loss: 0.006404\n",
      "Epoch 379 of 500, Train Loss: 0.006379\n",
      "Epoch 380 of 500, Train Loss: 0.006390\n",
      "Epoch 381 of 500, Train Loss: 0.006348\n",
      "Epoch 382 of 500, Train Loss: 0.006337\n",
      "Epoch 383 of 500, Train Loss: 0.006351\n",
      "Epoch 384 of 500, Train Loss: 0.006389\n",
      "Epoch 385 of 500, Train Loss: 0.006374\n",
      "Epoch 386 of 500, Train Loss: 0.006402\n",
      "Epoch 387 of 500, Train Loss: 0.006396\n",
      "Epoch 388 of 500, Train Loss: 0.006410\n",
      "Epoch 389 of 500, Train Loss: 0.006357\n",
      "Epoch 390 of 500, Train Loss: 0.006381\n",
      "Epoch 391 of 500, Train Loss: 0.006374\n",
      "Epoch 392 of 500, Train Loss: 0.006402\n",
      "Epoch 393 of 500, Train Loss: 0.006359\n",
      "Epoch 394 of 500, Train Loss: 0.006364\n",
      "Epoch 395 of 500, Train Loss: 0.006329\n",
      "Epoch 396 of 500, Train Loss: 0.006374\n",
      "Epoch 397 of 500, Train Loss: 0.006329\n",
      "Epoch 398 of 500, Train Loss: 0.006368\n",
      "Epoch 399 of 500, Train Loss: 0.006336\n",
      "Epoch 400 of 500, Train Loss: 0.006396\n",
      "Epoch 401 of 500, Train Loss: 0.006399\n",
      "Epoch 402 of 500, Train Loss: 0.006425\n",
      "Epoch 403 of 500, Train Loss: 0.006407\n",
      "Epoch 404 of 500, Train Loss: 0.006418\n",
      "Epoch 405 of 500, Train Loss: 0.006377\n",
      "Epoch 406 of 500, Train Loss: 0.006388\n",
      "Epoch 407 of 500, Train Loss: 0.006383\n",
      "Epoch 408 of 500, Train Loss: 0.006426\n",
      "Epoch 409 of 500, Train Loss: 0.006408\n",
      "Epoch 410 of 500, Train Loss: 0.006447\n",
      "Epoch 411 of 500, Train Loss: 0.006415\n",
      "Epoch 412 of 500, Train Loss: 0.006393\n",
      "Epoch 413 of 500, Train Loss: 0.006327\n",
      "Epoch 414 of 500, Train Loss: 0.006329\n",
      "Epoch 415 of 500, Train Loss: 0.006346\n",
      "Epoch 416 of 500, Train Loss: 0.006394\n",
      "Epoch 417 of 500, Train Loss: 0.006355\n",
      "Epoch 418 of 500, Train Loss: 0.006392\n",
      "Epoch 419 of 500, Train Loss: 0.006328\n",
      "Epoch 420 of 500, Train Loss: 0.006392\n",
      "Epoch 421 of 500, Train Loss: 0.006392\n",
      "Epoch 422 of 500, Train Loss: 0.006432\n",
      "Epoch 423 of 500, Train Loss: 0.006384\n",
      "Epoch 424 of 500, Train Loss: 0.006369\n",
      "Epoch 425 of 500, Train Loss: 0.006376\n",
      "Epoch 426 of 500, Train Loss: 0.006360\n",
      "Epoch 427 of 500, Train Loss: 0.006363\n",
      "Epoch 428 of 500, Train Loss: 0.006360\n",
      "Epoch 429 of 500, Train Loss: 0.006317\n",
      "Epoch 430 of 500, Train Loss: 0.006327\n",
      "Epoch 431 of 500, Train Loss: 0.006309\n",
      "Epoch 432 of 500, Train Loss: 0.006330\n",
      "Epoch 433 of 500, Train Loss: 0.006346\n",
      "Epoch 434 of 500, Train Loss: 0.006353\n",
      "Epoch 435 of 500, Train Loss: 0.006333\n",
      "Epoch 436 of 500, Train Loss: 0.006356\n",
      "Epoch 437 of 500, Train Loss: 0.006359\n",
      "Epoch 438 of 500, Train Loss: 0.006359\n",
      "Epoch 439 of 500, Train Loss: 0.006356\n",
      "Epoch 440 of 500, Train Loss: 0.006366\n",
      "Epoch 441 of 500, Train Loss: 0.006335\n",
      "Epoch 442 of 500, Train Loss: 0.006359\n",
      "Epoch 443 of 500, Train Loss: 0.006310\n",
      "Epoch 444 of 500, Train Loss: 0.006320\n",
      "Epoch 445 of 500, Train Loss: 0.006313\n",
      "Epoch 446 of 500, Train Loss: 0.006324\n",
      "Epoch 447 of 500, Train Loss: 0.006310\n",
      "Epoch 448 of 500, Train Loss: 0.006334\n",
      "Epoch 449 of 500, Train Loss: 0.006333\n",
      "Epoch 450 of 500, Train Loss: 0.006341\n",
      "Epoch 451 of 500, Train Loss: 0.006344\n",
      "Epoch 452 of 500, Train Loss: 0.006383\n",
      "Epoch 453 of 500, Train Loss: 0.006365\n",
      "Epoch 454 of 500, Train Loss: 0.006377\n",
      "Epoch 455 of 500, Train Loss: 0.006363\n",
      "Epoch 456 of 500, Train Loss: 0.006380\n",
      "Epoch 457 of 500, Train Loss: 0.006356\n",
      "Epoch 458 of 500, Train Loss: 0.006383\n",
      "Epoch 459 of 500, Train Loss: 0.006333\n",
      "Epoch 460 of 500, Train Loss: 0.006345\n",
      "Epoch 461 of 500, Train Loss: 0.006347\n",
      "Epoch 462 of 500, Train Loss: 0.006383\n",
      "Epoch 463 of 500, Train Loss: 0.006345\n",
      "Epoch 464 of 500, Train Loss: 0.006359\n",
      "Epoch 465 of 500, Train Loss: 0.006312\n",
      "Epoch 466 of 500, Train Loss: 0.006293\n",
      "Epoch 467 of 500, Train Loss: 0.006262\n",
      "best loss:  0.006261686929375251\n",
      "Epoch 468 of 500, Train Loss: 0.006275\n",
      "Epoch 469 of 500, Train Loss: 0.006277\n",
      "Epoch 470 of 500, Train Loss: 0.006294\n",
      "Epoch 471 of 500, Train Loss: 0.006305\n",
      "Epoch 472 of 500, Train Loss: 0.006314\n",
      "Epoch 473 of 500, Train Loss: 0.006294\n",
      "Epoch 474 of 500, Train Loss: 0.006331\n",
      "Epoch 475 of 500, Train Loss: 0.006316\n",
      "Epoch 476 of 500, Train Loss: 0.006347\n",
      "Epoch 477 of 500, Train Loss: 0.006323\n",
      "Epoch 478 of 500, Train Loss: 0.006342\n",
      "Epoch 479 of 500, Train Loss: 0.006308\n",
      "Epoch 480 of 500, Train Loss: 0.006341\n",
      "Epoch 481 of 500, Train Loss: 0.006333\n",
      "Epoch 482 of 500, Train Loss: 0.006350\n",
      "Epoch 483 of 500, Train Loss: 0.006329\n",
      "Epoch 484 of 500, Train Loss: 0.006347\n",
      "Epoch 485 of 500, Train Loss: 0.006331\n",
      "Epoch 486 of 500, Train Loss: 0.006355\n",
      "Epoch 487 of 500, Train Loss: 0.006346\n",
      "Epoch 488 of 500, Train Loss: 0.006359\n",
      "Epoch 489 of 500, Train Loss: 0.006348\n",
      "Epoch 490 of 500, Train Loss: 0.006394\n",
      "Epoch 491 of 500, Train Loss: 0.006392\n",
      "Epoch 492 of 500, Train Loss: 0.006438\n",
      "Epoch 493 of 500, Train Loss: 0.006383\n",
      "Epoch 494 of 500, Train Loss: 0.006388\n",
      "Epoch 495 of 500, Train Loss: 0.006364\n",
      "Epoch 496 of 500, Train Loss: 0.006418\n",
      "Epoch 497 of 500, Train Loss: 0.006402\n",
      "Epoch 498 of 500, Train Loss: 0.006406\n",
      "Epoch 499 of 500, Train Loss: 0.006397\n",
      "Epoch 500 of 500, Train Loss: 0.006464\n",
      "latent train shape:  (16395, 150)\n",
      "M: 150, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 32\n",
      "Training the subspace: 0 / 150\n",
      "Training the subspace: 1 / 150\n",
      "Training the subspace: 2 / 150\n",
      "Training the subspace: 3 / 150\n",
      "Training the subspace: 4 / 150\n",
      "Training the subspace: 5 / 150\n",
      "Training the subspace: 6 / 150\n",
      "Training the subspace: 7 / 150\n",
      "Training the subspace: 8 / 150\n",
      "Training the subspace: 9 / 150\n",
      "Training the subspace: 10 / 150\n",
      "Training the subspace: 11 / 150\n",
      "Training the subspace: 12 / 150\n",
      "Training the subspace: 13 / 150\n",
      "Training the subspace: 14 / 150\n",
      "Training the subspace: 15 / 150\n",
      "Training the subspace: 16 / 150\n",
      "Training the subspace: 17 / 150\n",
      "Training the subspace: 18 / 150\n",
      "Training the subspace: 19 / 150\n",
      "Training the subspace: 20 / 150\n",
      "Training the subspace: 21 / 150\n",
      "Training the subspace: 22 / 150\n",
      "Training the subspace: 23 / 150\n",
      "Training the subspace: 24 / 150\n",
      "Training the subspace: 25 / 150\n",
      "Training the subspace: 26 / 150\n",
      "Training the subspace: 27 / 150\n",
      "Training the subspace: 28 / 150\n",
      "Training the subspace: 29 / 150\n",
      "Training the subspace: 30 / 150\n",
      "Training the subspace: 31 / 150\n",
      "Training the subspace: 32 / 150\n",
      "Training the subspace: 33 / 150\n",
      "Training the subspace: 34 / 150\n",
      "Training the subspace: 35 / 150\n",
      "Training the subspace: 36 / 150\n",
      "Training the subspace: 37 / 150\n",
      "Training the subspace: 38 / 150\n",
      "Training the subspace: 39 / 150\n",
      "Training the subspace: 40 / 150\n",
      "Training the subspace: 41 / 150\n",
      "Training the subspace: 42 / 150\n",
      "Training the subspace: 43 / 150\n",
      "Training the subspace: 44 / 150\n",
      "Training the subspace: 45 / 150\n",
      "Training the subspace: 46 / 150\n",
      "Training the subspace: 47 / 150\n",
      "Training the subspace: 48 / 150\n",
      "Training the subspace: 49 / 150\n",
      "Training the subspace: 50 / 150\n",
      "Training the subspace: 51 / 150\n",
      "Training the subspace: 52 / 150\n",
      "Training the subspace: 53 / 150\n",
      "Training the subspace: 54 / 150\n",
      "Training the subspace: 55 / 150\n",
      "Training the subspace: 56 / 150\n",
      "Training the subspace: 57 / 150\n",
      "Training the subspace: 58 / 150\n",
      "Training the subspace: 59 / 150\n",
      "Training the subspace: 60 / 150\n",
      "Training the subspace: 61 / 150\n",
      "Training the subspace: 62 / 150\n",
      "Training the subspace: 63 / 150\n",
      "Training the subspace: 64 / 150\n",
      "Training the subspace: 65 / 150\n",
      "Training the subspace: 66 / 150\n",
      "Training the subspace: 67 / 150\n",
      "Training the subspace: 68 / 150\n",
      "Training the subspace: 69 / 150\n",
      "Training the subspace: 70 / 150\n",
      "Training the subspace: 71 / 150\n",
      "Training the subspace: 72 / 150\n",
      "Training the subspace: 73 / 150\n",
      "Training the subspace: 74 / 150\n",
      "Training the subspace: 75 / 150\n",
      "Training the subspace: 76 / 150\n",
      "Training the subspace: 77 / 150\n",
      "Training the subspace: 78 / 150\n",
      "Training the subspace: 79 / 150\n",
      "Training the subspace: 80 / 150\n",
      "Training the subspace: 81 / 150\n",
      "Training the subspace: 82 / 150\n",
      "Training the subspace: 83 / 150\n",
      "Training the subspace: 84 / 150\n",
      "Training the subspace: 85 / 150\n",
      "Training the subspace: 86 / 150\n",
      "Training the subspace: 87 / 150\n",
      "Training the subspace: 88 / 150\n",
      "Training the subspace: 89 / 150\n",
      "Training the subspace: 90 / 150\n",
      "Training the subspace: 91 / 150\n",
      "Training the subspace: 92 / 150\n",
      "Training the subspace: 93 / 150\n",
      "Training the subspace: 94 / 150\n",
      "Training the subspace: 95 / 150\n",
      "Training the subspace: 96 / 150\n",
      "Training the subspace: 97 / 150\n",
      "Training the subspace: 98 / 150\n",
      "Training the subspace: 99 / 150\n",
      "Training the subspace: 100 / 150\n",
      "Training the subspace: 101 / 150\n",
      "Training the subspace: 102 / 150\n",
      "Training the subspace: 103 / 150\n",
      "Training the subspace: 104 / 150\n",
      "Training the subspace: 105 / 150\n",
      "Training the subspace: 106 / 150\n",
      "Training the subspace: 107 / 150\n",
      "Training the subspace: 108 / 150\n",
      "Training the subspace: 109 / 150\n",
      "Training the subspace: 110 / 150\n",
      "Training the subspace: 111 / 150\n",
      "Training the subspace: 112 / 150\n",
      "Training the subspace: 113 / 150\n",
      "Training the subspace: 114 / 150\n",
      "Training the subspace: 115 / 150\n",
      "Training the subspace: 116 / 150\n",
      "Training the subspace: 117 / 150\n",
      "Training the subspace: 118 / 150\n",
      "Training the subspace: 119 / 150\n",
      "Training the subspace: 120 / 150\n",
      "Training the subspace: 121 / 150\n",
      "Training the subspace: 122 / 150\n",
      "Training the subspace: 123 / 150\n",
      "Training the subspace: 124 / 150\n",
      "Training the subspace: 125 / 150\n",
      "Training the subspace: 126 / 150\n",
      "Training the subspace: 127 / 150\n",
      "Training the subspace: 128 / 150\n",
      "Training the subspace: 129 / 150\n",
      "Training the subspace: 130 / 150\n",
      "Training the subspace: 131 / 150\n",
      "Training the subspace: 132 / 150\n",
      "Training the subspace: 133 / 150\n",
      "Training the subspace: 134 / 150\n",
      "Training the subspace: 135 / 150\n",
      "Training the subspace: 136 / 150\n",
      "Training the subspace: 137 / 150\n",
      "Training the subspace: 138 / 150\n",
      "Training the subspace: 139 / 150\n",
      "Training the subspace: 140 / 150\n",
      "Training the subspace: 141 / 150\n",
      "Training the subspace: 142 / 150\n",
      "Training the subspace: 143 / 150\n",
      "Training the subspace: 144 / 150\n",
      "Training the subspace: 145 / 150\n",
      "Training the subspace: 146 / 150\n",
      "Training the subspace: 147 / 150\n",
      "Training the subspace: 148 / 150\n",
      "Training the subspace: 149 / 150\n",
      "Encoding the subspace: 0 / 150\n",
      "Encoding the subspace: 1 / 150\n",
      "Encoding the subspace: 2 / 150\n",
      "Encoding the subspace: 3 / 150\n",
      "Encoding the subspace: 4 / 150\n",
      "Encoding the subspace: 5 / 150\n",
      "Encoding the subspace: 6 / 150\n",
      "Encoding the subspace: 7 / 150\n",
      "Encoding the subspace: 8 / 150\n",
      "Encoding the subspace: 9 / 150\n",
      "Encoding the subspace: 10 / 150\n",
      "Encoding the subspace: 11 / 150\n",
      "Encoding the subspace: 12 / 150\n",
      "Encoding the subspace: 13 / 150\n",
      "Encoding the subspace: 14 / 150\n",
      "Encoding the subspace: 15 / 150\n",
      "Encoding the subspace: 16 / 150\n",
      "Encoding the subspace: 17 / 150\n",
      "Encoding the subspace: 18 / 150\n",
      "Encoding the subspace: 19 / 150\n",
      "Encoding the subspace: 20 / 150\n",
      "Encoding the subspace: 21 / 150\n",
      "Encoding the subspace: 22 / 150\n",
      "Encoding the subspace: 23 / 150\n",
      "Encoding the subspace: 24 / 150\n",
      "Encoding the subspace: 25 / 150\n",
      "Encoding the subspace: 26 / 150\n",
      "Encoding the subspace: 27 / 150\n",
      "Encoding the subspace: 28 / 150\n",
      "Encoding the subspace: 29 / 150\n",
      "Encoding the subspace: 30 / 150\n",
      "Encoding the subspace: 31 / 150\n",
      "Encoding the subspace: 32 / 150\n",
      "Encoding the subspace: 33 / 150\n",
      "Encoding the subspace: 34 / 150\n",
      "Encoding the subspace: 35 / 150\n",
      "Encoding the subspace: 36 / 150\n",
      "Encoding the subspace: 37 / 150\n",
      "Encoding the subspace: 38 / 150\n",
      "Encoding the subspace: 39 / 150\n",
      "Encoding the subspace: 40 / 150\n",
      "Encoding the subspace: 41 / 150\n",
      "Encoding the subspace: 42 / 150\n",
      "Encoding the subspace: 43 / 150\n",
      "Encoding the subspace: 44 / 150\n",
      "Encoding the subspace: 45 / 150\n",
      "Encoding the subspace: 46 / 150\n",
      "Encoding the subspace: 47 / 150\n",
      "Encoding the subspace: 48 / 150\n",
      "Encoding the subspace: 49 / 150\n",
      "Encoding the subspace: 50 / 150\n",
      "Encoding the subspace: 51 / 150\n",
      "Encoding the subspace: 52 / 150\n",
      "Encoding the subspace: 53 / 150\n",
      "Encoding the subspace: 54 / 150\n",
      "Encoding the subspace: 55 / 150\n",
      "Encoding the subspace: 56 / 150\n",
      "Encoding the subspace: 57 / 150\n",
      "Encoding the subspace: 58 / 150\n",
      "Encoding the subspace: 59 / 150\n",
      "Encoding the subspace: 60 / 150\n",
      "Encoding the subspace: 61 / 150\n",
      "Encoding the subspace: 62 / 150\n",
      "Encoding the subspace: 63 / 150\n",
      "Encoding the subspace: 64 / 150\n",
      "Encoding the subspace: 65 / 150\n",
      "Encoding the subspace: 66 / 150\n",
      "Encoding the subspace: 67 / 150\n",
      "Encoding the subspace: 68 / 150\n",
      "Encoding the subspace: 69 / 150\n",
      "Encoding the subspace: 70 / 150\n",
      "Encoding the subspace: 71 / 150\n",
      "Encoding the subspace: 72 / 150\n",
      "Encoding the subspace: 73 / 150\n",
      "Encoding the subspace: 74 / 150\n",
      "Encoding the subspace: 75 / 150\n",
      "Encoding the subspace: 76 / 150\n",
      "Encoding the subspace: 77 / 150\n",
      "Encoding the subspace: 78 / 150\n",
      "Encoding the subspace: 79 / 150\n",
      "Encoding the subspace: 80 / 150\n",
      "Encoding the subspace: 81 / 150\n",
      "Encoding the subspace: 82 / 150\n",
      "Encoding the subspace: 83 / 150\n",
      "Encoding the subspace: 84 / 150\n",
      "Encoding the subspace: 85 / 150\n",
      "Encoding the subspace: 86 / 150\n",
      "Encoding the subspace: 87 / 150\n",
      "Encoding the subspace: 88 / 150\n",
      "Encoding the subspace: 89 / 150\n",
      "Encoding the subspace: 90 / 150\n",
      "Encoding the subspace: 91 / 150\n",
      "Encoding the subspace: 92 / 150\n",
      "Encoding the subspace: 93 / 150\n",
      "Encoding the subspace: 94 / 150\n",
      "Encoding the subspace: 95 / 150\n",
      "Encoding the subspace: 96 / 150\n",
      "Encoding the subspace: 97 / 150\n",
      "Encoding the subspace: 98 / 150\n",
      "Encoding the subspace: 99 / 150\n",
      "Encoding the subspace: 100 / 150\n",
      "Encoding the subspace: 101 / 150\n",
      "Encoding the subspace: 102 / 150\n",
      "Encoding the subspace: 103 / 150\n",
      "Encoding the subspace: 104 / 150\n",
      "Encoding the subspace: 105 / 150\n",
      "Encoding the subspace: 106 / 150\n",
      "Encoding the subspace: 107 / 150\n",
      "Encoding the subspace: 108 / 150\n",
      "Encoding the subspace: 109 / 150\n",
      "Encoding the subspace: 110 / 150\n",
      "Encoding the subspace: 111 / 150\n",
      "Encoding the subspace: 112 / 150\n",
      "Encoding the subspace: 113 / 150\n",
      "Encoding the subspace: 114 / 150\n",
      "Encoding the subspace: 115 / 150\n",
      "Encoding the subspace: 116 / 150\n",
      "Encoding the subspace: 117 / 150\n",
      "Encoding the subspace: 118 / 150\n",
      "Encoding the subspace: 119 / 150\n",
      "Encoding the subspace: 120 / 150\n",
      "Encoding the subspace: 121 / 150\n",
      "Encoding the subspace: 122 / 150\n",
      "Encoding the subspace: 123 / 150\n",
      "Encoding the subspace: 124 / 150\n",
      "Encoding the subspace: 125 / 150\n",
      "Encoding the subspace: 126 / 150\n",
      "Encoding the subspace: 127 / 150\n",
      "Encoding the subspace: 128 / 150\n",
      "Encoding the subspace: 129 / 150\n",
      "Encoding the subspace: 130 / 150\n",
      "Encoding the subspace: 131 / 150\n",
      "Encoding the subspace: 132 / 150\n",
      "Encoding the subspace: 133 / 150\n",
      "Encoding the subspace: 134 / 150\n",
      "Encoding the subspace: 135 / 150\n",
      "Encoding the subspace: 136 / 150\n",
      "Encoding the subspace: 137 / 150\n",
      "Encoding the subspace: 138 / 150\n",
      "Encoding the subspace: 139 / 150\n",
      "Encoding the subspace: 140 / 150\n",
      "Encoding the subspace: 141 / 150\n",
      "Encoding the subspace: 142 / 150\n",
      "Encoding the subspace: 143 / 150\n",
      "Encoding the subspace: 144 / 150\n",
      "Encoding the subspace: 145 / 150\n",
      "Encoding the subspace: 146 / 150\n",
      "Encoding the subspace: 147 / 150\n",
      "Encoding the subspace: 148 / 150\n",
      "Encoding the subspace: 149 / 150\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=180, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.220838\n",
      "best loss:  0.2208383667333297\n",
      "Epoch 2 of 500, Train Loss: 0.044080\n",
      "best loss:  0.04407960094616192\n",
      "Epoch 3 of 500, Train Loss: 0.032275\n",
      "best loss:  0.03227516560040977\n",
      "Epoch 4 of 500, Train Loss: 0.025639\n",
      "best loss:  0.025639470995136703\n",
      "Epoch 5 of 500, Train Loss: 0.021916\n",
      "best loss:  0.021916359102140476\n",
      "Epoch 6 of 500, Train Loss: 0.019409\n",
      "best loss:  0.019409183028123392\n",
      "Epoch 7 of 500, Train Loss: 0.017627\n",
      "best loss:  0.017627095965607194\n",
      "Epoch 8 of 500, Train Loss: 0.016278\n",
      "best loss:  0.016277724032086327\n",
      "Epoch 9 of 500, Train Loss: 0.015064\n",
      "best loss:  0.015063909310874594\n",
      "Epoch 10 of 500, Train Loss: 0.014027\n",
      "best loss:  0.014026822691716527\n",
      "Epoch 11 of 500, Train Loss: 0.013114\n",
      "best loss:  0.01311442055834219\n",
      "Epoch 12 of 500, Train Loss: 0.012364\n",
      "best loss:  0.012364070919611267\n",
      "Epoch 13 of 500, Train Loss: 0.011690\n",
      "best loss:  0.011690228924564946\n",
      "Epoch 14 of 500, Train Loss: 0.011078\n",
      "best loss:  0.011078460797416624\n",
      "Epoch 15 of 500, Train Loss: 0.010578\n",
      "best loss:  0.010577838270710203\n",
      "Epoch 16 of 500, Train Loss: 0.010116\n",
      "best loss:  0.01011636193147716\n",
      "Epoch 17 of 500, Train Loss: 0.009678\n",
      "best loss:  0.00967829643359296\n",
      "Epoch 18 of 500, Train Loss: 0.009315\n",
      "best loss:  0.009315029983413261\n",
      "Epoch 19 of 500, Train Loss: 0.009061\n",
      "best loss:  0.009061027432060007\n",
      "Epoch 20 of 500, Train Loss: 0.008719\n",
      "best loss:  0.008719248500880027\n",
      "Epoch 21 of 500, Train Loss: 0.008451\n",
      "best loss:  0.008451100081578077\n",
      "Epoch 22 of 500, Train Loss: 0.008138\n",
      "best loss:  0.008137980458875324\n",
      "Epoch 23 of 500, Train Loss: 0.007954\n",
      "best loss:  0.007954053029480261\n",
      "Epoch 24 of 500, Train Loss: 0.007729\n",
      "best loss:  0.007728505950482461\n",
      "Epoch 25 of 500, Train Loss: 0.007550\n",
      "best loss:  0.007549992219460118\n",
      "Epoch 26 of 500, Train Loss: 0.007396\n",
      "best loss:  0.007396012222120906\n",
      "Epoch 27 of 500, Train Loss: 0.007229\n",
      "best loss:  0.007229090202825375\n",
      "Epoch 28 of 500, Train Loss: 0.007102\n",
      "best loss:  0.007101788728921169\n",
      "Epoch 29 of 500, Train Loss: 0.006971\n",
      "best loss:  0.006971323382684261\n",
      "Epoch 30 of 500, Train Loss: 0.006912\n",
      "best loss:  0.0069122586065186765\n",
      "Epoch 31 of 500, Train Loss: 0.006765\n",
      "best loss:  0.006765283328795069\n",
      "Epoch 32 of 500, Train Loss: 0.006703\n",
      "best loss:  0.006703379617231472\n",
      "Epoch 33 of 500, Train Loss: 0.006589\n",
      "best loss:  0.006588826048374717\n",
      "Epoch 34 of 500, Train Loss: 0.006517\n",
      "best loss:  0.006516850697618455\n",
      "Epoch 35 of 500, Train Loss: 0.006478\n",
      "best loss:  0.006477847943861261\n",
      "Epoch 36 of 500, Train Loss: 0.006405\n",
      "best loss:  0.0064051366125074645\n",
      "Epoch 37 of 500, Train Loss: 0.006341\n",
      "best loss:  0.006340855347103613\n",
      "Epoch 38 of 500, Train Loss: 0.006316\n",
      "best loss:  0.006315614740037568\n",
      "Epoch 39 of 500, Train Loss: 0.006255\n",
      "best loss:  0.006255463893749109\n",
      "Epoch 40 of 500, Train Loss: 0.006218\n",
      "best loss:  0.006218150004477406\n",
      "Epoch 41 of 500, Train Loss: 0.006202\n",
      "best loss:  0.006201548022538417\n",
      "Epoch 42 of 500, Train Loss: 0.006171\n",
      "best loss:  0.006170571398043233\n",
      "Epoch 43 of 500, Train Loss: 0.006156\n",
      "best loss:  0.006155939294542403\n",
      "Epoch 44 of 500, Train Loss: 0.006114\n",
      "best loss:  0.006114397945975372\n",
      "Epoch 45 of 500, Train Loss: 0.006077\n",
      "best loss:  0.006076895150508508\n",
      "Epoch 46 of 500, Train Loss: 0.006033\n",
      "best loss:  0.006033183754689797\n",
      "Epoch 47 of 500, Train Loss: 0.005973\n",
      "best loss:  0.005973124195611313\n",
      "Epoch 48 of 500, Train Loss: 0.005914\n",
      "best loss:  0.005914018301711988\n",
      "Epoch 49 of 500, Train Loss: 0.005853\n",
      "best loss:  0.005852973399965396\n",
      "Epoch 50 of 500, Train Loss: 0.005813\n",
      "best loss:  0.00581281712525091\n",
      "Epoch 51 of 500, Train Loss: 0.005786\n",
      "best loss:  0.005785849803773047\n",
      "Epoch 52 of 500, Train Loss: 0.005784\n",
      "best loss:  0.005783976241641646\n",
      "Epoch 53 of 500, Train Loss: 0.005773\n",
      "best loss:  0.0057727311363997835\n",
      "Epoch 54 of 500, Train Loss: 0.005769\n",
      "best loss:  0.0057690436756098465\n",
      "Epoch 55 of 500, Train Loss: 0.005726\n",
      "best loss:  0.0057255854660260685\n",
      "Epoch 56 of 500, Train Loss: 0.005701\n",
      "best loss:  0.005700891686487463\n",
      "Epoch 57 of 500, Train Loss: 0.005650\n",
      "best loss:  0.005649546364738783\n",
      "Epoch 58 of 500, Train Loss: 0.005610\n",
      "best loss:  0.005609936737683295\n",
      "Epoch 59 of 500, Train Loss: 0.005555\n",
      "best loss:  0.005555105740574455\n",
      "Epoch 60 of 500, Train Loss: 0.005535\n",
      "best loss:  0.005535433040444501\n",
      "Epoch 61 of 500, Train Loss: 0.005490\n",
      "best loss:  0.00549041988093915\n",
      "Epoch 62 of 500, Train Loss: 0.005490\n",
      "best loss:  0.00548993152018696\n",
      "Epoch 63 of 500, Train Loss: 0.005452\n",
      "best loss:  0.0054524488721851465\n",
      "Epoch 64 of 500, Train Loss: 0.005464\n",
      "Epoch 65 of 500, Train Loss: 0.005434\n",
      "best loss:  0.005434102240147596\n",
      "Epoch 66 of 500, Train Loss: 0.005446\n",
      "Epoch 67 of 500, Train Loss: 0.005415\n",
      "best loss:  0.005414863488902681\n",
      "Epoch 68 of 500, Train Loss: 0.005442\n",
      "Epoch 69 of 500, Train Loss: 0.005414\n",
      "best loss:  0.005413784938961423\n",
      "Epoch 70 of 500, Train Loss: 0.005452\n",
      "Epoch 71 of 500, Train Loss: 0.005440\n",
      "Epoch 72 of 500, Train Loss: 0.005469\n",
      "Epoch 73 of 500, Train Loss: 0.005456\n",
      "Epoch 74 of 500, Train Loss: 0.005486\n",
      "Epoch 75 of 500, Train Loss: 0.005453\n",
      "Epoch 76 of 500, Train Loss: 0.005483\n",
      "Epoch 77 of 500, Train Loss: 0.005432\n",
      "Epoch 78 of 500, Train Loss: 0.005466\n",
      "Epoch 79 of 500, Train Loss: 0.005400\n",
      "best loss:  0.0054004961798349285\n",
      "Epoch 80 of 500, Train Loss: 0.005455\n",
      "Epoch 81 of 500, Train Loss: 0.005410\n",
      "Epoch 82 of 500, Train Loss: 0.005515\n",
      "Epoch 83 of 500, Train Loss: 0.005507\n",
      "Epoch 84 of 500, Train Loss: 0.005682\n",
      "Epoch 85 of 500, Train Loss: 0.005688\n",
      "Epoch 86 of 500, Train Loss: 0.005872\n",
      "Epoch 87 of 500, Train Loss: 0.005912\n",
      "Epoch 88 of 500, Train Loss: 0.005947\n",
      "Epoch 89 of 500, Train Loss: 0.006001\n",
      "Epoch 90 of 500, Train Loss: 0.005953\n",
      "Epoch 91 of 500, Train Loss: 0.005901\n",
      "Epoch 92 of 500, Train Loss: 0.005832\n",
      "Epoch 93 of 500, Train Loss: 0.005859\n",
      "Epoch 94 of 500, Train Loss: 0.005818\n",
      "Epoch 95 of 500, Train Loss: 0.005752\n",
      "Epoch 96 of 500, Train Loss: 0.005687\n",
      "Epoch 97 of 500, Train Loss: 0.005694\n",
      "Epoch 98 of 500, Train Loss: 0.005759\n",
      "Epoch 99 of 500, Train Loss: 0.005806\n",
      "Epoch 100 of 500, Train Loss: 0.005882\n",
      "Epoch 101 of 500, Train Loss: 0.005737\n",
      "Epoch 102 of 500, Train Loss: 0.005722\n",
      "Epoch 103 of 500, Train Loss: 0.005603\n",
      "Epoch 104 of 500, Train Loss: 0.005548\n",
      "Epoch 105 of 500, Train Loss: 0.005535\n",
      "Epoch 106 of 500, Train Loss: 0.005545\n",
      "Epoch 107 of 500, Train Loss: 0.005568\n",
      "Epoch 108 of 500, Train Loss: 0.005581\n",
      "Epoch 109 of 500, Train Loss: 0.005623\n",
      "Epoch 110 of 500, Train Loss: 0.005648\n",
      "Epoch 111 of 500, Train Loss: 0.005711\n",
      "Epoch 112 of 500, Train Loss: 0.005731\n",
      "Epoch 113 of 500, Train Loss: 0.005696\n",
      "Epoch 114 of 500, Train Loss: 0.005625\n",
      "Epoch 115 of 500, Train Loss: 0.005614\n",
      "Epoch 116 of 500, Train Loss: 0.005561\n",
      "Epoch 117 of 500, Train Loss: 0.005589\n",
      "Epoch 118 of 500, Train Loss: 0.005588\n",
      "Epoch 119 of 500, Train Loss: 0.005623\n",
      "Epoch 120 of 500, Train Loss: 0.005677\n",
      "Epoch 121 of 500, Train Loss: 0.005632\n",
      "Epoch 122 of 500, Train Loss: 0.005647\n",
      "Epoch 123 of 500, Train Loss: 0.005586\n",
      "Epoch 124 of 500, Train Loss: 0.005584\n",
      "Epoch 125 of 500, Train Loss: 0.005602\n",
      "Epoch 126 of 500, Train Loss: 0.005579\n",
      "Epoch 127 of 500, Train Loss: 0.005634\n",
      "Epoch 128 of 500, Train Loss: 0.005637\n",
      "Epoch 129 of 500, Train Loss: 0.005643\n",
      "Epoch 130 of 500, Train Loss: 0.005691\n",
      "Epoch 131 of 500, Train Loss: 0.005683\n",
      "Epoch 132 of 500, Train Loss: 0.005724\n",
      "Epoch 133 of 500, Train Loss: 0.005745\n",
      "Epoch 134 of 500, Train Loss: 0.005794\n",
      "Epoch 135 of 500, Train Loss: 0.005747\n",
      "Epoch 136 of 500, Train Loss: 0.005700\n",
      "Epoch 137 of 500, Train Loss: 0.005599\n",
      "Epoch 138 of 500, Train Loss: 0.005582\n",
      "Epoch 139 of 500, Train Loss: 0.005537\n",
      "Epoch 140 of 500, Train Loss: 0.005534\n",
      "Epoch 141 of 500, Train Loss: 0.005482\n",
      "Epoch 142 of 500, Train Loss: 0.005518\n",
      "Epoch 143 of 500, Train Loss: 0.005512\n",
      "Epoch 144 of 500, Train Loss: 0.005615\n",
      "Epoch 145 of 500, Train Loss: 0.005666\n",
      "Epoch 146 of 500, Train Loss: 0.005718\n",
      "Epoch 147 of 500, Train Loss: 0.005682\n",
      "Epoch 148 of 500, Train Loss: 0.005735\n",
      "Epoch 149 of 500, Train Loss: 0.005711\n",
      "Epoch 150 of 500, Train Loss: 0.005699\n",
      "Epoch 151 of 500, Train Loss: 0.005708\n",
      "Epoch 152 of 500, Train Loss: 0.005744\n",
      "Epoch 153 of 500, Train Loss: 0.005721\n",
      "Epoch 154 of 500, Train Loss: 0.005724\n",
      "Epoch 155 of 500, Train Loss: 0.005656\n",
      "Epoch 156 of 500, Train Loss: 0.005666\n",
      "Epoch 157 of 500, Train Loss: 0.005631\n",
      "Epoch 158 of 500, Train Loss: 0.005664\n",
      "Epoch 159 of 500, Train Loss: 0.005600\n",
      "Epoch 160 of 500, Train Loss: 0.005641\n",
      "Epoch 161 of 500, Train Loss: 0.005619\n",
      "Epoch 162 of 500, Train Loss: 0.005657\n",
      "Epoch 163 of 500, Train Loss: 0.005634\n",
      "Epoch 164 of 500, Train Loss: 0.005667\n",
      "Epoch 165 of 500, Train Loss: 0.005635\n",
      "Epoch 166 of 500, Train Loss: 0.005649\n",
      "Epoch 167 of 500, Train Loss: 0.005623\n",
      "Epoch 168 of 500, Train Loss: 0.005673\n",
      "Epoch 169 of 500, Train Loss: 0.005639\n",
      "Epoch 170 of 500, Train Loss: 0.005659\n",
      "Epoch 171 of 500, Train Loss: 0.005606\n",
      "Epoch 172 of 500, Train Loss: 0.005638\n",
      "Epoch 173 of 500, Train Loss: 0.005604\n",
      "Epoch 174 of 500, Train Loss: 0.005654\n",
      "Epoch 175 of 500, Train Loss: 0.005653\n",
      "Epoch 176 of 500, Train Loss: 0.005698\n",
      "Epoch 177 of 500, Train Loss: 0.005695\n",
      "Epoch 178 of 500, Train Loss: 0.005730\n",
      "Epoch 179 of 500, Train Loss: 0.005679\n",
      "Epoch 180 of 500, Train Loss: 0.005701\n",
      "Epoch 181 of 500, Train Loss: 0.005696\n",
      "Epoch 182 of 500, Train Loss: 0.005691\n",
      "Epoch 183 of 500, Train Loss: 0.005635\n",
      "Epoch 184 of 500, Train Loss: 0.005647\n",
      "Epoch 185 of 500, Train Loss: 0.005640\n",
      "Epoch 186 of 500, Train Loss: 0.005671\n",
      "Epoch 187 of 500, Train Loss: 0.005658\n",
      "Epoch 188 of 500, Train Loss: 0.005658\n",
      "Epoch 189 of 500, Train Loss: 0.005640\n",
      "Epoch 190 of 500, Train Loss: 0.005647\n",
      "Epoch 191 of 500, Train Loss: 0.005665\n",
      "Epoch 192 of 500, Train Loss: 0.005694\n",
      "Epoch 193 of 500, Train Loss: 0.005684\n",
      "Epoch 194 of 500, Train Loss: 0.005730\n",
      "Epoch 195 of 500, Train Loss: 0.005715\n",
      "Epoch 196 of 500, Train Loss: 0.005794\n",
      "Epoch 197 of 500, Train Loss: 0.005774\n",
      "Epoch 198 of 500, Train Loss: 0.005800\n",
      "Epoch 199 of 500, Train Loss: 0.005813\n",
      "Epoch 200 of 500, Train Loss: 0.005856\n",
      "Epoch 201 of 500, Train Loss: 0.005876\n",
      "Epoch 202 of 500, Train Loss: 0.005882\n",
      "Epoch 203 of 500, Train Loss: 0.005784\n",
      "Epoch 204 of 500, Train Loss: 0.005757\n",
      "Epoch 205 of 500, Train Loss: 0.005733\n",
      "Epoch 206 of 500, Train Loss: 0.005750\n",
      "Epoch 207 of 500, Train Loss: 0.005719\n",
      "Epoch 208 of 500, Train Loss: 0.005773\n",
      "Epoch 209 of 500, Train Loss: 0.005780\n",
      "Epoch 210 of 500, Train Loss: 0.005842\n",
      "Epoch 211 of 500, Train Loss: 0.005872\n",
      "Epoch 212 of 500, Train Loss: 0.005896\n",
      "Epoch 213 of 500, Train Loss: 0.005852\n",
      "Epoch 214 of 500, Train Loss: 0.005814\n",
      "Epoch 215 of 500, Train Loss: 0.005856\n",
      "Epoch 216 of 500, Train Loss: 0.005839\n",
      "Epoch 217 of 500, Train Loss: 0.005864\n",
      "Epoch 218 of 500, Train Loss: 0.005842\n",
      "Epoch 219 of 500, Train Loss: 0.005747\n",
      "Epoch 220 of 500, Train Loss: 0.005682\n",
      "Epoch 221 of 500, Train Loss: 0.005693\n",
      "Epoch 222 of 500, Train Loss: 0.005721\n",
      "Epoch 223 of 500, Train Loss: 0.005801\n",
      "Epoch 224 of 500, Train Loss: 0.005829\n",
      "Epoch 225 of 500, Train Loss: 0.005778\n",
      "Epoch 226 of 500, Train Loss: 0.005787\n",
      "Epoch 227 of 500, Train Loss: 0.005705\n",
      "Epoch 228 of 500, Train Loss: 0.005648\n",
      "Epoch 229 of 500, Train Loss: 0.005648\n",
      "Epoch 230 of 500, Train Loss: 0.005714\n",
      "Epoch 231 of 500, Train Loss: 0.005758\n",
      "Epoch 232 of 500, Train Loss: 0.005798\n",
      "Epoch 233 of 500, Train Loss: 0.005760\n",
      "Epoch 234 of 500, Train Loss: 0.005765\n",
      "Epoch 235 of 500, Train Loss: 0.005707\n",
      "Epoch 236 of 500, Train Loss: 0.005675\n",
      "Epoch 237 of 500, Train Loss: 0.005657\n",
      "Epoch 238 of 500, Train Loss: 0.005675\n",
      "Epoch 239 of 500, Train Loss: 0.005656\n",
      "Epoch 240 of 500, Train Loss: 0.005732\n",
      "Epoch 241 of 500, Train Loss: 0.005716\n",
      "Epoch 242 of 500, Train Loss: 0.005754\n",
      "Epoch 243 of 500, Train Loss: 0.005715\n",
      "Epoch 244 of 500, Train Loss: 0.005747\n",
      "Epoch 245 of 500, Train Loss: 0.005704\n",
      "Epoch 246 of 500, Train Loss: 0.005732\n",
      "Epoch 247 of 500, Train Loss: 0.005730\n",
      "Epoch 248 of 500, Train Loss: 0.005743\n",
      "Epoch 249 of 500, Train Loss: 0.005713\n",
      "Epoch 250 of 500, Train Loss: 0.005684\n",
      "Epoch 251 of 500, Train Loss: 0.005673\n",
      "Epoch 252 of 500, Train Loss: 0.005682\n",
      "Epoch 253 of 500, Train Loss: 0.005672\n",
      "Epoch 254 of 500, Train Loss: 0.005646\n",
      "Epoch 255 of 500, Train Loss: 0.005638\n",
      "Epoch 256 of 500, Train Loss: 0.005632\n",
      "Epoch 257 of 500, Train Loss: 0.005649\n",
      "Epoch 258 of 500, Train Loss: 0.005647\n",
      "Epoch 259 of 500, Train Loss: 0.005663\n",
      "Epoch 260 of 500, Train Loss: 0.005668\n",
      "Epoch 261 of 500, Train Loss: 0.005643\n",
      "Epoch 262 of 500, Train Loss: 0.005661\n",
      "Epoch 263 of 500, Train Loss: 0.005647\n",
      "Epoch 264 of 500, Train Loss: 0.005666\n",
      "Epoch 265 of 500, Train Loss: 0.005644\n",
      "Epoch 266 of 500, Train Loss: 0.005632\n",
      "Epoch 267 of 500, Train Loss: 0.005614\n",
      "Epoch 268 of 500, Train Loss: 0.005633\n",
      "Epoch 269 of 500, Train Loss: 0.005603\n",
      "Epoch 270 of 500, Train Loss: 0.005589\n",
      "Epoch 271 of 500, Train Loss: 0.005597\n",
      "Epoch 272 of 500, Train Loss: 0.005613\n",
      "Epoch 273 of 500, Train Loss: 0.005586\n",
      "Epoch 274 of 500, Train Loss: 0.005593\n",
      "Epoch 275 of 500, Train Loss: 0.005590\n",
      "Epoch 276 of 500, Train Loss: 0.005616\n",
      "Epoch 277 of 500, Train Loss: 0.005612\n",
      "Epoch 278 of 500, Train Loss: 0.005610\n",
      "Epoch 279 of 500, Train Loss: 0.005582\n",
      "Epoch 280 of 500, Train Loss: 0.005594\n",
      "Epoch 281 of 500, Train Loss: 0.005591\n",
      "Epoch 282 of 500, Train Loss: 0.005605\n",
      "Epoch 283 of 500, Train Loss: 0.005593\n",
      "Epoch 284 of 500, Train Loss: 0.005604\n",
      "Epoch 285 of 500, Train Loss: 0.005539\n",
      "Epoch 286 of 500, Train Loss: 0.005563\n",
      "Epoch 287 of 500, Train Loss: 0.005570\n",
      "Epoch 288 of 500, Train Loss: 0.005588\n",
      "Epoch 289 of 500, Train Loss: 0.005558\n",
      "Epoch 290 of 500, Train Loss: 0.005565\n",
      "Epoch 291 of 500, Train Loss: 0.005546\n",
      "Epoch 292 of 500, Train Loss: 0.005561\n",
      "Epoch 293 of 500, Train Loss: 0.005542\n",
      "Epoch 294 of 500, Train Loss: 0.005550\n",
      "Epoch 295 of 500, Train Loss: 0.005565\n",
      "Epoch 296 of 500, Train Loss: 0.005563\n",
      "Epoch 297 of 500, Train Loss: 0.005522\n",
      "Epoch 298 of 500, Train Loss: 0.005537\n",
      "Epoch 299 of 500, Train Loss: 0.005522\n",
      "Epoch 300 of 500, Train Loss: 0.005549\n",
      "Epoch 301 of 500, Train Loss: 0.005506\n",
      "Epoch 302 of 500, Train Loss: 0.005517\n",
      "Epoch 303 of 500, Train Loss: 0.005512\n",
      "Epoch 304 of 500, Train Loss: 0.005540\n",
      "Epoch 305 of 500, Train Loss: 0.005520\n",
      "Epoch 306 of 500, Train Loss: 0.005500\n",
      "Epoch 307 of 500, Train Loss: 0.005506\n",
      "Epoch 308 of 500, Train Loss: 0.005569\n",
      "Epoch 309 of 500, Train Loss: 0.005550\n",
      "Epoch 310 of 500, Train Loss: 0.005555\n",
      "Epoch 311 of 500, Train Loss: 0.005529\n",
      "Epoch 312 of 500, Train Loss: 0.005559\n",
      "Epoch 313 of 500, Train Loss: 0.005509\n",
      "Epoch 314 of 500, Train Loss: 0.005510\n",
      "Epoch 315 of 500, Train Loss: 0.005501\n",
      "Epoch 316 of 500, Train Loss: 0.005513\n",
      "Epoch 317 of 500, Train Loss: 0.005516\n",
      "Epoch 318 of 500, Train Loss: 0.005547\n",
      "Epoch 319 of 500, Train Loss: 0.005528\n",
      "Epoch 320 of 500, Train Loss: 0.005538\n",
      "Epoch 321 of 500, Train Loss: 0.005524\n",
      "Epoch 322 of 500, Train Loss: 0.005580\n",
      "Epoch 323 of 500, Train Loss: 0.005574\n",
      "Epoch 324 of 500, Train Loss: 0.005598\n",
      "Epoch 325 of 500, Train Loss: 0.005553\n",
      "Epoch 326 of 500, Train Loss: 0.005566\n",
      "Epoch 327 of 500, Train Loss: 0.005559\n",
      "Epoch 328 of 500, Train Loss: 0.005588\n",
      "Epoch 329 of 500, Train Loss: 0.005547\n",
      "Epoch 330 of 500, Train Loss: 0.005562\n",
      "Epoch 331 of 500, Train Loss: 0.005545\n",
      "Epoch 332 of 500, Train Loss: 0.005569\n",
      "Epoch 333 of 500, Train Loss: 0.005597\n",
      "Epoch 334 of 500, Train Loss: 0.005586\n",
      "Epoch 335 of 500, Train Loss: 0.005566\n",
      "Epoch 336 of 500, Train Loss: 0.005549\n",
      "Epoch 337 of 500, Train Loss: 0.005552\n",
      "Epoch 338 of 500, Train Loss: 0.005567\n",
      "Epoch 339 of 500, Train Loss: 0.005568\n",
      "Epoch 340 of 500, Train Loss: 0.005571\n",
      "Epoch 341 of 500, Train Loss: 0.005565\n",
      "Epoch 342 of 500, Train Loss: 0.005627\n",
      "Epoch 343 of 500, Train Loss: 0.005627\n",
      "Epoch 344 of 500, Train Loss: 0.005611\n",
      "Epoch 345 of 500, Train Loss: 0.005548\n",
      "Epoch 346 of 500, Train Loss: 0.005547\n",
      "Epoch 347 of 500, Train Loss: 0.005554\n",
      "Epoch 348 of 500, Train Loss: 0.005577\n",
      "Epoch 349 of 500, Train Loss: 0.005570\n",
      "Epoch 350 of 500, Train Loss: 0.005585\n",
      "Epoch 351 of 500, Train Loss: 0.005575\n",
      "Epoch 352 of 500, Train Loss: 0.005570\n",
      "Epoch 353 of 500, Train Loss: 0.005558\n",
      "Epoch 354 of 500, Train Loss: 0.005572\n",
      "Epoch 355 of 500, Train Loss: 0.005519\n",
      "Epoch 356 of 500, Train Loss: 0.005539\n",
      "Epoch 357 of 500, Train Loss: 0.005548\n",
      "Epoch 358 of 500, Train Loss: 0.005556\n",
      "Epoch 359 of 500, Train Loss: 0.005529\n",
      "Epoch 360 of 500, Train Loss: 0.005508\n",
      "Epoch 361 of 500, Train Loss: 0.005508\n",
      "Epoch 362 of 500, Train Loss: 0.005555\n",
      "Epoch 363 of 500, Train Loss: 0.005556\n",
      "Epoch 364 of 500, Train Loss: 0.005582\n",
      "Epoch 365 of 500, Train Loss: 0.005554\n",
      "Epoch 366 of 500, Train Loss: 0.005582\n",
      "Epoch 367 of 500, Train Loss: 0.005580\n",
      "Epoch 368 of 500, Train Loss: 0.005584\n",
      "Epoch 369 of 500, Train Loss: 0.005580\n",
      "Epoch 370 of 500, Train Loss: 0.005590\n",
      "Epoch 371 of 500, Train Loss: 0.005562\n",
      "Epoch 372 of 500, Train Loss: 0.005553\n",
      "Epoch 373 of 500, Train Loss: 0.005542\n",
      "Epoch 374 of 500, Train Loss: 0.005581\n",
      "Epoch 375 of 500, Train Loss: 0.005566\n",
      "Epoch 376 of 500, Train Loss: 0.005581\n",
      "Epoch 377 of 500, Train Loss: 0.005604\n",
      "Epoch 378 of 500, Train Loss: 0.005659\n",
      "Epoch 379 of 500, Train Loss: 0.005697\n",
      "Epoch 380 of 500, Train Loss: 0.005657\n",
      "Epoch 381 of 500, Train Loss: 0.005653\n",
      "Epoch 382 of 500, Train Loss: 0.005686\n",
      "Epoch 383 of 500, Train Loss: 0.005737\n",
      "Epoch 384 of 500, Train Loss: 0.005800\n",
      "Epoch 385 of 500, Train Loss: 0.005821\n",
      "Epoch 386 of 500, Train Loss: 0.005861\n",
      "Epoch 387 of 500, Train Loss: 0.005811\n",
      "Epoch 388 of 500, Train Loss: 0.005837\n",
      "Epoch 389 of 500, Train Loss: 0.005704\n",
      "Epoch 390 of 500, Train Loss: 0.005725\n",
      "Epoch 391 of 500, Train Loss: 0.005694\n",
      "Epoch 392 of 500, Train Loss: 0.005718\n",
      "Epoch 393 of 500, Train Loss: 0.005650\n",
      "Epoch 394 of 500, Train Loss: 0.005679\n",
      "Epoch 395 of 500, Train Loss: 0.005653\n",
      "Epoch 396 of 500, Train Loss: 0.005676\n",
      "Epoch 397 of 500, Train Loss: 0.005641\n",
      "Epoch 398 of 500, Train Loss: 0.005709\n",
      "Epoch 399 of 500, Train Loss: 0.005695\n",
      "Epoch 400 of 500, Train Loss: 0.005714\n",
      "Epoch 401 of 500, Train Loss: 0.005702\n",
      "Epoch 402 of 500, Train Loss: 0.005729\n",
      "Epoch 403 of 500, Train Loss: 0.005655\n",
      "Epoch 404 of 500, Train Loss: 0.005664\n",
      "Epoch 405 of 500, Train Loss: 0.005635\n",
      "Epoch 406 of 500, Train Loss: 0.005672\n",
      "Epoch 407 of 500, Train Loss: 0.005634\n",
      "Epoch 408 of 500, Train Loss: 0.005665\n",
      "Epoch 409 of 500, Train Loss: 0.005604\n",
      "Epoch 410 of 500, Train Loss: 0.005640\n",
      "Epoch 411 of 500, Train Loss: 0.005628\n",
      "Epoch 412 of 500, Train Loss: 0.005659\n",
      "Epoch 413 of 500, Train Loss: 0.005619\n",
      "Epoch 414 of 500, Train Loss: 0.005647\n",
      "Epoch 415 of 500, Train Loss: 0.005559\n",
      "Epoch 416 of 500, Train Loss: 0.005615\n",
      "Epoch 417 of 500, Train Loss: 0.005607\n",
      "Epoch 418 of 500, Train Loss: 0.005667\n",
      "Epoch 419 of 500, Train Loss: 0.005641\n",
      "Epoch 420 of 500, Train Loss: 0.005708\n",
      "Epoch 421 of 500, Train Loss: 0.005695\n",
      "Epoch 422 of 500, Train Loss: 0.005735\n",
      "Epoch 423 of 500, Train Loss: 0.005684\n",
      "Epoch 424 of 500, Train Loss: 0.005709\n",
      "Epoch 425 of 500, Train Loss: 0.005650\n",
      "Epoch 426 of 500, Train Loss: 0.005723\n",
      "Epoch 427 of 500, Train Loss: 0.005625\n",
      "Epoch 428 of 500, Train Loss: 0.005680\n",
      "Epoch 429 of 500, Train Loss: 0.005600\n",
      "Epoch 430 of 500, Train Loss: 0.005628\n",
      "Epoch 431 of 500, Train Loss: 0.005570\n",
      "Epoch 432 of 500, Train Loss: 0.005637\n",
      "Epoch 433 of 500, Train Loss: 0.005610\n",
      "Epoch 434 of 500, Train Loss: 0.005675\n",
      "Epoch 435 of 500, Train Loss: 0.005651\n",
      "Epoch 436 of 500, Train Loss: 0.005701\n",
      "Epoch 437 of 500, Train Loss: 0.005655\n",
      "Epoch 438 of 500, Train Loss: 0.005738\n",
      "Epoch 439 of 500, Train Loss: 0.005660\n",
      "Epoch 440 of 500, Train Loss: 0.005744\n",
      "Epoch 441 of 500, Train Loss: 0.005697\n",
      "Epoch 442 of 500, Train Loss: 0.005791\n",
      "Epoch 443 of 500, Train Loss: 0.005671\n",
      "Epoch 444 of 500, Train Loss: 0.005721\n",
      "Epoch 445 of 500, Train Loss: 0.005668\n",
      "Epoch 446 of 500, Train Loss: 0.005770\n",
      "Epoch 447 of 500, Train Loss: 0.005641\n",
      "Epoch 448 of 500, Train Loss: 0.005718\n",
      "Epoch 449 of 500, Train Loss: 0.005570\n",
      "Epoch 450 of 500, Train Loss: 0.005621\n",
      "Epoch 451 of 500, Train Loss: 0.005525\n",
      "Epoch 452 of 500, Train Loss: 0.005588\n",
      "Epoch 453 of 500, Train Loss: 0.005585\n",
      "Epoch 454 of 500, Train Loss: 0.005701\n",
      "Epoch 455 of 500, Train Loss: 0.005692\n",
      "Epoch 456 of 500, Train Loss: 0.005744\n",
      "Epoch 457 of 500, Train Loss: 0.005693\n",
      "Epoch 458 of 500, Train Loss: 0.005759\n",
      "Epoch 459 of 500, Train Loss: 0.005619\n",
      "Epoch 460 of 500, Train Loss: 0.005620\n",
      "Epoch 461 of 500, Train Loss: 0.005582\n",
      "Epoch 462 of 500, Train Loss: 0.005648\n",
      "Epoch 463 of 500, Train Loss: 0.005653\n",
      "Epoch 464 of 500, Train Loss: 0.005768\n",
      "Epoch 465 of 500, Train Loss: 0.005760\n",
      "Epoch 466 of 500, Train Loss: 0.005827\n",
      "Epoch 467 of 500, Train Loss: 0.005608\n",
      "Epoch 468 of 500, Train Loss: 0.005624\n",
      "Epoch 469 of 500, Train Loss: 0.005574\n",
      "Epoch 470 of 500, Train Loss: 0.005634\n",
      "Epoch 471 of 500, Train Loss: 0.005617\n",
      "Epoch 472 of 500, Train Loss: 0.005671\n",
      "Epoch 473 of 500, Train Loss: 0.005703\n",
      "Epoch 474 of 500, Train Loss: 0.005777\n",
      "Epoch 475 of 500, Train Loss: 0.005721\n",
      "Epoch 476 of 500, Train Loss: 0.005773\n",
      "Epoch 477 of 500, Train Loss: 0.005677\n",
      "Epoch 478 of 500, Train Loss: 0.005683\n",
      "Epoch 479 of 500, Train Loss: 0.005639\n",
      "Epoch 480 of 500, Train Loss: 0.005675\n",
      "Epoch 481 of 500, Train Loss: 0.005717\n",
      "Epoch 482 of 500, Train Loss: 0.005782\n",
      "Epoch 483 of 500, Train Loss: 0.005795\n",
      "Epoch 484 of 500, Train Loss: 0.005850\n",
      "Epoch 485 of 500, Train Loss: 0.005672\n",
      "Epoch 486 of 500, Train Loss: 0.005666\n",
      "Epoch 487 of 500, Train Loss: 0.005583\n",
      "Epoch 488 of 500, Train Loss: 0.005597\n",
      "Epoch 489 of 500, Train Loss: 0.005613\n",
      "Epoch 490 of 500, Train Loss: 0.005635\n",
      "Epoch 491 of 500, Train Loss: 0.005664\n",
      "Epoch 492 of 500, Train Loss: 0.005692\n",
      "Epoch 493 of 500, Train Loss: 0.005718\n",
      "Epoch 494 of 500, Train Loss: 0.005747\n",
      "Epoch 495 of 500, Train Loss: 0.005634\n",
      "Epoch 496 of 500, Train Loss: 0.005614\n",
      "Epoch 497 of 500, Train Loss: 0.005584\n",
      "Epoch 498 of 500, Train Loss: 0.005617\n",
      "Epoch 499 of 500, Train Loss: 0.005604\n",
      "Epoch 500 of 500, Train Loss: 0.005675\n",
      "latent train shape:  (16395, 180)\n",
      "M: 180, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 34\n",
      "Training the subspace: 0 / 180\n",
      "Training the subspace: 1 / 180\n",
      "Training the subspace: 2 / 180\n",
      "Training the subspace: 3 / 180\n",
      "Training the subspace: 4 / 180\n",
      "Training the subspace: 5 / 180\n",
      "Training the subspace: 6 / 180\n",
      "Training the subspace: 7 / 180\n",
      "Training the subspace: 8 / 180\n",
      "Training the subspace: 9 / 180\n",
      "Training the subspace: 10 / 180\n",
      "Training the subspace: 11 / 180\n",
      "Training the subspace: 12 / 180\n",
      "Training the subspace: 13 / 180\n",
      "Training the subspace: 14 / 180\n",
      "Training the subspace: 15 / 180\n",
      "Training the subspace: 16 / 180\n",
      "Training the subspace: 17 / 180\n",
      "Training the subspace: 18 / 180\n",
      "Training the subspace: 19 / 180\n",
      "Training the subspace: 20 / 180\n",
      "Training the subspace: 21 / 180\n",
      "Training the subspace: 22 / 180\n",
      "Training the subspace: 23 / 180\n",
      "Training the subspace: 24 / 180\n",
      "Training the subspace: 25 / 180\n",
      "Training the subspace: 26 / 180\n",
      "Training the subspace: 27 / 180\n",
      "Training the subspace: 28 / 180\n",
      "Training the subspace: 29 / 180\n",
      "Training the subspace: 30 / 180\n",
      "Training the subspace: 31 / 180\n",
      "Training the subspace: 32 / 180\n",
      "Training the subspace: 33 / 180\n",
      "Training the subspace: 34 / 180\n",
      "Training the subspace: 35 / 180\n",
      "Training the subspace: 36 / 180\n",
      "Training the subspace: 37 / 180\n",
      "Training the subspace: 38 / 180\n",
      "Training the subspace: 39 / 180\n",
      "Training the subspace: 40 / 180\n",
      "Training the subspace: 41 / 180\n",
      "Training the subspace: 42 / 180\n",
      "Training the subspace: 43 / 180\n",
      "Training the subspace: 44 / 180\n",
      "Training the subspace: 45 / 180\n",
      "Training the subspace: 46 / 180\n",
      "Training the subspace: 47 / 180\n",
      "Training the subspace: 48 / 180\n",
      "Training the subspace: 49 / 180\n",
      "Training the subspace: 50 / 180\n",
      "Training the subspace: 51 / 180\n",
      "Training the subspace: 52 / 180\n",
      "Training the subspace: 53 / 180\n",
      "Training the subspace: 54 / 180\n",
      "Training the subspace: 55 / 180\n",
      "Training the subspace: 56 / 180\n",
      "Training the subspace: 57 / 180\n",
      "Training the subspace: 58 / 180\n",
      "Training the subspace: 59 / 180\n",
      "Training the subspace: 60 / 180\n",
      "Training the subspace: 61 / 180\n",
      "Training the subspace: 62 / 180\n",
      "Training the subspace: 63 / 180\n",
      "Training the subspace: 64 / 180\n",
      "Training the subspace: 65 / 180\n",
      "Training the subspace: 66 / 180\n",
      "Training the subspace: 67 / 180\n",
      "Training the subspace: 68 / 180\n",
      "Training the subspace: 69 / 180\n",
      "Training the subspace: 70 / 180\n",
      "Training the subspace: 71 / 180\n",
      "Training the subspace: 72 / 180\n",
      "Training the subspace: 73 / 180\n",
      "Training the subspace: 74 / 180\n",
      "Training the subspace: 75 / 180\n",
      "Training the subspace: 76 / 180\n",
      "Training the subspace: 77 / 180\n",
      "Training the subspace: 78 / 180\n",
      "Training the subspace: 79 / 180\n",
      "Training the subspace: 80 / 180\n",
      "Training the subspace: 81 / 180\n",
      "Training the subspace: 82 / 180\n",
      "Training the subspace: 83 / 180\n",
      "Training the subspace: 84 / 180\n",
      "Training the subspace: 85 / 180\n",
      "Training the subspace: 86 / 180\n",
      "Training the subspace: 87 / 180\n",
      "Training the subspace: 88 / 180\n",
      "Training the subspace: 89 / 180\n",
      "Training the subspace: 90 / 180\n",
      "Training the subspace: 91 / 180\n",
      "Training the subspace: 92 / 180\n",
      "Training the subspace: 93 / 180\n",
      "Training the subspace: 94 / 180\n",
      "Training the subspace: 95 / 180\n",
      "Training the subspace: 96 / 180\n",
      "Training the subspace: 97 / 180\n",
      "Training the subspace: 98 / 180\n",
      "Training the subspace: 99 / 180\n",
      "Training the subspace: 100 / 180\n",
      "Training the subspace: 101 / 180\n",
      "Training the subspace: 102 / 180\n",
      "Training the subspace: 103 / 180\n",
      "Training the subspace: 104 / 180\n",
      "Training the subspace: 105 / 180\n",
      "Training the subspace: 106 / 180\n",
      "Training the subspace: 107 / 180\n",
      "Training the subspace: 108 / 180\n",
      "Training the subspace: 109 / 180\n",
      "Training the subspace: 110 / 180\n",
      "Training the subspace: 111 / 180\n",
      "Training the subspace: 112 / 180\n",
      "Training the subspace: 113 / 180\n",
      "Training the subspace: 114 / 180\n",
      "Training the subspace: 115 / 180\n",
      "Training the subspace: 116 / 180\n",
      "Training the subspace: 117 / 180\n",
      "Training the subspace: 118 / 180\n",
      "Training the subspace: 119 / 180\n",
      "Training the subspace: 120 / 180\n",
      "Training the subspace: 121 / 180\n",
      "Training the subspace: 122 / 180\n",
      "Training the subspace: 123 / 180\n",
      "Training the subspace: 124 / 180\n",
      "Training the subspace: 125 / 180\n",
      "Training the subspace: 126 / 180\n",
      "Training the subspace: 127 / 180\n",
      "Training the subspace: 128 / 180\n",
      "Training the subspace: 129 / 180\n",
      "Training the subspace: 130 / 180\n",
      "Training the subspace: 131 / 180\n",
      "Training the subspace: 132 / 180\n",
      "Training the subspace: 133 / 180\n",
      "Training the subspace: 134 / 180\n",
      "Training the subspace: 135 / 180\n",
      "Training the subspace: 136 / 180\n",
      "Training the subspace: 137 / 180\n",
      "Training the subspace: 138 / 180\n",
      "Training the subspace: 139 / 180\n",
      "Training the subspace: 140 / 180\n",
      "Training the subspace: 141 / 180\n",
      "Training the subspace: 142 / 180\n",
      "Training the subspace: 143 / 180\n",
      "Training the subspace: 144 / 180\n",
      "Training the subspace: 145 / 180\n",
      "Training the subspace: 146 / 180\n",
      "Training the subspace: 147 / 180\n",
      "Training the subspace: 148 / 180\n",
      "Training the subspace: 149 / 180\n",
      "Training the subspace: 150 / 180\n",
      "Training the subspace: 151 / 180\n",
      "Training the subspace: 152 / 180\n",
      "Training the subspace: 153 / 180\n",
      "Training the subspace: 154 / 180\n",
      "Training the subspace: 155 / 180\n",
      "Training the subspace: 156 / 180\n",
      "Training the subspace: 157 / 180\n",
      "Training the subspace: 158 / 180\n",
      "Training the subspace: 159 / 180\n",
      "Training the subspace: 160 / 180\n",
      "Training the subspace: 161 / 180\n",
      "Training the subspace: 162 / 180\n",
      "Training the subspace: 163 / 180\n",
      "Training the subspace: 164 / 180\n",
      "Training the subspace: 165 / 180\n",
      "Training the subspace: 166 / 180\n",
      "Training the subspace: 167 / 180\n",
      "Training the subspace: 168 / 180\n",
      "Training the subspace: 169 / 180\n",
      "Training the subspace: 170 / 180\n",
      "Training the subspace: 171 / 180\n",
      "Training the subspace: 172 / 180\n",
      "Training the subspace: 173 / 180\n",
      "Training the subspace: 174 / 180\n",
      "Training the subspace: 175 / 180\n",
      "Training the subspace: 176 / 180\n",
      "Training the subspace: 177 / 180\n",
      "Training the subspace: 178 / 180\n",
      "Training the subspace: 179 / 180\n",
      "Encoding the subspace: 0 / 180\n",
      "Encoding the subspace: 1 / 180\n",
      "Encoding the subspace: 2 / 180\n",
      "Encoding the subspace: 3 / 180\n",
      "Encoding the subspace: 4 / 180\n",
      "Encoding the subspace: 5 / 180\n",
      "Encoding the subspace: 6 / 180\n",
      "Encoding the subspace: 7 / 180\n",
      "Encoding the subspace: 8 / 180\n",
      "Encoding the subspace: 9 / 180\n",
      "Encoding the subspace: 10 / 180\n",
      "Encoding the subspace: 11 / 180\n",
      "Encoding the subspace: 12 / 180\n",
      "Encoding the subspace: 13 / 180\n",
      "Encoding the subspace: 14 / 180\n",
      "Encoding the subspace: 15 / 180\n",
      "Encoding the subspace: 16 / 180\n",
      "Encoding the subspace: 17 / 180\n",
      "Encoding the subspace: 18 / 180\n",
      "Encoding the subspace: 19 / 180\n",
      "Encoding the subspace: 20 / 180\n",
      "Encoding the subspace: 21 / 180\n",
      "Encoding the subspace: 22 / 180\n",
      "Encoding the subspace: 23 / 180\n",
      "Encoding the subspace: 24 / 180\n",
      "Encoding the subspace: 25 / 180\n",
      "Encoding the subspace: 26 / 180\n",
      "Encoding the subspace: 27 / 180\n",
      "Encoding the subspace: 28 / 180\n",
      "Encoding the subspace: 29 / 180\n",
      "Encoding the subspace: 30 / 180\n",
      "Encoding the subspace: 31 / 180\n",
      "Encoding the subspace: 32 / 180\n",
      "Encoding the subspace: 33 / 180\n",
      "Encoding the subspace: 34 / 180\n",
      "Encoding the subspace: 35 / 180\n",
      "Encoding the subspace: 36 / 180\n",
      "Encoding the subspace: 37 / 180\n",
      "Encoding the subspace: 38 / 180\n",
      "Encoding the subspace: 39 / 180\n",
      "Encoding the subspace: 40 / 180\n",
      "Encoding the subspace: 41 / 180\n",
      "Encoding the subspace: 42 / 180\n",
      "Encoding the subspace: 43 / 180\n",
      "Encoding the subspace: 44 / 180\n",
      "Encoding the subspace: 45 / 180\n",
      "Encoding the subspace: 46 / 180\n",
      "Encoding the subspace: 47 / 180\n",
      "Encoding the subspace: 48 / 180\n",
      "Encoding the subspace: 49 / 180\n",
      "Encoding the subspace: 50 / 180\n",
      "Encoding the subspace: 51 / 180\n",
      "Encoding the subspace: 52 / 180\n",
      "Encoding the subspace: 53 / 180\n",
      "Encoding the subspace: 54 / 180\n",
      "Encoding the subspace: 55 / 180\n",
      "Encoding the subspace: 56 / 180\n",
      "Encoding the subspace: 57 / 180\n",
      "Encoding the subspace: 58 / 180\n",
      "Encoding the subspace: 59 / 180\n",
      "Encoding the subspace: 60 / 180\n",
      "Encoding the subspace: 61 / 180\n",
      "Encoding the subspace: 62 / 180\n",
      "Encoding the subspace: 63 / 180\n",
      "Encoding the subspace: 64 / 180\n",
      "Encoding the subspace: 65 / 180\n",
      "Encoding the subspace: 66 / 180\n",
      "Encoding the subspace: 67 / 180\n",
      "Encoding the subspace: 68 / 180\n",
      "Encoding the subspace: 69 / 180\n",
      "Encoding the subspace: 70 / 180\n",
      "Encoding the subspace: 71 / 180\n",
      "Encoding the subspace: 72 / 180\n",
      "Encoding the subspace: 73 / 180\n",
      "Encoding the subspace: 74 / 180\n",
      "Encoding the subspace: 75 / 180\n",
      "Encoding the subspace: 76 / 180\n",
      "Encoding the subspace: 77 / 180\n",
      "Encoding the subspace: 78 / 180\n",
      "Encoding the subspace: 79 / 180\n",
      "Encoding the subspace: 80 / 180\n",
      "Encoding the subspace: 81 / 180\n",
      "Encoding the subspace: 82 / 180\n",
      "Encoding the subspace: 83 / 180\n",
      "Encoding the subspace: 84 / 180\n",
      "Encoding the subspace: 85 / 180\n",
      "Encoding the subspace: 86 / 180\n",
      "Encoding the subspace: 87 / 180\n",
      "Encoding the subspace: 88 / 180\n",
      "Encoding the subspace: 89 / 180\n",
      "Encoding the subspace: 90 / 180\n",
      "Encoding the subspace: 91 / 180\n",
      "Encoding the subspace: 92 / 180\n",
      "Encoding the subspace: 93 / 180\n",
      "Encoding the subspace: 94 / 180\n",
      "Encoding the subspace: 95 / 180\n",
      "Encoding the subspace: 96 / 180\n",
      "Encoding the subspace: 97 / 180\n",
      "Encoding the subspace: 98 / 180\n",
      "Encoding the subspace: 99 / 180\n",
      "Encoding the subspace: 100 / 180\n",
      "Encoding the subspace: 101 / 180\n",
      "Encoding the subspace: 102 / 180\n",
      "Encoding the subspace: 103 / 180\n",
      "Encoding the subspace: 104 / 180\n",
      "Encoding the subspace: 105 / 180\n",
      "Encoding the subspace: 106 / 180\n",
      "Encoding the subspace: 107 / 180\n",
      "Encoding the subspace: 108 / 180\n",
      "Encoding the subspace: 109 / 180\n",
      "Encoding the subspace: 110 / 180\n",
      "Encoding the subspace: 111 / 180\n",
      "Encoding the subspace: 112 / 180\n",
      "Encoding the subspace: 113 / 180\n",
      "Encoding the subspace: 114 / 180\n",
      "Encoding the subspace: 115 / 180\n",
      "Encoding the subspace: 116 / 180\n",
      "Encoding the subspace: 117 / 180\n",
      "Encoding the subspace: 118 / 180\n",
      "Encoding the subspace: 119 / 180\n",
      "Encoding the subspace: 120 / 180\n",
      "Encoding the subspace: 121 / 180\n",
      "Encoding the subspace: 122 / 180\n",
      "Encoding the subspace: 123 / 180\n",
      "Encoding the subspace: 124 / 180\n",
      "Encoding the subspace: 125 / 180\n",
      "Encoding the subspace: 126 / 180\n",
      "Encoding the subspace: 127 / 180\n",
      "Encoding the subspace: 128 / 180\n",
      "Encoding the subspace: 129 / 180\n",
      "Encoding the subspace: 130 / 180\n",
      "Encoding the subspace: 131 / 180\n",
      "Encoding the subspace: 132 / 180\n",
      "Encoding the subspace: 133 / 180\n",
      "Encoding the subspace: 134 / 180\n",
      "Encoding the subspace: 135 / 180\n",
      "Encoding the subspace: 136 / 180\n",
      "Encoding the subspace: 137 / 180\n",
      "Encoding the subspace: 138 / 180\n",
      "Encoding the subspace: 139 / 180\n",
      "Encoding the subspace: 140 / 180\n",
      "Encoding the subspace: 141 / 180\n",
      "Encoding the subspace: 142 / 180\n",
      "Encoding the subspace: 143 / 180\n",
      "Encoding the subspace: 144 / 180\n",
      "Encoding the subspace: 145 / 180\n",
      "Encoding the subspace: 146 / 180\n",
      "Encoding the subspace: 147 / 180\n",
      "Encoding the subspace: 148 / 180\n",
      "Encoding the subspace: 149 / 180\n",
      "Encoding the subspace: 150 / 180\n",
      "Encoding the subspace: 151 / 180\n",
      "Encoding the subspace: 152 / 180\n",
      "Encoding the subspace: 153 / 180\n",
      "Encoding the subspace: 154 / 180\n",
      "Encoding the subspace: 155 / 180\n",
      "Encoding the subspace: 156 / 180\n",
      "Encoding the subspace: 157 / 180\n",
      "Encoding the subspace: 158 / 180\n",
      "Encoding the subspace: 159 / 180\n",
      "Encoding the subspace: 160 / 180\n",
      "Encoding the subspace: 161 / 180\n",
      "Encoding the subspace: 162 / 180\n",
      "Encoding the subspace: 163 / 180\n",
      "Encoding the subspace: 164 / 180\n",
      "Encoding the subspace: 165 / 180\n",
      "Encoding the subspace: 166 / 180\n",
      "Encoding the subspace: 167 / 180\n",
      "Encoding the subspace: 168 / 180\n",
      "Encoding the subspace: 169 / 180\n",
      "Encoding the subspace: 170 / 180\n",
      "Encoding the subspace: 171 / 180\n",
      "Encoding the subspace: 172 / 180\n",
      "Encoding the subspace: 173 / 180\n",
      "Encoding the subspace: 174 / 180\n",
      "Encoding the subspace: 175 / 180\n",
      "Encoding the subspace: 176 / 180\n",
      "Encoding the subspace: 177 / 180\n",
      "Encoding the subspace: 178 / 180\n",
      "Encoding the subspace: 179 / 180\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=210, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.206009\n",
      "best loss:  0.20600861199506984\n",
      "Epoch 2 of 500, Train Loss: 0.040932\n",
      "best loss:  0.040932146271928975\n",
      "Epoch 3 of 500, Train Loss: 0.029075\n",
      "best loss:  0.02907489150647045\n",
      "Epoch 4 of 500, Train Loss: 0.023709\n",
      "best loss:  0.023708798020154007\n",
      "Epoch 5 of 500, Train Loss: 0.020331\n",
      "best loss:  0.020330841200247536\n",
      "Epoch 6 of 500, Train Loss: 0.018013\n",
      "best loss:  0.018012744199950248\n",
      "Epoch 7 of 500, Train Loss: 0.016374\n",
      "best loss:  0.01637399610463176\n",
      "Epoch 8 of 500, Train Loss: 0.015092\n",
      "best loss:  0.015091584276817668\n",
      "Epoch 9 of 500, Train Loss: 0.013923\n",
      "best loss:  0.013922646230667858\n",
      "Epoch 10 of 500, Train Loss: 0.012888\n",
      "best loss:  0.01288829268499588\n",
      "Epoch 11 of 500, Train Loss: 0.012069\n",
      "best loss:  0.012069117120384101\n",
      "Epoch 12 of 500, Train Loss: 0.011374\n",
      "best loss:  0.011374260795026542\n",
      "Epoch 13 of 500, Train Loss: 0.010744\n",
      "best loss:  0.010744451983992457\n",
      "Epoch 14 of 500, Train Loss: 0.010207\n",
      "best loss:  0.01020732000217477\n",
      "Epoch 15 of 500, Train Loss: 0.009729\n",
      "best loss:  0.009729459182839378\n",
      "Epoch 16 of 500, Train Loss: 0.009275\n",
      "best loss:  0.009275422595660636\n",
      "Epoch 17 of 500, Train Loss: 0.008871\n",
      "best loss:  0.00887083987330444\n",
      "Epoch 18 of 500, Train Loss: 0.008511\n",
      "best loss:  0.00851081992833886\n",
      "Epoch 19 of 500, Train Loss: 0.008190\n",
      "best loss:  0.00819006211994317\n",
      "Epoch 20 of 500, Train Loss: 0.007916\n",
      "best loss:  0.007915894674406163\n",
      "Epoch 21 of 500, Train Loss: 0.007651\n",
      "best loss:  0.007651461301756799\n",
      "Epoch 22 of 500, Train Loss: 0.007442\n",
      "best loss:  0.007442175955651508\n",
      "Epoch 23 of 500, Train Loss: 0.007244\n",
      "best loss:  0.007244029750617929\n",
      "Epoch 24 of 500, Train Loss: 0.007093\n",
      "best loss:  0.007093071959445311\n",
      "Epoch 25 of 500, Train Loss: 0.006899\n",
      "best loss:  0.006898915666212371\n",
      "Epoch 26 of 500, Train Loss: 0.006739\n",
      "best loss:  0.006738677535606765\n",
      "Epoch 27 of 500, Train Loss: 0.006591\n",
      "best loss:  0.006590750887619996\n",
      "Epoch 28 of 500, Train Loss: 0.006455\n",
      "best loss:  0.00645523331032853\n",
      "Epoch 29 of 500, Train Loss: 0.006366\n",
      "best loss:  0.006365919041798392\n",
      "Epoch 30 of 500, Train Loss: 0.006266\n",
      "best loss:  0.006266332240824862\n",
      "Epoch 31 of 500, Train Loss: 0.006232\n",
      "best loss:  0.006231867186540139\n",
      "Epoch 32 of 500, Train Loss: 0.006150\n",
      "best loss:  0.006150010130670492\n",
      "Epoch 33 of 500, Train Loss: 0.006046\n",
      "best loss:  0.006045568961597791\n",
      "Epoch 34 of 500, Train Loss: 0.006025\n",
      "best loss:  0.006025015288181495\n",
      "Epoch 35 of 500, Train Loss: 0.005933\n",
      "best loss:  0.0059332712851248165\n",
      "Epoch 36 of 500, Train Loss: 0.005889\n",
      "best loss:  0.005888589519769127\n",
      "Epoch 37 of 500, Train Loss: 0.005813\n",
      "best loss:  0.005812900867574855\n",
      "Epoch 38 of 500, Train Loss: 0.005752\n",
      "best loss:  0.005751808462849486\n",
      "Epoch 39 of 500, Train Loss: 0.005715\n",
      "best loss:  0.005714550272781806\n",
      "Epoch 40 of 500, Train Loss: 0.005634\n",
      "best loss:  0.005634019890319116\n",
      "Epoch 41 of 500, Train Loss: 0.005599\n",
      "best loss:  0.0055989095742761\n",
      "Epoch 42 of 500, Train Loss: 0.005597\n",
      "best loss:  0.005597276801275913\n",
      "Epoch 43 of 500, Train Loss: 0.005536\n",
      "best loss:  0.005535668158220567\n",
      "Epoch 44 of 500, Train Loss: 0.005542\n",
      "Epoch 45 of 500, Train Loss: 0.005461\n",
      "best loss:  0.00546097898588005\n",
      "Epoch 46 of 500, Train Loss: 0.005495\n",
      "Epoch 47 of 500, Train Loss: 0.005453\n",
      "best loss:  0.005453032313290146\n",
      "Epoch 48 of 500, Train Loss: 0.005446\n",
      "best loss:  0.005445990146012786\n",
      "Epoch 49 of 500, Train Loss: 0.005412\n",
      "best loss:  0.005411986960887787\n",
      "Epoch 50 of 500, Train Loss: 0.005394\n",
      "best loss:  0.005393801791656152\n",
      "Epoch 51 of 500, Train Loss: 0.005485\n",
      "Epoch 52 of 500, Train Loss: 0.005499\n",
      "Epoch 53 of 500, Train Loss: 0.005664\n",
      "Epoch 54 of 500, Train Loss: 0.005633\n",
      "Epoch 55 of 500, Train Loss: 0.005779\n",
      "Epoch 56 of 500, Train Loss: 0.005679\n",
      "Epoch 57 of 500, Train Loss: 0.005778\n",
      "Epoch 58 of 500, Train Loss: 0.005694\n",
      "Epoch 59 of 500, Train Loss: 0.005643\n",
      "Epoch 60 of 500, Train Loss: 0.005579\n",
      "Epoch 61 of 500, Train Loss: 0.005383\n",
      "best loss:  0.005382785815795227\n",
      "Epoch 62 of 500, Train Loss: 0.005298\n",
      "best loss:  0.005297764208390695\n",
      "Epoch 63 of 500, Train Loss: 0.005136\n",
      "best loss:  0.005136137273001968\n",
      "Epoch 64 of 500, Train Loss: 0.005056\n",
      "best loss:  0.005056404209264388\n",
      "Epoch 65 of 500, Train Loss: 0.005003\n",
      "best loss:  0.005003223076774991\n",
      "Epoch 66 of 500, Train Loss: 0.004942\n",
      "best loss:  0.004942179045717993\n",
      "Epoch 67 of 500, Train Loss: 0.004906\n",
      "best loss:  0.004905739295239517\n",
      "Epoch 68 of 500, Train Loss: 0.004889\n",
      "best loss:  0.00488858607564033\n",
      "Epoch 69 of 500, Train Loss: 0.004893\n",
      "Epoch 70 of 500, Train Loss: 0.004884\n",
      "best loss:  0.004883931116722145\n",
      "Epoch 71 of 500, Train Loss: 0.004903\n",
      "Epoch 72 of 500, Train Loss: 0.004903\n",
      "Epoch 73 of 500, Train Loss: 0.004963\n",
      "Epoch 74 of 500, Train Loss: 0.004979\n",
      "Epoch 75 of 500, Train Loss: 0.005036\n",
      "Epoch 76 of 500, Train Loss: 0.005077\n",
      "Epoch 77 of 500, Train Loss: 0.005103\n",
      "Epoch 78 of 500, Train Loss: 0.005120\n",
      "Epoch 79 of 500, Train Loss: 0.005132\n",
      "Epoch 80 of 500, Train Loss: 0.005126\n",
      "Epoch 81 of 500, Train Loss: 0.005121\n",
      "Epoch 82 of 500, Train Loss: 0.005119\n",
      "Epoch 83 of 500, Train Loss: 0.005116\n",
      "Epoch 84 of 500, Train Loss: 0.005127\n",
      "Epoch 85 of 500, Train Loss: 0.005120\n",
      "Epoch 86 of 500, Train Loss: 0.005127\n",
      "Epoch 87 of 500, Train Loss: 0.005096\n",
      "Epoch 88 of 500, Train Loss: 0.005120\n",
      "Epoch 89 of 500, Train Loss: 0.005035\n",
      "Epoch 90 of 500, Train Loss: 0.005037\n",
      "Epoch 91 of 500, Train Loss: 0.004993\n",
      "Epoch 92 of 500, Train Loss: 0.005017\n",
      "Epoch 93 of 500, Train Loss: 0.004991\n",
      "Epoch 94 of 500, Train Loss: 0.005008\n",
      "Epoch 95 of 500, Train Loss: 0.005009\n",
      "Epoch 96 of 500, Train Loss: 0.005022\n",
      "Epoch 97 of 500, Train Loss: 0.004999\n",
      "Epoch 98 of 500, Train Loss: 0.005024\n",
      "Epoch 99 of 500, Train Loss: 0.004979\n",
      "Epoch 100 of 500, Train Loss: 0.005027\n",
      "Epoch 101 of 500, Train Loss: 0.004964\n",
      "Epoch 102 of 500, Train Loss: 0.004930\n",
      "Epoch 103 of 500, Train Loss: 0.004870\n",
      "best loss:  0.004870486004598883\n",
      "Epoch 104 of 500, Train Loss: 0.004876\n",
      "Epoch 105 of 500, Train Loss: 0.004866\n",
      "best loss:  0.004866440182591449\n",
      "Epoch 106 of 500, Train Loss: 0.004938\n",
      "Epoch 107 of 500, Train Loss: 0.004921\n",
      "Epoch 108 of 500, Train Loss: 0.004966\n",
      "Epoch 109 of 500, Train Loss: 0.004952\n",
      "Epoch 110 of 500, Train Loss: 0.004968\n",
      "Epoch 111 of 500, Train Loss: 0.004994\n",
      "Epoch 112 of 500, Train Loss: 0.005026\n",
      "Epoch 113 of 500, Train Loss: 0.005041\n",
      "Epoch 114 of 500, Train Loss: 0.005060\n",
      "Epoch 115 of 500, Train Loss: 0.005067\n",
      "Epoch 116 of 500, Train Loss: 0.005044\n",
      "Epoch 117 of 500, Train Loss: 0.005007\n",
      "Epoch 118 of 500, Train Loss: 0.004976\n",
      "Epoch 119 of 500, Train Loss: 0.005055\n",
      "Epoch 120 of 500, Train Loss: 0.005055\n",
      "Epoch 121 of 500, Train Loss: 0.005031\n",
      "Epoch 122 of 500, Train Loss: 0.005022\n",
      "Epoch 123 of 500, Train Loss: 0.005023\n",
      "Epoch 124 of 500, Train Loss: 0.004996\n",
      "Epoch 125 of 500, Train Loss: 0.004922\n",
      "Epoch 126 of 500, Train Loss: 0.004897\n",
      "Epoch 127 of 500, Train Loss: 0.004897\n",
      "Epoch 128 of 500, Train Loss: 0.004905\n",
      "Epoch 129 of 500, Train Loss: 0.004918\n",
      "Epoch 130 of 500, Train Loss: 0.004927\n",
      "Epoch 131 of 500, Train Loss: 0.004921\n",
      "Epoch 132 of 500, Train Loss: 0.004918\n",
      "Epoch 133 of 500, Train Loss: 0.004964\n",
      "Epoch 134 of 500, Train Loss: 0.005010\n",
      "Epoch 135 of 500, Train Loss: 0.005040\n",
      "Epoch 136 of 500, Train Loss: 0.005016\n",
      "Epoch 137 of 500, Train Loss: 0.004975\n",
      "Epoch 138 of 500, Train Loss: 0.004965\n",
      "Epoch 139 of 500, Train Loss: 0.004978\n",
      "Epoch 140 of 500, Train Loss: 0.004996\n",
      "Epoch 141 of 500, Train Loss: 0.004986\n",
      "Epoch 142 of 500, Train Loss: 0.004935\n",
      "Epoch 143 of 500, Train Loss: 0.004944\n",
      "Epoch 144 of 500, Train Loss: 0.004962\n",
      "Epoch 145 of 500, Train Loss: 0.005030\n",
      "Epoch 146 of 500, Train Loss: 0.005001\n",
      "Epoch 147 of 500, Train Loss: 0.004991\n",
      "Epoch 148 of 500, Train Loss: 0.004972\n",
      "Epoch 149 of 500, Train Loss: 0.004988\n",
      "Epoch 150 of 500, Train Loss: 0.004963\n",
      "Epoch 151 of 500, Train Loss: 0.004986\n",
      "Epoch 152 of 500, Train Loss: 0.004927\n",
      "Epoch 153 of 500, Train Loss: 0.004917\n",
      "Epoch 154 of 500, Train Loss: 0.004882\n",
      "Epoch 155 of 500, Train Loss: 0.004882\n",
      "Epoch 156 of 500, Train Loss: 0.004908\n",
      "Epoch 157 of 500, Train Loss: 0.004936\n",
      "Epoch 158 of 500, Train Loss: 0.004894\n",
      "Epoch 159 of 500, Train Loss: 0.004918\n",
      "Epoch 160 of 500, Train Loss: 0.004929\n",
      "Epoch 161 of 500, Train Loss: 0.004908\n",
      "Epoch 162 of 500, Train Loss: 0.004890\n",
      "Epoch 163 of 500, Train Loss: 0.004900\n",
      "Epoch 164 of 500, Train Loss: 0.004926\n",
      "Epoch 165 of 500, Train Loss: 0.004944\n",
      "Epoch 166 of 500, Train Loss: 0.004909\n",
      "Epoch 167 of 500, Train Loss: 0.004928\n",
      "Epoch 168 of 500, Train Loss: 0.004955\n",
      "Epoch 169 of 500, Train Loss: 0.004901\n",
      "Epoch 170 of 500, Train Loss: 0.004898\n",
      "Epoch 171 of 500, Train Loss: 0.004917\n",
      "Epoch 172 of 500, Train Loss: 0.004919\n",
      "Epoch 173 of 500, Train Loss: 0.004861\n",
      "best loss:  0.004861217650360168\n",
      "Epoch 174 of 500, Train Loss: 0.004868\n",
      "Epoch 175 of 500, Train Loss: 0.004881\n",
      "Epoch 176 of 500, Train Loss: 0.004931\n",
      "Epoch 177 of 500, Train Loss: 0.004876\n",
      "Epoch 178 of 500, Train Loss: 0.004896\n",
      "Epoch 179 of 500, Train Loss: 0.004885\n",
      "Epoch 180 of 500, Train Loss: 0.004919\n",
      "Epoch 181 of 500, Train Loss: 0.004933\n",
      "Epoch 182 of 500, Train Loss: 0.004934\n",
      "Epoch 183 of 500, Train Loss: 0.004912\n",
      "Epoch 184 of 500, Train Loss: 0.004885\n",
      "Epoch 185 of 500, Train Loss: 0.004865\n",
      "Epoch 186 of 500, Train Loss: 0.004867\n",
      "Epoch 187 of 500, Train Loss: 0.004892\n",
      "Epoch 188 of 500, Train Loss: 0.004921\n",
      "Epoch 189 of 500, Train Loss: 0.004900\n",
      "Epoch 190 of 500, Train Loss: 0.004887\n",
      "Epoch 191 of 500, Train Loss: 0.004863\n",
      "Epoch 192 of 500, Train Loss: 0.004884\n",
      "Epoch 193 of 500, Train Loss: 0.004869\n",
      "Epoch 194 of 500, Train Loss: 0.004913\n",
      "Epoch 195 of 500, Train Loss: 0.004861\n",
      "best loss:  0.004860939285363695\n",
      "Epoch 196 of 500, Train Loss: 0.004899\n",
      "Epoch 197 of 500, Train Loss: 0.004833\n",
      "best loss:  0.004833206240301627\n",
      "Epoch 198 of 500, Train Loss: 0.004855\n",
      "Epoch 199 of 500, Train Loss: 0.004873\n",
      "Epoch 200 of 500, Train Loss: 0.004874\n",
      "Epoch 201 of 500, Train Loss: 0.004865\n",
      "Epoch 202 of 500, Train Loss: 0.004866\n",
      "Epoch 203 of 500, Train Loss: 0.004894\n",
      "Epoch 204 of 500, Train Loss: 0.004884\n",
      "Epoch 205 of 500, Train Loss: 0.004878\n",
      "Epoch 206 of 500, Train Loss: 0.004888\n",
      "Epoch 207 of 500, Train Loss: 0.004895\n",
      "Epoch 208 of 500, Train Loss: 0.004922\n",
      "Epoch 209 of 500, Train Loss: 0.004919\n",
      "Epoch 210 of 500, Train Loss: 0.004938\n",
      "Epoch 211 of 500, Train Loss: 0.004880\n",
      "Epoch 212 of 500, Train Loss: 0.004942\n",
      "Epoch 213 of 500, Train Loss: 0.004904\n",
      "Epoch 214 of 500, Train Loss: 0.004927\n",
      "Epoch 215 of 500, Train Loss: 0.004921\n",
      "Epoch 216 of 500, Train Loss: 0.004925\n",
      "Epoch 217 of 500, Train Loss: 0.004875\n",
      "Epoch 218 of 500, Train Loss: 0.004872\n",
      "Epoch 219 of 500, Train Loss: 0.004890\n",
      "Epoch 220 of 500, Train Loss: 0.004956\n",
      "Epoch 221 of 500, Train Loss: 0.004942\n",
      "Epoch 222 of 500, Train Loss: 0.004932\n",
      "Epoch 223 of 500, Train Loss: 0.004908\n",
      "Epoch 224 of 500, Train Loss: 0.004950\n",
      "Epoch 225 of 500, Train Loss: 0.004945\n",
      "Epoch 226 of 500, Train Loss: 0.004936\n",
      "Epoch 227 of 500, Train Loss: 0.004938\n",
      "Epoch 228 of 500, Train Loss: 0.004969\n",
      "Epoch 229 of 500, Train Loss: 0.004945\n",
      "Epoch 230 of 500, Train Loss: 0.004962\n",
      "Epoch 231 of 500, Train Loss: 0.004951\n",
      "Epoch 232 of 500, Train Loss: 0.004987\n",
      "Epoch 233 of 500, Train Loss: 0.004980\n",
      "Epoch 234 of 500, Train Loss: 0.004997\n",
      "Epoch 235 of 500, Train Loss: 0.005023\n",
      "Epoch 236 of 500, Train Loss: 0.005078\n",
      "Epoch 237 of 500, Train Loss: 0.005060\n",
      "Epoch 238 of 500, Train Loss: 0.005079\n",
      "Epoch 239 of 500, Train Loss: 0.005102\n",
      "Epoch 240 of 500, Train Loss: 0.005153\n",
      "Epoch 241 of 500, Train Loss: 0.005106\n",
      "Epoch 242 of 500, Train Loss: 0.005069\n",
      "Epoch 243 of 500, Train Loss: 0.005041\n",
      "Epoch 244 of 500, Train Loss: 0.005091\n",
      "Epoch 245 of 500, Train Loss: 0.005062\n",
      "Epoch 246 of 500, Train Loss: 0.005103\n",
      "Epoch 247 of 500, Train Loss: 0.005104\n",
      "Epoch 248 of 500, Train Loss: 0.005089\n",
      "Epoch 249 of 500, Train Loss: 0.005054\n",
      "Epoch 250 of 500, Train Loss: 0.005044\n",
      "Epoch 251 of 500, Train Loss: 0.005003\n",
      "Epoch 252 of 500, Train Loss: 0.005063\n",
      "Epoch 253 of 500, Train Loss: 0.005054\n",
      "Epoch 254 of 500, Train Loss: 0.005109\n",
      "Epoch 255 of 500, Train Loss: 0.005091\n",
      "Epoch 256 of 500, Train Loss: 0.005108\n",
      "Epoch 257 of 500, Train Loss: 0.005106\n",
      "Epoch 258 of 500, Train Loss: 0.005166\n",
      "Epoch 259 of 500, Train Loss: 0.005134\n",
      "Epoch 260 of 500, Train Loss: 0.005127\n",
      "Epoch 261 of 500, Train Loss: 0.005035\n",
      "Epoch 262 of 500, Train Loss: 0.005023\n",
      "Epoch 263 of 500, Train Loss: 0.005038\n",
      "Epoch 264 of 500, Train Loss: 0.005075\n",
      "Epoch 265 of 500, Train Loss: 0.005043\n",
      "Epoch 266 of 500, Train Loss: 0.005071\n",
      "Epoch 267 of 500, Train Loss: 0.005048\n",
      "Epoch 268 of 500, Train Loss: 0.005064\n",
      "Epoch 269 of 500, Train Loss: 0.005073\n",
      "Epoch 270 of 500, Train Loss: 0.005076\n",
      "Epoch 271 of 500, Train Loss: 0.005061\n",
      "Epoch 272 of 500, Train Loss: 0.005116\n",
      "Epoch 273 of 500, Train Loss: 0.005118\n",
      "Epoch 274 of 500, Train Loss: 0.005099\n",
      "Epoch 275 of 500, Train Loss: 0.005027\n",
      "Epoch 276 of 500, Train Loss: 0.005048\n",
      "Epoch 277 of 500, Train Loss: 0.004989\n",
      "Epoch 278 of 500, Train Loss: 0.004974\n",
      "Epoch 279 of 500, Train Loss: 0.004936\n",
      "Epoch 280 of 500, Train Loss: 0.004979\n",
      "Epoch 281 of 500, Train Loss: 0.004958\n",
      "Epoch 282 of 500, Train Loss: 0.005021\n",
      "Epoch 283 of 500, Train Loss: 0.005045\n",
      "Epoch 284 of 500, Train Loss: 0.005131\n",
      "Epoch 285 of 500, Train Loss: 0.005055\n",
      "Epoch 286 of 500, Train Loss: 0.005070\n",
      "Epoch 287 of 500, Train Loss: 0.005045\n",
      "Epoch 288 of 500, Train Loss: 0.005094\n",
      "Epoch 289 of 500, Train Loss: 0.005028\n",
      "Epoch 290 of 500, Train Loss: 0.005043\n",
      "Epoch 291 of 500, Train Loss: 0.004999\n",
      "Epoch 292 of 500, Train Loss: 0.005058\n",
      "Epoch 293 of 500, Train Loss: 0.005030\n",
      "Epoch 294 of 500, Train Loss: 0.005065\n",
      "Epoch 295 of 500, Train Loss: 0.005019\n",
      "Epoch 296 of 500, Train Loss: 0.005076\n",
      "Epoch 297 of 500, Train Loss: 0.005060\n",
      "Epoch 298 of 500, Train Loss: 0.005099\n",
      "Epoch 299 of 500, Train Loss: 0.005004\n",
      "Epoch 300 of 500, Train Loss: 0.004999\n",
      "Epoch 301 of 500, Train Loss: 0.004978\n",
      "Epoch 302 of 500, Train Loss: 0.005033\n",
      "Epoch 303 of 500, Train Loss: 0.005026\n",
      "Epoch 304 of 500, Train Loss: 0.005090\n",
      "Epoch 305 of 500, Train Loss: 0.005067\n",
      "Epoch 306 of 500, Train Loss: 0.005119\n",
      "Epoch 307 of 500, Train Loss: 0.005029\n",
      "Epoch 308 of 500, Train Loss: 0.005114\n",
      "Epoch 309 of 500, Train Loss: 0.005029\n",
      "Epoch 310 of 500, Train Loss: 0.004984\n",
      "Epoch 311 of 500, Train Loss: 0.004937\n",
      "Epoch 312 of 500, Train Loss: 0.005025\n",
      "Epoch 313 of 500, Train Loss: 0.005028\n",
      "Epoch 314 of 500, Train Loss: 0.005093\n",
      "Epoch 315 of 500, Train Loss: 0.005040\n",
      "Epoch 316 of 500, Train Loss: 0.005114\n",
      "Epoch 317 of 500, Train Loss: 0.004980\n",
      "Epoch 318 of 500, Train Loss: 0.004990\n",
      "Epoch 319 of 500, Train Loss: 0.004959\n",
      "Epoch 320 of 500, Train Loss: 0.004987\n",
      "Epoch 321 of 500, Train Loss: 0.005013\n",
      "Epoch 322 of 500, Train Loss: 0.005063\n",
      "Epoch 323 of 500, Train Loss: 0.005049\n",
      "Epoch 324 of 500, Train Loss: 0.005093\n",
      "Epoch 325 of 500, Train Loss: 0.005010\n",
      "Epoch 326 of 500, Train Loss: 0.005020\n",
      "Epoch 327 of 500, Train Loss: 0.005073\n",
      "Epoch 328 of 500, Train Loss: 0.005096\n",
      "Epoch 329 of 500, Train Loss: 0.005055\n",
      "Epoch 330 of 500, Train Loss: 0.005109\n",
      "Epoch 331 of 500, Train Loss: 0.005013\n",
      "Epoch 332 of 500, Train Loss: 0.005005\n",
      "Epoch 333 of 500, Train Loss: 0.004941\n",
      "Epoch 334 of 500, Train Loss: 0.004934\n",
      "Epoch 335 of 500, Train Loss: 0.004931\n",
      "Epoch 336 of 500, Train Loss: 0.004989\n",
      "Epoch 337 of 500, Train Loss: 0.005048\n",
      "Epoch 338 of 500, Train Loss: 0.005103\n",
      "Epoch 339 of 500, Train Loss: 0.005087\n",
      "Epoch 340 of 500, Train Loss: 0.005120\n",
      "Epoch 341 of 500, Train Loss: 0.004988\n",
      "Epoch 342 of 500, Train Loss: 0.004974\n",
      "Epoch 343 of 500, Train Loss: 0.004917\n",
      "Epoch 344 of 500, Train Loss: 0.004971\n",
      "Epoch 345 of 500, Train Loss: 0.004990\n",
      "Epoch 346 of 500, Train Loss: 0.005045\n",
      "Epoch 347 of 500, Train Loss: 0.005072\n",
      "Epoch 348 of 500, Train Loss: 0.005160\n",
      "Epoch 349 of 500, Train Loss: 0.005098\n",
      "Epoch 350 of 500, Train Loss: 0.005139\n",
      "Epoch 351 of 500, Train Loss: 0.005033\n",
      "Epoch 352 of 500, Train Loss: 0.005037\n",
      "Epoch 353 of 500, Train Loss: 0.004947\n",
      "Epoch 354 of 500, Train Loss: 0.004990\n",
      "Epoch 355 of 500, Train Loss: 0.005029\n",
      "Epoch 356 of 500, Train Loss: 0.005121\n",
      "Epoch 357 of 500, Train Loss: 0.005034\n",
      "Epoch 358 of 500, Train Loss: 0.005075\n",
      "Epoch 359 of 500, Train Loss: 0.004988\n",
      "Epoch 360 of 500, Train Loss: 0.004986\n",
      "Epoch 361 of 500, Train Loss: 0.004929\n",
      "Epoch 362 of 500, Train Loss: 0.004982\n",
      "Epoch 363 of 500, Train Loss: 0.005005\n",
      "Epoch 364 of 500, Train Loss: 0.005054\n",
      "Epoch 365 of 500, Train Loss: 0.005079\n",
      "Epoch 366 of 500, Train Loss: 0.005189\n",
      "Epoch 367 of 500, Train Loss: 0.005210\n",
      "Epoch 368 of 500, Train Loss: 0.005188\n",
      "Epoch 369 of 500, Train Loss: 0.005126\n",
      "Epoch 370 of 500, Train Loss: 0.005132\n",
      "Epoch 371 of 500, Train Loss: 0.005049\n",
      "Epoch 372 of 500, Train Loss: 0.005057\n",
      "Epoch 373 of 500, Train Loss: 0.005011\n",
      "Epoch 374 of 500, Train Loss: 0.005057\n",
      "Epoch 375 of 500, Train Loss: 0.005038\n",
      "Epoch 376 of 500, Train Loss: 0.005062\n",
      "Epoch 377 of 500, Train Loss: 0.004987\n",
      "Epoch 378 of 500, Train Loss: 0.005000\n",
      "Epoch 379 of 500, Train Loss: 0.004956\n",
      "Epoch 380 of 500, Train Loss: 0.005006\n",
      "Epoch 381 of 500, Train Loss: 0.005015\n",
      "Epoch 382 of 500, Train Loss: 0.005104\n",
      "Epoch 383 of 500, Train Loss: 0.005077\n",
      "Epoch 384 of 500, Train Loss: 0.005119\n",
      "Epoch 385 of 500, Train Loss: 0.005072\n",
      "Epoch 386 of 500, Train Loss: 0.005068\n",
      "Epoch 387 of 500, Train Loss: 0.005006\n",
      "Epoch 388 of 500, Train Loss: 0.005022\n",
      "Epoch 389 of 500, Train Loss: 0.005014\n",
      "Epoch 390 of 500, Train Loss: 0.005056\n",
      "Epoch 391 of 500, Train Loss: 0.005020\n",
      "Epoch 392 of 500, Train Loss: 0.005042\n",
      "Epoch 393 of 500, Train Loss: 0.005017\n",
      "Epoch 394 of 500, Train Loss: 0.005043\n",
      "Epoch 395 of 500, Train Loss: 0.005016\n",
      "Epoch 396 of 500, Train Loss: 0.004985\n",
      "Epoch 397 of 500, Train Loss: 0.004948\n",
      "Epoch 398 of 500, Train Loss: 0.004994\n",
      "Epoch 399 of 500, Train Loss: 0.004983\n",
      "Epoch 400 of 500, Train Loss: 0.004957\n",
      "Epoch 401 of 500, Train Loss: 0.004918\n",
      "Epoch 402 of 500, Train Loss: 0.004933\n",
      "Epoch 403 of 500, Train Loss: 0.004979\n",
      "Epoch 404 of 500, Train Loss: 0.005017\n",
      "Epoch 405 of 500, Train Loss: 0.004994\n",
      "Epoch 406 of 500, Train Loss: 0.005016\n",
      "Epoch 407 of 500, Train Loss: 0.005057\n",
      "Epoch 408 of 500, Train Loss: 0.005075\n",
      "Epoch 409 of 500, Train Loss: 0.005028\n",
      "Epoch 410 of 500, Train Loss: 0.005085\n",
      "Epoch 411 of 500, Train Loss: 0.005062\n",
      "Epoch 412 of 500, Train Loss: 0.005072\n",
      "Epoch 413 of 500, Train Loss: 0.005016\n",
      "Epoch 414 of 500, Train Loss: 0.005034\n",
      "Epoch 415 of 500, Train Loss: 0.005037\n",
      "Epoch 416 of 500, Train Loss: 0.005049\n",
      "Epoch 417 of 500, Train Loss: 0.004986\n",
      "Epoch 418 of 500, Train Loss: 0.005012\n",
      "Epoch 419 of 500, Train Loss: 0.004944\n",
      "Epoch 420 of 500, Train Loss: 0.004990\n",
      "Epoch 421 of 500, Train Loss: 0.004967\n",
      "Epoch 422 of 500, Train Loss: 0.005010\n",
      "Epoch 423 of 500, Train Loss: 0.004982\n",
      "Epoch 424 of 500, Train Loss: 0.005042\n",
      "Epoch 425 of 500, Train Loss: 0.004994\n",
      "Epoch 426 of 500, Train Loss: 0.005019\n",
      "Epoch 427 of 500, Train Loss: 0.005006\n",
      "Epoch 428 of 500, Train Loss: 0.005014\n",
      "Epoch 429 of 500, Train Loss: 0.004990\n",
      "Epoch 430 of 500, Train Loss: 0.005037\n",
      "Epoch 431 of 500, Train Loss: 0.005051\n",
      "Epoch 432 of 500, Train Loss: 0.005085\n",
      "Epoch 433 of 500, Train Loss: 0.005092\n",
      "Epoch 434 of 500, Train Loss: 0.005171\n",
      "Epoch 435 of 500, Train Loss: 0.005065\n",
      "Epoch 436 of 500, Train Loss: 0.005059\n",
      "Epoch 437 of 500, Train Loss: 0.005009\n",
      "Epoch 438 of 500, Train Loss: 0.005033\n",
      "Epoch 439 of 500, Train Loss: 0.005027\n",
      "Epoch 440 of 500, Train Loss: 0.005118\n",
      "Epoch 441 of 500, Train Loss: 0.005061\n",
      "Epoch 442 of 500, Train Loss: 0.005035\n",
      "Epoch 443 of 500, Train Loss: 0.005027\n",
      "Epoch 444 of 500, Train Loss: 0.005041\n",
      "Epoch 445 of 500, Train Loss: 0.004996\n",
      "Epoch 446 of 500, Train Loss: 0.005046\n",
      "Epoch 447 of 500, Train Loss: 0.005050\n",
      "Epoch 448 of 500, Train Loss: 0.005127\n",
      "Epoch 449 of 500, Train Loss: 0.005036\n",
      "Epoch 450 of 500, Train Loss: 0.005094\n",
      "Epoch 451 of 500, Train Loss: 0.005051\n",
      "Epoch 452 of 500, Train Loss: 0.005110\n",
      "Epoch 453 of 500, Train Loss: 0.005032\n",
      "Epoch 454 of 500, Train Loss: 0.005038\n",
      "Epoch 455 of 500, Train Loss: 0.005029\n",
      "Epoch 456 of 500, Train Loss: 0.005084\n",
      "Epoch 457 of 500, Train Loss: 0.005001\n",
      "Epoch 458 of 500, Train Loss: 0.005034\n",
      "Epoch 459 of 500, Train Loss: 0.004972\n",
      "Epoch 460 of 500, Train Loss: 0.005013\n",
      "Epoch 461 of 500, Train Loss: 0.004986\n",
      "Epoch 462 of 500, Train Loss: 0.005038\n",
      "Epoch 463 of 500, Train Loss: 0.005012\n",
      "Epoch 464 of 500, Train Loss: 0.005124\n",
      "Epoch 465 of 500, Train Loss: 0.005076\n",
      "Epoch 466 of 500, Train Loss: 0.005158\n",
      "Epoch 467 of 500, Train Loss: 0.005053\n",
      "Epoch 468 of 500, Train Loss: 0.005114\n",
      "Epoch 469 of 500, Train Loss: 0.005011\n",
      "Epoch 470 of 500, Train Loss: 0.005120\n",
      "Epoch 471 of 500, Train Loss: 0.005097\n",
      "Epoch 472 of 500, Train Loss: 0.005195\n",
      "Epoch 473 of 500, Train Loss: 0.005065\n",
      "Epoch 474 of 500, Train Loss: 0.005080\n",
      "Epoch 475 of 500, Train Loss: 0.004964\n",
      "Epoch 476 of 500, Train Loss: 0.005042\n",
      "Epoch 477 of 500, Train Loss: 0.004977\n",
      "Epoch 478 of 500, Train Loss: 0.005094\n",
      "Epoch 479 of 500, Train Loss: 0.005048\n",
      "Epoch 480 of 500, Train Loss: 0.005107\n",
      "Epoch 481 of 500, Train Loss: 0.005014\n",
      "Epoch 482 of 500, Train Loss: 0.005043\n",
      "Epoch 483 of 500, Train Loss: 0.004991\n",
      "Epoch 484 of 500, Train Loss: 0.005092\n",
      "Epoch 485 of 500, Train Loss: 0.005040\n",
      "Epoch 486 of 500, Train Loss: 0.005069\n",
      "Epoch 487 of 500, Train Loss: 0.005078\n",
      "Epoch 488 of 500, Train Loss: 0.005173\n",
      "Epoch 489 of 500, Train Loss: 0.005067\n",
      "Epoch 490 of 500, Train Loss: 0.005102\n",
      "Epoch 491 of 500, Train Loss: 0.004997\n",
      "Epoch 492 of 500, Train Loss: 0.005046\n",
      "Epoch 493 of 500, Train Loss: 0.005060\n",
      "Epoch 494 of 500, Train Loss: 0.005100\n",
      "Epoch 495 of 500, Train Loss: 0.005041\n",
      "Epoch 496 of 500, Train Loss: 0.005081\n",
      "Epoch 497 of 500, Train Loss: 0.005079\n",
      "Epoch 498 of 500, Train Loss: 0.005072\n",
      "Epoch 499 of 500, Train Loss: 0.005018\n",
      "Epoch 500 of 500, Train Loss: 0.005081\n",
      "latent train shape:  (16395, 210)\n",
      "M: 210, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 22\n",
      "Training the subspace: 0 / 210\n",
      "Training the subspace: 1 / 210\n",
      "Training the subspace: 2 / 210\n",
      "Training the subspace: 3 / 210\n",
      "Training the subspace: 4 / 210\n",
      "Training the subspace: 5 / 210\n",
      "Training the subspace: 6 / 210\n",
      "Training the subspace: 7 / 210\n",
      "Training the subspace: 8 / 210\n",
      "Training the subspace: 9 / 210\n",
      "Training the subspace: 10 / 210\n",
      "Training the subspace: 11 / 210\n",
      "Training the subspace: 12 / 210\n",
      "Training the subspace: 13 / 210\n",
      "Training the subspace: 14 / 210\n",
      "Training the subspace: 15 / 210\n",
      "Training the subspace: 16 / 210\n",
      "Training the subspace: 17 / 210\n",
      "Training the subspace: 18 / 210\n",
      "Training the subspace: 19 / 210\n",
      "Training the subspace: 20 / 210\n",
      "Training the subspace: 21 / 210\n",
      "Training the subspace: 22 / 210\n",
      "Training the subspace: 23 / 210\n",
      "Training the subspace: 24 / 210\n",
      "Training the subspace: 25 / 210\n",
      "Training the subspace: 26 / 210\n",
      "Training the subspace: 27 / 210\n",
      "Training the subspace: 28 / 210\n",
      "Training the subspace: 29 / 210\n",
      "Training the subspace: 30 / 210\n",
      "Training the subspace: 31 / 210\n",
      "Training the subspace: 32 / 210\n",
      "Training the subspace: 33 / 210\n",
      "Training the subspace: 34 / 210\n",
      "Training the subspace: 35 / 210\n",
      "Training the subspace: 36 / 210\n",
      "Training the subspace: 37 / 210\n",
      "Training the subspace: 38 / 210\n",
      "Training the subspace: 39 / 210\n",
      "Training the subspace: 40 / 210\n",
      "Training the subspace: 41 / 210\n",
      "Training the subspace: 42 / 210\n",
      "Training the subspace: 43 / 210\n",
      "Training the subspace: 44 / 210\n",
      "Training the subspace: 45 / 210\n",
      "Training the subspace: 46 / 210\n",
      "Training the subspace: 47 / 210\n",
      "Training the subspace: 48 / 210\n",
      "Training the subspace: 49 / 210\n",
      "Training the subspace: 50 / 210\n",
      "Training the subspace: 51 / 210\n",
      "Training the subspace: 52 / 210\n",
      "Training the subspace: 53 / 210\n",
      "Training the subspace: 54 / 210\n",
      "Training the subspace: 55 / 210\n",
      "Training the subspace: 56 / 210\n",
      "Training the subspace: 57 / 210\n",
      "Training the subspace: 58 / 210\n",
      "Training the subspace: 59 / 210\n",
      "Training the subspace: 60 / 210\n",
      "Training the subspace: 61 / 210\n",
      "Training the subspace: 62 / 210\n",
      "Training the subspace: 63 / 210\n",
      "Training the subspace: 64 / 210\n",
      "Training the subspace: 65 / 210\n",
      "Training the subspace: 66 / 210\n",
      "Training the subspace: 67 / 210\n",
      "Training the subspace: 68 / 210\n",
      "Training the subspace: 69 / 210\n",
      "Training the subspace: 70 / 210\n",
      "Training the subspace: 71 / 210\n",
      "Training the subspace: 72 / 210\n",
      "Training the subspace: 73 / 210\n",
      "Training the subspace: 74 / 210\n",
      "Training the subspace: 75 / 210\n",
      "Training the subspace: 76 / 210\n",
      "Training the subspace: 77 / 210\n",
      "Training the subspace: 78 / 210\n",
      "Training the subspace: 79 / 210\n",
      "Training the subspace: 80 / 210\n",
      "Training the subspace: 81 / 210\n",
      "Training the subspace: 82 / 210\n",
      "Training the subspace: 83 / 210\n",
      "Training the subspace: 84 / 210\n",
      "Training the subspace: 85 / 210\n",
      "Training the subspace: 86 / 210\n",
      "Training the subspace: 87 / 210\n",
      "Training the subspace: 88 / 210\n",
      "Training the subspace: 89 / 210\n",
      "Training the subspace: 90 / 210\n",
      "Training the subspace: 91 / 210\n",
      "Training the subspace: 92 / 210\n",
      "Training the subspace: 93 / 210\n",
      "Training the subspace: 94 / 210\n",
      "Training the subspace: 95 / 210\n",
      "Training the subspace: 96 / 210\n",
      "Training the subspace: 97 / 210\n",
      "Training the subspace: 98 / 210\n",
      "Training the subspace: 99 / 210\n",
      "Training the subspace: 100 / 210\n",
      "Training the subspace: 101 / 210\n",
      "Training the subspace: 102 / 210\n",
      "Training the subspace: 103 / 210\n",
      "Training the subspace: 104 / 210\n",
      "Training the subspace: 105 / 210\n",
      "Training the subspace: 106 / 210\n",
      "Training the subspace: 107 / 210\n",
      "Training the subspace: 108 / 210\n",
      "Training the subspace: 109 / 210\n",
      "Training the subspace: 110 / 210\n",
      "Training the subspace: 111 / 210\n",
      "Training the subspace: 112 / 210\n",
      "Training the subspace: 113 / 210\n",
      "Training the subspace: 114 / 210\n",
      "Training the subspace: 115 / 210\n",
      "Training the subspace: 116 / 210\n",
      "Training the subspace: 117 / 210\n",
      "Training the subspace: 118 / 210\n",
      "Training the subspace: 119 / 210\n",
      "Training the subspace: 120 / 210\n",
      "Training the subspace: 121 / 210\n",
      "Training the subspace: 122 / 210\n",
      "Training the subspace: 123 / 210\n",
      "Training the subspace: 124 / 210\n",
      "Training the subspace: 125 / 210\n",
      "Training the subspace: 126 / 210\n",
      "Training the subspace: 127 / 210\n",
      "Training the subspace: 128 / 210\n",
      "Training the subspace: 129 / 210\n",
      "Training the subspace: 130 / 210\n",
      "Training the subspace: 131 / 210\n",
      "Training the subspace: 132 / 210\n",
      "Training the subspace: 133 / 210\n",
      "Training the subspace: 134 / 210\n",
      "Training the subspace: 135 / 210\n",
      "Training the subspace: 136 / 210\n",
      "Training the subspace: 137 / 210\n",
      "Training the subspace: 138 / 210\n",
      "Training the subspace: 139 / 210\n",
      "Training the subspace: 140 / 210\n",
      "Training the subspace: 141 / 210\n",
      "Training the subspace: 142 / 210\n",
      "Training the subspace: 143 / 210\n",
      "Training the subspace: 144 / 210\n",
      "Training the subspace: 145 / 210\n",
      "Training the subspace: 146 / 210\n",
      "Training the subspace: 147 / 210\n",
      "Training the subspace: 148 / 210\n",
      "Training the subspace: 149 / 210\n",
      "Training the subspace: 150 / 210\n",
      "Training the subspace: 151 / 210\n",
      "Training the subspace: 152 / 210\n",
      "Training the subspace: 153 / 210\n",
      "Training the subspace: 154 / 210\n",
      "Training the subspace: 155 / 210\n",
      "Training the subspace: 156 / 210\n",
      "Training the subspace: 157 / 210\n",
      "Training the subspace: 158 / 210\n",
      "Training the subspace: 159 / 210\n",
      "Training the subspace: 160 / 210\n",
      "Training the subspace: 161 / 210\n",
      "Training the subspace: 162 / 210\n",
      "Training the subspace: 163 / 210\n",
      "Training the subspace: 164 / 210\n",
      "Training the subspace: 165 / 210\n",
      "Training the subspace: 166 / 210\n",
      "Training the subspace: 167 / 210\n",
      "Training the subspace: 168 / 210\n",
      "Training the subspace: 169 / 210\n",
      "Training the subspace: 170 / 210\n",
      "Training the subspace: 171 / 210\n",
      "Training the subspace: 172 / 210\n",
      "Training the subspace: 173 / 210\n",
      "Training the subspace: 174 / 210\n",
      "Training the subspace: 175 / 210\n",
      "Training the subspace: 176 / 210\n",
      "Training the subspace: 177 / 210\n",
      "Training the subspace: 178 / 210\n",
      "Training the subspace: 179 / 210\n",
      "Training the subspace: 180 / 210\n",
      "Training the subspace: 181 / 210\n",
      "Training the subspace: 182 / 210\n",
      "Training the subspace: 183 / 210\n",
      "Training the subspace: 184 / 210\n",
      "Training the subspace: 185 / 210\n",
      "Training the subspace: 186 / 210\n",
      "Training the subspace: 187 / 210\n",
      "Training the subspace: 188 / 210\n",
      "Training the subspace: 189 / 210\n",
      "Training the subspace: 190 / 210\n",
      "Training the subspace: 191 / 210\n",
      "Training the subspace: 192 / 210\n",
      "Training the subspace: 193 / 210\n",
      "Training the subspace: 194 / 210\n",
      "Training the subspace: 195 / 210\n",
      "Training the subspace: 196 / 210\n",
      "Training the subspace: 197 / 210\n",
      "Training the subspace: 198 / 210\n",
      "Training the subspace: 199 / 210\n",
      "Training the subspace: 200 / 210\n",
      "Training the subspace: 201 / 210\n",
      "Training the subspace: 202 / 210\n",
      "Training the subspace: 203 / 210\n",
      "Training the subspace: 204 / 210\n",
      "Training the subspace: 205 / 210\n",
      "Training the subspace: 206 / 210\n",
      "Training the subspace: 207 / 210\n",
      "Training the subspace: 208 / 210\n",
      "Training the subspace: 209 / 210\n",
      "Encoding the subspace: 0 / 210\n",
      "Encoding the subspace: 1 / 210\n",
      "Encoding the subspace: 2 / 210\n",
      "Encoding the subspace: 3 / 210\n",
      "Encoding the subspace: 4 / 210\n",
      "Encoding the subspace: 5 / 210\n",
      "Encoding the subspace: 6 / 210\n",
      "Encoding the subspace: 7 / 210\n",
      "Encoding the subspace: 8 / 210\n",
      "Encoding the subspace: 9 / 210\n",
      "Encoding the subspace: 10 / 210\n",
      "Encoding the subspace: 11 / 210\n",
      "Encoding the subspace: 12 / 210\n",
      "Encoding the subspace: 13 / 210\n",
      "Encoding the subspace: 14 / 210\n",
      "Encoding the subspace: 15 / 210\n",
      "Encoding the subspace: 16 / 210\n",
      "Encoding the subspace: 17 / 210\n",
      "Encoding the subspace: 18 / 210\n",
      "Encoding the subspace: 19 / 210\n",
      "Encoding the subspace: 20 / 210\n",
      "Encoding the subspace: 21 / 210\n",
      "Encoding the subspace: 22 / 210\n",
      "Encoding the subspace: 23 / 210\n",
      "Encoding the subspace: 24 / 210\n",
      "Encoding the subspace: 25 / 210\n",
      "Encoding the subspace: 26 / 210\n",
      "Encoding the subspace: 27 / 210\n",
      "Encoding the subspace: 28 / 210\n",
      "Encoding the subspace: 29 / 210\n",
      "Encoding the subspace: 30 / 210\n",
      "Encoding the subspace: 31 / 210\n",
      "Encoding the subspace: 32 / 210\n",
      "Encoding the subspace: 33 / 210\n",
      "Encoding the subspace: 34 / 210\n",
      "Encoding the subspace: 35 / 210\n",
      "Encoding the subspace: 36 / 210\n",
      "Encoding the subspace: 37 / 210\n",
      "Encoding the subspace: 38 / 210\n",
      "Encoding the subspace: 39 / 210\n",
      "Encoding the subspace: 40 / 210\n",
      "Encoding the subspace: 41 / 210\n",
      "Encoding the subspace: 42 / 210\n",
      "Encoding the subspace: 43 / 210\n",
      "Encoding the subspace: 44 / 210\n",
      "Encoding the subspace: 45 / 210\n",
      "Encoding the subspace: 46 / 210\n",
      "Encoding the subspace: 47 / 210\n",
      "Encoding the subspace: 48 / 210\n",
      "Encoding the subspace: 49 / 210\n",
      "Encoding the subspace: 50 / 210\n",
      "Encoding the subspace: 51 / 210\n",
      "Encoding the subspace: 52 / 210\n",
      "Encoding the subspace: 53 / 210\n",
      "Encoding the subspace: 54 / 210\n",
      "Encoding the subspace: 55 / 210\n",
      "Encoding the subspace: 56 / 210\n",
      "Encoding the subspace: 57 / 210\n",
      "Encoding the subspace: 58 / 210\n",
      "Encoding the subspace: 59 / 210\n",
      "Encoding the subspace: 60 / 210\n",
      "Encoding the subspace: 61 / 210\n",
      "Encoding the subspace: 62 / 210\n",
      "Encoding the subspace: 63 / 210\n",
      "Encoding the subspace: 64 / 210\n",
      "Encoding the subspace: 65 / 210\n",
      "Encoding the subspace: 66 / 210\n",
      "Encoding the subspace: 67 / 210\n",
      "Encoding the subspace: 68 / 210\n",
      "Encoding the subspace: 69 / 210\n",
      "Encoding the subspace: 70 / 210\n",
      "Encoding the subspace: 71 / 210\n",
      "Encoding the subspace: 72 / 210\n",
      "Encoding the subspace: 73 / 210\n",
      "Encoding the subspace: 74 / 210\n",
      "Encoding the subspace: 75 / 210\n",
      "Encoding the subspace: 76 / 210\n",
      "Encoding the subspace: 77 / 210\n",
      "Encoding the subspace: 78 / 210\n",
      "Encoding the subspace: 79 / 210\n",
      "Encoding the subspace: 80 / 210\n",
      "Encoding the subspace: 81 / 210\n",
      "Encoding the subspace: 82 / 210\n",
      "Encoding the subspace: 83 / 210\n",
      "Encoding the subspace: 84 / 210\n",
      "Encoding the subspace: 85 / 210\n",
      "Encoding the subspace: 86 / 210\n",
      "Encoding the subspace: 87 / 210\n",
      "Encoding the subspace: 88 / 210\n",
      "Encoding the subspace: 89 / 210\n",
      "Encoding the subspace: 90 / 210\n",
      "Encoding the subspace: 91 / 210\n",
      "Encoding the subspace: 92 / 210\n",
      "Encoding the subspace: 93 / 210\n",
      "Encoding the subspace: 94 / 210\n",
      "Encoding the subspace: 95 / 210\n",
      "Encoding the subspace: 96 / 210\n",
      "Encoding the subspace: 97 / 210\n",
      "Encoding the subspace: 98 / 210\n",
      "Encoding the subspace: 99 / 210\n",
      "Encoding the subspace: 100 / 210\n",
      "Encoding the subspace: 101 / 210\n",
      "Encoding the subspace: 102 / 210\n",
      "Encoding the subspace: 103 / 210\n",
      "Encoding the subspace: 104 / 210\n",
      "Encoding the subspace: 105 / 210\n",
      "Encoding the subspace: 106 / 210\n",
      "Encoding the subspace: 107 / 210\n",
      "Encoding the subspace: 108 / 210\n",
      "Encoding the subspace: 109 / 210\n",
      "Encoding the subspace: 110 / 210\n",
      "Encoding the subspace: 111 / 210\n",
      "Encoding the subspace: 112 / 210\n",
      "Encoding the subspace: 113 / 210\n",
      "Encoding the subspace: 114 / 210\n",
      "Encoding the subspace: 115 / 210\n",
      "Encoding the subspace: 116 / 210\n",
      "Encoding the subspace: 117 / 210\n",
      "Encoding the subspace: 118 / 210\n",
      "Encoding the subspace: 119 / 210\n",
      "Encoding the subspace: 120 / 210\n",
      "Encoding the subspace: 121 / 210\n",
      "Encoding the subspace: 122 / 210\n",
      "Encoding the subspace: 123 / 210\n",
      "Encoding the subspace: 124 / 210\n",
      "Encoding the subspace: 125 / 210\n",
      "Encoding the subspace: 126 / 210\n",
      "Encoding the subspace: 127 / 210\n",
      "Encoding the subspace: 128 / 210\n",
      "Encoding the subspace: 129 / 210\n",
      "Encoding the subspace: 130 / 210\n",
      "Encoding the subspace: 131 / 210\n",
      "Encoding the subspace: 132 / 210\n",
      "Encoding the subspace: 133 / 210\n",
      "Encoding the subspace: 134 / 210\n",
      "Encoding the subspace: 135 / 210\n",
      "Encoding the subspace: 136 / 210\n",
      "Encoding the subspace: 137 / 210\n",
      "Encoding the subspace: 138 / 210\n",
      "Encoding the subspace: 139 / 210\n",
      "Encoding the subspace: 140 / 210\n",
      "Encoding the subspace: 141 / 210\n",
      "Encoding the subspace: 142 / 210\n",
      "Encoding the subspace: 143 / 210\n",
      "Encoding the subspace: 144 / 210\n",
      "Encoding the subspace: 145 / 210\n",
      "Encoding the subspace: 146 / 210\n",
      "Encoding the subspace: 147 / 210\n",
      "Encoding the subspace: 148 / 210\n",
      "Encoding the subspace: 149 / 210\n",
      "Encoding the subspace: 150 / 210\n",
      "Encoding the subspace: 151 / 210\n",
      "Encoding the subspace: 152 / 210\n",
      "Encoding the subspace: 153 / 210\n",
      "Encoding the subspace: 154 / 210\n",
      "Encoding the subspace: 155 / 210\n",
      "Encoding the subspace: 156 / 210\n",
      "Encoding the subspace: 157 / 210\n",
      "Encoding the subspace: 158 / 210\n",
      "Encoding the subspace: 159 / 210\n",
      "Encoding the subspace: 160 / 210\n",
      "Encoding the subspace: 161 / 210\n",
      "Encoding the subspace: 162 / 210\n",
      "Encoding the subspace: 163 / 210\n",
      "Encoding the subspace: 164 / 210\n",
      "Encoding the subspace: 165 / 210\n",
      "Encoding the subspace: 166 / 210\n",
      "Encoding the subspace: 167 / 210\n",
      "Encoding the subspace: 168 / 210\n",
      "Encoding the subspace: 169 / 210\n",
      "Encoding the subspace: 170 / 210\n",
      "Encoding the subspace: 171 / 210\n",
      "Encoding the subspace: 172 / 210\n",
      "Encoding the subspace: 173 / 210\n",
      "Encoding the subspace: 174 / 210\n",
      "Encoding the subspace: 175 / 210\n",
      "Encoding the subspace: 176 / 210\n",
      "Encoding the subspace: 177 / 210\n",
      "Encoding the subspace: 178 / 210\n",
      "Encoding the subspace: 179 / 210\n",
      "Encoding the subspace: 180 / 210\n",
      "Encoding the subspace: 181 / 210\n",
      "Encoding the subspace: 182 / 210\n",
      "Encoding the subspace: 183 / 210\n",
      "Encoding the subspace: 184 / 210\n",
      "Encoding the subspace: 185 / 210\n",
      "Encoding the subspace: 186 / 210\n",
      "Encoding the subspace: 187 / 210\n",
      "Encoding the subspace: 188 / 210\n",
      "Encoding the subspace: 189 / 210\n",
      "Encoding the subspace: 190 / 210\n",
      "Encoding the subspace: 191 / 210\n",
      "Encoding the subspace: 192 / 210\n",
      "Encoding the subspace: 193 / 210\n",
      "Encoding the subspace: 194 / 210\n",
      "Encoding the subspace: 195 / 210\n",
      "Encoding the subspace: 196 / 210\n",
      "Encoding the subspace: 197 / 210\n",
      "Encoding the subspace: 198 / 210\n",
      "Encoding the subspace: 199 / 210\n",
      "Encoding the subspace: 200 / 210\n",
      "Encoding the subspace: 201 / 210\n",
      "Encoding the subspace: 202 / 210\n",
      "Encoding the subspace: 203 / 210\n",
      "Encoding the subspace: 204 / 210\n",
      "Encoding the subspace: 205 / 210\n",
      "Encoding the subspace: 206 / 210\n",
      "Encoding the subspace: 207 / 210\n",
      "Encoding the subspace: 208 / 210\n",
      "Encoding the subspace: 209 / 210\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=240, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.188462\n",
      "best loss:  0.18846167177738002\n",
      "Epoch 2 of 500, Train Loss: 0.038248\n",
      "best loss:  0.0382480302689301\n",
      "Epoch 3 of 500, Train Loss: 0.027288\n",
      "best loss:  0.027288330942229908\n",
      "Epoch 4 of 500, Train Loss: 0.022326\n",
      "best loss:  0.02232573481498633\n",
      "Epoch 5 of 500, Train Loss: 0.019084\n",
      "best loss:  0.019084140941069295\n",
      "Epoch 6 of 500, Train Loss: 0.016966\n",
      "best loss:  0.016965512381216814\n",
      "Epoch 7 of 500, Train Loss: 0.015421\n",
      "best loss:  0.015421015559018827\n",
      "Epoch 8 of 500, Train Loss: 0.014138\n",
      "best loss:  0.014138164303974833\n",
      "Epoch 9 of 500, Train Loss: 0.012974\n",
      "best loss:  0.012973819484134645\n",
      "Epoch 10 of 500, Train Loss: 0.012042\n",
      "best loss:  0.012042175675877695\n",
      "Epoch 11 of 500, Train Loss: 0.011239\n",
      "best loss:  0.01123908005610592\n",
      "Epoch 12 of 500, Train Loss: 0.010580\n",
      "best loss:  0.01057981487540456\n",
      "Epoch 13 of 500, Train Loss: 0.009896\n",
      "best loss:  0.00989597191993064\n",
      "Epoch 14 of 500, Train Loss: 0.009431\n",
      "best loss:  0.009431284193090314\n",
      "Epoch 15 of 500, Train Loss: 0.008967\n",
      "best loss:  0.00896653086418564\n",
      "Epoch 16 of 500, Train Loss: 0.008553\n",
      "best loss:  0.00855329843107593\n",
      "Epoch 17 of 500, Train Loss: 0.008159\n",
      "best loss:  0.008158549808088488\n",
      "Epoch 18 of 500, Train Loss: 0.007865\n",
      "best loss:  0.007865028676952329\n",
      "Epoch 19 of 500, Train Loss: 0.007573\n",
      "best loss:  0.007573311715616931\n",
      "Epoch 20 of 500, Train Loss: 0.007297\n",
      "best loss:  0.007297169449066837\n",
      "Epoch 21 of 500, Train Loss: 0.007073\n",
      "best loss:  0.007072667987559662\n",
      "Epoch 22 of 500, Train Loss: 0.006909\n",
      "best loss:  0.006909100917960308\n",
      "Epoch 23 of 500, Train Loss: 0.006757\n",
      "best loss:  0.006757495598032775\n",
      "Epoch 24 of 500, Train Loss: 0.006563\n",
      "best loss:  0.006563417774254218\n",
      "Epoch 25 of 500, Train Loss: 0.006450\n",
      "best loss:  0.0064495075128561015\n",
      "Epoch 26 of 500, Train Loss: 0.006282\n",
      "best loss:  0.006282476283918503\n",
      "Epoch 27 of 500, Train Loss: 0.006217\n",
      "best loss:  0.0062171361851776664\n",
      "Epoch 28 of 500, Train Loss: 0.006079\n",
      "best loss:  0.006078727668680994\n",
      "Epoch 29 of 500, Train Loss: 0.006057\n",
      "best loss:  0.006056846839726845\n",
      "Epoch 30 of 500, Train Loss: 0.005939\n",
      "best loss:  0.005938963523737422\n",
      "Epoch 31 of 500, Train Loss: 0.005922\n",
      "best loss:  0.005921873360914684\n",
      "Epoch 32 of 500, Train Loss: 0.005826\n",
      "best loss:  0.005825713594410705\n",
      "Epoch 33 of 500, Train Loss: 0.005818\n",
      "best loss:  0.005817762072930878\n",
      "Epoch 34 of 500, Train Loss: 0.005753\n",
      "best loss:  0.005753190316780571\n",
      "Epoch 35 of 500, Train Loss: 0.005706\n",
      "best loss:  0.005706472515242261\n",
      "Epoch 36 of 500, Train Loss: 0.005804\n",
      "Epoch 37 of 500, Train Loss: 0.005711\n",
      "Epoch 38 of 500, Train Loss: 0.006025\n",
      "Epoch 39 of 500, Train Loss: 0.005886\n",
      "Epoch 40 of 500, Train Loss: 0.006274\n",
      "Epoch 41 of 500, Train Loss: 0.005942\n",
      "Epoch 42 of 500, Train Loss: 0.005915\n",
      "Epoch 43 of 500, Train Loss: 0.005632\n",
      "best loss:  0.0056321922375625645\n",
      "Epoch 44 of 500, Train Loss: 0.005547\n",
      "best loss:  0.005547098355250913\n",
      "Epoch 45 of 500, Train Loss: 0.005387\n",
      "best loss:  0.0053867825472550535\n",
      "Epoch 46 of 500, Train Loss: 0.005224\n",
      "best loss:  0.005223939941483983\n",
      "Epoch 47 of 500, Train Loss: 0.005140\n",
      "best loss:  0.005139945192120847\n",
      "Epoch 48 of 500, Train Loss: 0.005043\n",
      "best loss:  0.005043336406023902\n",
      "Epoch 49 of 500, Train Loss: 0.004995\n",
      "best loss:  0.004995371541899517\n",
      "Epoch 50 of 500, Train Loss: 0.004882\n",
      "best loss:  0.00488223766009215\n",
      "Epoch 51 of 500, Train Loss: 0.004862\n",
      "best loss:  0.004862348451378373\n",
      "Epoch 52 of 500, Train Loss: 0.004763\n",
      "best loss:  0.004762841267148015\n",
      "Epoch 53 of 500, Train Loss: 0.004751\n",
      "best loss:  0.0047508553231122375\n",
      "Epoch 54 of 500, Train Loss: 0.004659\n",
      "best loss:  0.00465871260750519\n",
      "Epoch 55 of 500, Train Loss: 0.004640\n",
      "best loss:  0.004639832767743216\n",
      "Epoch 56 of 500, Train Loss: 0.004589\n",
      "best loss:  0.0045887200110375365\n",
      "Epoch 57 of 500, Train Loss: 0.004586\n",
      "best loss:  0.004585577324770532\n",
      "Epoch 58 of 500, Train Loss: 0.004529\n",
      "best loss:  0.0045289549106405326\n",
      "Epoch 59 of 500, Train Loss: 0.004529\n",
      "Epoch 60 of 500, Train Loss: 0.004502\n",
      "best loss:  0.004502164117995083\n",
      "Epoch 61 of 500, Train Loss: 0.004533\n",
      "Epoch 62 of 500, Train Loss: 0.004505\n",
      "Epoch 63 of 500, Train Loss: 0.004543\n",
      "Epoch 64 of 500, Train Loss: 0.004503\n",
      "Epoch 65 of 500, Train Loss: 0.004510\n",
      "Epoch 66 of 500, Train Loss: 0.004485\n",
      "best loss:  0.004484677707874343\n",
      "Epoch 67 of 500, Train Loss: 0.004479\n",
      "best loss:  0.004478533486605535\n",
      "Epoch 68 of 500, Train Loss: 0.004448\n",
      "best loss:  0.004448159528269465\n",
      "Epoch 69 of 500, Train Loss: 0.004414\n",
      "best loss:  0.004414165150662956\n",
      "Epoch 70 of 500, Train Loss: 0.004414\n",
      "best loss:  0.004413981326808792\n",
      "Epoch 71 of 500, Train Loss: 0.004371\n",
      "best loss:  0.004371282923481583\n",
      "Epoch 72 of 500, Train Loss: 0.004384\n",
      "Epoch 73 of 500, Train Loss: 0.004372\n",
      "Epoch 74 of 500, Train Loss: 0.004384\n",
      "Epoch 75 of 500, Train Loss: 0.004410\n",
      "Epoch 76 of 500, Train Loss: 0.004459\n",
      "Epoch 77 of 500, Train Loss: 0.004442\n",
      "Epoch 78 of 500, Train Loss: 0.004491\n",
      "Epoch 79 of 500, Train Loss: 0.004466\n",
      "Epoch 80 of 500, Train Loss: 0.004471\n",
      "Epoch 81 of 500, Train Loss: 0.004427\n",
      "Epoch 82 of 500, Train Loss: 0.004439\n",
      "Epoch 83 of 500, Train Loss: 0.004453\n",
      "Epoch 84 of 500, Train Loss: 0.004517\n",
      "Epoch 85 of 500, Train Loss: 0.004487\n",
      "Epoch 86 of 500, Train Loss: 0.004566\n",
      "Epoch 87 of 500, Train Loss: 0.004542\n",
      "Epoch 88 of 500, Train Loss: 0.004510\n",
      "Epoch 89 of 500, Train Loss: 0.004517\n",
      "Epoch 90 of 500, Train Loss: 0.004520\n",
      "Epoch 91 of 500, Train Loss: 0.004526\n",
      "Epoch 92 of 500, Train Loss: 0.004571\n",
      "Epoch 93 of 500, Train Loss: 0.004577\n",
      "Epoch 94 of 500, Train Loss: 0.004598\n",
      "Epoch 95 of 500, Train Loss: 0.004627\n",
      "Epoch 96 of 500, Train Loss: 0.004667\n",
      "Epoch 97 of 500, Train Loss: 0.004694\n",
      "Epoch 98 of 500, Train Loss: 0.004656\n",
      "Epoch 99 of 500, Train Loss: 0.004588\n",
      "Epoch 100 of 500, Train Loss: 0.004545\n",
      "Epoch 101 of 500, Train Loss: 0.004537\n",
      "Epoch 102 of 500, Train Loss: 0.004486\n",
      "Epoch 103 of 500, Train Loss: 0.004527\n",
      "Epoch 104 of 500, Train Loss: 0.004636\n",
      "Epoch 105 of 500, Train Loss: 0.004597\n",
      "Epoch 106 of 500, Train Loss: 0.004597\n",
      "Epoch 107 of 500, Train Loss: 0.004587\n",
      "Epoch 108 of 500, Train Loss: 0.004583\n",
      "Epoch 109 of 500, Train Loss: 0.004539\n",
      "Epoch 110 of 500, Train Loss: 0.004581\n",
      "Epoch 111 of 500, Train Loss: 0.004594\n",
      "Epoch 112 of 500, Train Loss: 0.004648\n",
      "Epoch 113 of 500, Train Loss: 0.004633\n",
      "Epoch 114 of 500, Train Loss: 0.004649\n",
      "Epoch 115 of 500, Train Loss: 0.004667\n",
      "Epoch 116 of 500, Train Loss: 0.004608\n",
      "Epoch 117 of 500, Train Loss: 0.004560\n",
      "Epoch 118 of 500, Train Loss: 0.004577\n",
      "Epoch 119 of 500, Train Loss: 0.004592\n",
      "Epoch 120 of 500, Train Loss: 0.004597\n",
      "Epoch 121 of 500, Train Loss: 0.004607\n",
      "Epoch 122 of 500, Train Loss: 0.004587\n",
      "Epoch 123 of 500, Train Loss: 0.004619\n",
      "Epoch 124 of 500, Train Loss: 0.004591\n",
      "Epoch 125 of 500, Train Loss: 0.004545\n",
      "Epoch 126 of 500, Train Loss: 0.004596\n",
      "Epoch 127 of 500, Train Loss: 0.004586\n",
      "Epoch 128 of 500, Train Loss: 0.004609\n",
      "Epoch 129 of 500, Train Loss: 0.004560\n",
      "Epoch 130 of 500, Train Loss: 0.004523\n",
      "Epoch 131 of 500, Train Loss: 0.004590\n",
      "Epoch 132 of 500, Train Loss: 0.004534\n",
      "Epoch 133 of 500, Train Loss: 0.004498\n",
      "Epoch 134 of 500, Train Loss: 0.004477\n",
      "Epoch 135 of 500, Train Loss: 0.004486\n",
      "Epoch 136 of 500, Train Loss: 0.004504\n",
      "Epoch 137 of 500, Train Loss: 0.004488\n",
      "Epoch 138 of 500, Train Loss: 0.004512\n",
      "Epoch 139 of 500, Train Loss: 0.004506\n",
      "Epoch 140 of 500, Train Loss: 0.004506\n",
      "Epoch 141 of 500, Train Loss: 0.004483\n",
      "Epoch 142 of 500, Train Loss: 0.004491\n",
      "Epoch 143 of 500, Train Loss: 0.004537\n",
      "Epoch 144 of 500, Train Loss: 0.004576\n",
      "Epoch 145 of 500, Train Loss: 0.004578\n",
      "Epoch 146 of 500, Train Loss: 0.004615\n",
      "Epoch 147 of 500, Train Loss: 0.004625\n",
      "Epoch 148 of 500, Train Loss: 0.004658\n",
      "Epoch 149 of 500, Train Loss: 0.004541\n",
      "Epoch 150 of 500, Train Loss: 0.004463\n",
      "Epoch 151 of 500, Train Loss: 0.004440\n",
      "Epoch 152 of 500, Train Loss: 0.004427\n",
      "Epoch 153 of 500, Train Loss: 0.004422\n",
      "Epoch 154 of 500, Train Loss: 0.004413\n",
      "Epoch 155 of 500, Train Loss: 0.004453\n",
      "Epoch 156 of 500, Train Loss: 0.004444\n",
      "Epoch 157 of 500, Train Loss: 0.004476\n",
      "Epoch 158 of 500, Train Loss: 0.004478\n",
      "Epoch 159 of 500, Train Loss: 0.004520\n",
      "Epoch 160 of 500, Train Loss: 0.004498\n",
      "Epoch 161 of 500, Train Loss: 0.004532\n",
      "Epoch 162 of 500, Train Loss: 0.004550\n",
      "Epoch 163 of 500, Train Loss: 0.004545\n",
      "Epoch 164 of 500, Train Loss: 0.004501\n",
      "Epoch 165 of 500, Train Loss: 0.004512\n",
      "Epoch 166 of 500, Train Loss: 0.004488\n",
      "Epoch 167 of 500, Train Loss: 0.004491\n",
      "Epoch 168 of 500, Train Loss: 0.004435\n",
      "Epoch 169 of 500, Train Loss: 0.004437\n",
      "Epoch 170 of 500, Train Loss: 0.004373\n",
      "Epoch 171 of 500, Train Loss: 0.004445\n",
      "Epoch 172 of 500, Train Loss: 0.004439\n",
      "Epoch 173 of 500, Train Loss: 0.004441\n",
      "Epoch 174 of 500, Train Loss: 0.004458\n",
      "Epoch 175 of 500, Train Loss: 0.004470\n",
      "Epoch 176 of 500, Train Loss: 0.004508\n",
      "Epoch 177 of 500, Train Loss: 0.004579\n",
      "Epoch 178 of 500, Train Loss: 0.004610\n",
      "Epoch 179 of 500, Train Loss: 0.004638\n",
      "Epoch 180 of 500, Train Loss: 0.004644\n",
      "Epoch 181 of 500, Train Loss: 0.004659\n",
      "Epoch 182 of 500, Train Loss: 0.004655\n",
      "Epoch 183 of 500, Train Loss: 0.004655\n",
      "Epoch 184 of 500, Train Loss: 0.004586\n",
      "Epoch 185 of 500, Train Loss: 0.004550\n",
      "Epoch 186 of 500, Train Loss: 0.004484\n",
      "Epoch 187 of 500, Train Loss: 0.004530\n",
      "Epoch 188 of 500, Train Loss: 0.004484\n",
      "Epoch 189 of 500, Train Loss: 0.004501\n",
      "Epoch 190 of 500, Train Loss: 0.004438\n",
      "Epoch 191 of 500, Train Loss: 0.004502\n",
      "Epoch 192 of 500, Train Loss: 0.004504\n",
      "Epoch 193 of 500, Train Loss: 0.004548\n",
      "Epoch 194 of 500, Train Loss: 0.004482\n",
      "Epoch 195 of 500, Train Loss: 0.004527\n",
      "Epoch 196 of 500, Train Loss: 0.004559\n",
      "Epoch 197 of 500, Train Loss: 0.004609\n",
      "Epoch 198 of 500, Train Loss: 0.004568\n",
      "Epoch 199 of 500, Train Loss: 0.004570\n",
      "Epoch 200 of 500, Train Loss: 0.004551\n",
      "Epoch 201 of 500, Train Loss: 0.004556\n",
      "Epoch 202 of 500, Train Loss: 0.004504\n",
      "Epoch 203 of 500, Train Loss: 0.004530\n",
      "Epoch 204 of 500, Train Loss: 0.004500\n",
      "Epoch 205 of 500, Train Loss: 0.004526\n",
      "Epoch 206 of 500, Train Loss: 0.004507\n",
      "Epoch 207 of 500, Train Loss: 0.004557\n",
      "Epoch 208 of 500, Train Loss: 0.004536\n",
      "Epoch 209 of 500, Train Loss: 0.004528\n",
      "Epoch 210 of 500, Train Loss: 0.004488\n",
      "Epoch 211 of 500, Train Loss: 0.004525\n",
      "Epoch 212 of 500, Train Loss: 0.004543\n",
      "Epoch 213 of 500, Train Loss: 0.004523\n",
      "Epoch 214 of 500, Train Loss: 0.004470\n",
      "Epoch 215 of 500, Train Loss: 0.004498\n",
      "Epoch 216 of 500, Train Loss: 0.004455\n",
      "Epoch 217 of 500, Train Loss: 0.004519\n",
      "Epoch 218 of 500, Train Loss: 0.004465\n",
      "Epoch 219 of 500, Train Loss: 0.004474\n",
      "Epoch 220 of 500, Train Loss: 0.004477\n",
      "Epoch 221 of 500, Train Loss: 0.004480\n",
      "Epoch 222 of 500, Train Loss: 0.004492\n",
      "Epoch 223 of 500, Train Loss: 0.004479\n",
      "Epoch 224 of 500, Train Loss: 0.004484\n",
      "Epoch 225 of 500, Train Loss: 0.004532\n",
      "Epoch 226 of 500, Train Loss: 0.004483\n",
      "Epoch 227 of 500, Train Loss: 0.004474\n",
      "Epoch 228 of 500, Train Loss: 0.004456\n",
      "Epoch 229 of 500, Train Loss: 0.004463\n",
      "Epoch 230 of 500, Train Loss: 0.004439\n",
      "Epoch 231 of 500, Train Loss: 0.004425\n",
      "Epoch 232 of 500, Train Loss: 0.004435\n",
      "Epoch 233 of 500, Train Loss: 0.004422\n",
      "Epoch 234 of 500, Train Loss: 0.004424\n",
      "Epoch 235 of 500, Train Loss: 0.004455\n",
      "Epoch 236 of 500, Train Loss: 0.004493\n",
      "Epoch 237 of 500, Train Loss: 0.004488\n",
      "Epoch 238 of 500, Train Loss: 0.004488\n",
      "Epoch 239 of 500, Train Loss: 0.004437\n",
      "Epoch 240 of 500, Train Loss: 0.004435\n",
      "Epoch 241 of 500, Train Loss: 0.004405\n",
      "Epoch 242 of 500, Train Loss: 0.004417\n",
      "Epoch 243 of 500, Train Loss: 0.004440\n",
      "Epoch 244 of 500, Train Loss: 0.004433\n",
      "Epoch 245 of 500, Train Loss: 0.004432\n",
      "Epoch 246 of 500, Train Loss: 0.004464\n",
      "Epoch 247 of 500, Train Loss: 0.004455\n",
      "Epoch 248 of 500, Train Loss: 0.004505\n",
      "Epoch 249 of 500, Train Loss: 0.004458\n",
      "Epoch 250 of 500, Train Loss: 0.004481\n",
      "Epoch 251 of 500, Train Loss: 0.004481\n",
      "Epoch 252 of 500, Train Loss: 0.004484\n",
      "Epoch 253 of 500, Train Loss: 0.004456\n",
      "Epoch 254 of 500, Train Loss: 0.004442\n",
      "Epoch 255 of 500, Train Loss: 0.004454\n",
      "Epoch 256 of 500, Train Loss: 0.004468\n",
      "Epoch 257 of 500, Train Loss: 0.004451\n",
      "Epoch 258 of 500, Train Loss: 0.004428\n",
      "Epoch 259 of 500, Train Loss: 0.004394\n",
      "Epoch 260 of 500, Train Loss: 0.004375\n",
      "Epoch 261 of 500, Train Loss: 0.004362\n",
      "best loss:  0.004361733560366448\n",
      "Epoch 262 of 500, Train Loss: 0.004375\n",
      "Epoch 263 of 500, Train Loss: 0.004377\n",
      "Epoch 264 of 500, Train Loss: 0.004399\n",
      "Epoch 265 of 500, Train Loss: 0.004412\n",
      "Epoch 266 of 500, Train Loss: 0.004448\n",
      "Epoch 267 of 500, Train Loss: 0.004433\n",
      "Epoch 268 of 500, Train Loss: 0.004450\n",
      "Epoch 269 of 500, Train Loss: 0.004423\n",
      "Epoch 270 of 500, Train Loss: 0.004473\n",
      "Epoch 271 of 500, Train Loss: 0.004443\n",
      "Epoch 272 of 500, Train Loss: 0.004464\n",
      "Epoch 273 of 500, Train Loss: 0.004454\n",
      "Epoch 274 of 500, Train Loss: 0.004462\n",
      "Epoch 275 of 500, Train Loss: 0.004438\n",
      "Epoch 276 of 500, Train Loss: 0.004439\n",
      "Epoch 277 of 500, Train Loss: 0.004455\n",
      "Epoch 278 of 500, Train Loss: 0.004454\n",
      "Epoch 279 of 500, Train Loss: 0.004446\n",
      "Epoch 280 of 500, Train Loss: 0.004452\n",
      "Epoch 281 of 500, Train Loss: 0.004425\n",
      "Epoch 282 of 500, Train Loss: 0.004421\n",
      "Epoch 283 of 500, Train Loss: 0.004416\n",
      "Epoch 284 of 500, Train Loss: 0.004423\n",
      "Epoch 285 of 500, Train Loss: 0.004401\n",
      "Epoch 286 of 500, Train Loss: 0.004395\n",
      "Epoch 287 of 500, Train Loss: 0.004435\n",
      "Epoch 288 of 500, Train Loss: 0.004423\n",
      "Epoch 289 of 500, Train Loss: 0.004424\n",
      "Epoch 290 of 500, Train Loss: 0.004430\n",
      "Epoch 291 of 500, Train Loss: 0.004413\n",
      "Epoch 292 of 500, Train Loss: 0.004423\n",
      "Epoch 293 of 500, Train Loss: 0.004456\n",
      "Epoch 294 of 500, Train Loss: 0.004466\n",
      "Epoch 295 of 500, Train Loss: 0.004433\n",
      "Epoch 296 of 500, Train Loss: 0.004421\n",
      "Epoch 297 of 500, Train Loss: 0.004415\n",
      "Epoch 298 of 500, Train Loss: 0.004407\n",
      "Epoch 299 of 500, Train Loss: 0.004410\n",
      "Epoch 300 of 500, Train Loss: 0.004426\n",
      "Epoch 301 of 500, Train Loss: 0.004416\n",
      "Epoch 302 of 500, Train Loss: 0.004420\n",
      "Epoch 303 of 500, Train Loss: 0.004408\n",
      "Epoch 304 of 500, Train Loss: 0.004441\n",
      "Epoch 305 of 500, Train Loss: 0.004451\n",
      "Epoch 306 of 500, Train Loss: 0.004431\n",
      "Epoch 307 of 500, Train Loss: 0.004428\n",
      "Epoch 308 of 500, Train Loss: 0.004425\n",
      "Epoch 309 of 500, Train Loss: 0.004399\n",
      "Epoch 310 of 500, Train Loss: 0.004393\n",
      "Epoch 311 of 500, Train Loss: 0.004371\n",
      "Epoch 312 of 500, Train Loss: 0.004383\n",
      "Epoch 313 of 500, Train Loss: 0.004410\n",
      "Epoch 314 of 500, Train Loss: 0.004435\n",
      "Epoch 315 of 500, Train Loss: 0.004448\n",
      "Epoch 316 of 500, Train Loss: 0.004413\n",
      "Epoch 317 of 500, Train Loss: 0.004399\n",
      "Epoch 318 of 500, Train Loss: 0.004431\n",
      "Epoch 319 of 500, Train Loss: 0.004450\n",
      "Epoch 320 of 500, Train Loss: 0.004418\n",
      "Epoch 321 of 500, Train Loss: 0.004414\n",
      "Epoch 322 of 500, Train Loss: 0.004476\n",
      "Epoch 323 of 500, Train Loss: 0.004530\n",
      "Epoch 324 of 500, Train Loss: 0.004483\n",
      "Epoch 325 of 500, Train Loss: 0.004465\n",
      "Epoch 326 of 500, Train Loss: 0.004433\n",
      "Epoch 327 of 500, Train Loss: 0.004455\n",
      "Epoch 328 of 500, Train Loss: 0.004427\n",
      "Epoch 329 of 500, Train Loss: 0.004438\n",
      "Epoch 330 of 500, Train Loss: 0.004407\n",
      "Epoch 331 of 500, Train Loss: 0.004444\n",
      "Epoch 332 of 500, Train Loss: 0.004454\n",
      "Epoch 333 of 500, Train Loss: 0.004457\n",
      "Epoch 334 of 500, Train Loss: 0.004405\n",
      "Epoch 335 of 500, Train Loss: 0.004414\n",
      "Epoch 336 of 500, Train Loss: 0.004401\n",
      "Epoch 337 of 500, Train Loss: 0.004439\n",
      "Epoch 338 of 500, Train Loss: 0.004449\n",
      "Epoch 339 of 500, Train Loss: 0.004460\n",
      "Epoch 340 of 500, Train Loss: 0.004423\n",
      "Epoch 341 of 500, Train Loss: 0.004411\n",
      "Epoch 342 of 500, Train Loss: 0.004406\n",
      "Epoch 343 of 500, Train Loss: 0.004441\n",
      "Epoch 344 of 500, Train Loss: 0.004395\n",
      "Epoch 345 of 500, Train Loss: 0.004471\n",
      "Epoch 346 of 500, Train Loss: 0.004480\n",
      "Epoch 347 of 500, Train Loss: 0.004476\n",
      "Epoch 348 of 500, Train Loss: 0.004468\n",
      "Epoch 349 of 500, Train Loss: 0.004476\n",
      "Epoch 350 of 500, Train Loss: 0.004544\n",
      "Epoch 351 of 500, Train Loss: 0.004530\n",
      "Epoch 352 of 500, Train Loss: 0.004511\n",
      "Epoch 353 of 500, Train Loss: 0.004539\n",
      "Epoch 354 of 500, Train Loss: 0.004492\n",
      "Epoch 355 of 500, Train Loss: 0.004473\n",
      "Epoch 356 of 500, Train Loss: 0.004438\n",
      "Epoch 357 of 500, Train Loss: 0.004466\n",
      "Epoch 358 of 500, Train Loss: 0.004450\n",
      "Epoch 359 of 500, Train Loss: 0.004436\n",
      "Epoch 360 of 500, Train Loss: 0.004385\n",
      "Epoch 361 of 500, Train Loss: 0.004413\n",
      "Epoch 362 of 500, Train Loss: 0.004420\n",
      "Epoch 363 of 500, Train Loss: 0.004404\n",
      "Epoch 364 of 500, Train Loss: 0.004382\n",
      "Epoch 365 of 500, Train Loss: 0.004445\n",
      "Epoch 366 of 500, Train Loss: 0.004426\n",
      "Epoch 367 of 500, Train Loss: 0.004450\n",
      "Epoch 368 of 500, Train Loss: 0.004413\n",
      "Epoch 369 of 500, Train Loss: 0.004444\n",
      "Epoch 370 of 500, Train Loss: 0.004460\n",
      "Epoch 371 of 500, Train Loss: 0.004463\n",
      "Epoch 372 of 500, Train Loss: 0.004398\n",
      "Epoch 373 of 500, Train Loss: 0.004426\n",
      "Epoch 374 of 500, Train Loss: 0.004440\n",
      "Epoch 375 of 500, Train Loss: 0.004517\n",
      "Epoch 376 of 500, Train Loss: 0.004449\n",
      "Epoch 377 of 500, Train Loss: 0.004525\n",
      "Epoch 378 of 500, Train Loss: 0.004471\n",
      "Epoch 379 of 500, Train Loss: 0.004498\n",
      "Epoch 380 of 500, Train Loss: 0.004409\n",
      "Epoch 381 of 500, Train Loss: 0.004460\n",
      "Epoch 382 of 500, Train Loss: 0.004462\n",
      "Epoch 383 of 500, Train Loss: 0.004527\n",
      "Epoch 384 of 500, Train Loss: 0.004513\n",
      "Epoch 385 of 500, Train Loss: 0.004505\n",
      "Epoch 386 of 500, Train Loss: 0.004482\n",
      "Epoch 387 of 500, Train Loss: 0.004499\n",
      "Epoch 388 of 500, Train Loss: 0.004400\n",
      "Epoch 389 of 500, Train Loss: 0.004411\n",
      "Epoch 390 of 500, Train Loss: 0.004376\n",
      "Epoch 391 of 500, Train Loss: 0.004437\n",
      "Epoch 392 of 500, Train Loss: 0.004432\n",
      "Epoch 393 of 500, Train Loss: 0.004513\n",
      "Epoch 394 of 500, Train Loss: 0.004512\n",
      "Epoch 395 of 500, Train Loss: 0.004548\n",
      "Epoch 396 of 500, Train Loss: 0.004551\n",
      "Epoch 397 of 500, Train Loss: 0.004578\n",
      "Epoch 398 of 500, Train Loss: 0.004516\n",
      "Epoch 399 of 500, Train Loss: 0.004558\n",
      "Epoch 400 of 500, Train Loss: 0.004497\n",
      "Epoch 401 of 500, Train Loss: 0.004493\n",
      "Epoch 402 of 500, Train Loss: 0.004473\n",
      "Epoch 403 of 500, Train Loss: 0.004521\n",
      "Epoch 404 of 500, Train Loss: 0.004504\n",
      "Epoch 405 of 500, Train Loss: 0.004521\n",
      "Epoch 406 of 500, Train Loss: 0.004435\n",
      "Epoch 407 of 500, Train Loss: 0.004435\n",
      "Epoch 408 of 500, Train Loss: 0.004398\n",
      "Epoch 409 of 500, Train Loss: 0.004516\n",
      "Epoch 410 of 500, Train Loss: 0.004488\n",
      "Epoch 411 of 500, Train Loss: 0.004535\n",
      "Epoch 412 of 500, Train Loss: 0.004515\n",
      "Epoch 413 of 500, Train Loss: 0.004538\n",
      "Epoch 414 of 500, Train Loss: 0.004466\n",
      "Epoch 415 of 500, Train Loss: 0.004468\n",
      "Epoch 416 of 500, Train Loss: 0.004415\n",
      "Epoch 417 of 500, Train Loss: 0.004500\n",
      "Epoch 418 of 500, Train Loss: 0.004443\n",
      "Epoch 419 of 500, Train Loss: 0.004500\n",
      "Epoch 420 of 500, Train Loss: 0.004480\n",
      "Epoch 421 of 500, Train Loss: 0.004491\n",
      "Epoch 422 of 500, Train Loss: 0.004464\n",
      "Epoch 423 of 500, Train Loss: 0.004509\n",
      "Epoch 424 of 500, Train Loss: 0.004471\n",
      "Epoch 425 of 500, Train Loss: 0.004511\n",
      "Epoch 426 of 500, Train Loss: 0.004484\n",
      "Epoch 427 of 500, Train Loss: 0.004519\n",
      "Epoch 428 of 500, Train Loss: 0.004514\n",
      "Epoch 429 of 500, Train Loss: 0.004529\n",
      "Epoch 430 of 500, Train Loss: 0.004453\n",
      "Epoch 431 of 500, Train Loss: 0.004501\n",
      "Epoch 432 of 500, Train Loss: 0.004464\n",
      "Epoch 433 of 500, Train Loss: 0.004496\n",
      "Epoch 434 of 500, Train Loss: 0.004471\n",
      "Epoch 435 of 500, Train Loss: 0.004494\n",
      "Epoch 436 of 500, Train Loss: 0.004469\n",
      "Epoch 437 of 500, Train Loss: 0.004485\n",
      "Epoch 438 of 500, Train Loss: 0.004427\n",
      "Epoch 439 of 500, Train Loss: 0.004491\n",
      "Epoch 440 of 500, Train Loss: 0.004433\n",
      "Epoch 441 of 500, Train Loss: 0.004485\n",
      "Epoch 442 of 500, Train Loss: 0.004438\n",
      "Epoch 443 of 500, Train Loss: 0.004494\n",
      "Epoch 444 of 500, Train Loss: 0.004459\n",
      "Epoch 445 of 500, Train Loss: 0.004509\n",
      "Epoch 446 of 500, Train Loss: 0.004500\n",
      "Epoch 447 of 500, Train Loss: 0.004603\n",
      "Epoch 448 of 500, Train Loss: 0.004564\n",
      "Epoch 449 of 500, Train Loss: 0.004579\n",
      "Epoch 450 of 500, Train Loss: 0.004475\n",
      "Epoch 451 of 500, Train Loss: 0.004477\n",
      "Epoch 452 of 500, Train Loss: 0.004434\n",
      "Epoch 453 of 500, Train Loss: 0.004483\n",
      "Epoch 454 of 500, Train Loss: 0.004485\n",
      "Epoch 455 of 500, Train Loss: 0.004526\n",
      "Epoch 456 of 500, Train Loss: 0.004479\n",
      "Epoch 457 of 500, Train Loss: 0.004518\n",
      "Epoch 458 of 500, Train Loss: 0.004503\n",
      "Epoch 459 of 500, Train Loss: 0.004592\n",
      "Epoch 460 of 500, Train Loss: 0.004542\n",
      "Epoch 461 of 500, Train Loss: 0.004541\n",
      "Epoch 462 of 500, Train Loss: 0.004489\n",
      "Epoch 463 of 500, Train Loss: 0.004509\n",
      "Epoch 464 of 500, Train Loss: 0.004482\n",
      "Epoch 465 of 500, Train Loss: 0.004553\n",
      "Epoch 466 of 500, Train Loss: 0.004518\n",
      "Epoch 467 of 500, Train Loss: 0.004544\n",
      "Epoch 468 of 500, Train Loss: 0.004519\n",
      "Epoch 469 of 500, Train Loss: 0.004541\n",
      "Epoch 470 of 500, Train Loss: 0.004511\n",
      "Epoch 471 of 500, Train Loss: 0.004569\n",
      "Epoch 472 of 500, Train Loss: 0.004513\n",
      "Epoch 473 of 500, Train Loss: 0.004545\n",
      "Epoch 474 of 500, Train Loss: 0.004508\n",
      "Epoch 475 of 500, Train Loss: 0.004540\n",
      "Epoch 476 of 500, Train Loss: 0.004513\n",
      "Epoch 477 of 500, Train Loss: 0.004539\n",
      "Epoch 478 of 500, Train Loss: 0.004538\n",
      "Epoch 479 of 500, Train Loss: 0.004575\n",
      "Epoch 480 of 500, Train Loss: 0.004540\n",
      "Epoch 481 of 500, Train Loss: 0.004553\n",
      "Epoch 482 of 500, Train Loss: 0.004489\n",
      "Epoch 483 of 500, Train Loss: 0.004557\n",
      "Epoch 484 of 500, Train Loss: 0.004534\n",
      "Epoch 485 of 500, Train Loss: 0.004582\n",
      "Epoch 486 of 500, Train Loss: 0.004527\n",
      "Epoch 487 of 500, Train Loss: 0.004511\n",
      "Epoch 488 of 500, Train Loss: 0.004452\n",
      "Epoch 489 of 500, Train Loss: 0.004487\n",
      "Epoch 490 of 500, Train Loss: 0.004498\n",
      "Epoch 491 of 500, Train Loss: 0.004485\n",
      "Epoch 492 of 500, Train Loss: 0.004493\n",
      "Epoch 493 of 500, Train Loss: 0.004556\n",
      "Epoch 494 of 500, Train Loss: 0.004561\n",
      "Epoch 495 of 500, Train Loss: 0.004609\n",
      "Epoch 496 of 500, Train Loss: 0.004608\n",
      "Epoch 497 of 500, Train Loss: 0.004620\n",
      "Epoch 498 of 500, Train Loss: 0.004521\n",
      "Epoch 499 of 500, Train Loss: 0.004535\n",
      "Epoch 500 of 500, Train Loss: 0.004535\n",
      "latent train shape:  (16395, 240)\n",
      "M: 240, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 50\n",
      "Training the subspace: 0 / 240\n",
      "Training the subspace: 1 / 240\n",
      "Training the subspace: 2 / 240\n",
      "Training the subspace: 3 / 240\n",
      "Training the subspace: 4 / 240\n",
      "Training the subspace: 5 / 240\n",
      "Training the subspace: 6 / 240\n",
      "Training the subspace: 7 / 240\n",
      "Training the subspace: 8 / 240\n",
      "Training the subspace: 9 / 240\n",
      "Training the subspace: 10 / 240\n",
      "Training the subspace: 11 / 240\n",
      "Training the subspace: 12 / 240\n",
      "Training the subspace: 13 / 240\n",
      "Training the subspace: 14 / 240\n",
      "Training the subspace: 15 / 240\n",
      "Training the subspace: 16 / 240\n",
      "Training the subspace: 17 / 240\n",
      "Training the subspace: 18 / 240\n",
      "Training the subspace: 19 / 240\n",
      "Training the subspace: 20 / 240\n",
      "Training the subspace: 21 / 240\n",
      "Training the subspace: 22 / 240\n",
      "Training the subspace: 23 / 240\n",
      "Training the subspace: 24 / 240\n",
      "Training the subspace: 25 / 240\n",
      "Training the subspace: 26 / 240\n",
      "Training the subspace: 27 / 240\n",
      "Training the subspace: 28 / 240\n",
      "Training the subspace: 29 / 240\n",
      "Training the subspace: 30 / 240\n",
      "Training the subspace: 31 / 240\n",
      "Training the subspace: 32 / 240\n",
      "Training the subspace: 33 / 240\n",
      "Training the subspace: 34 / 240\n",
      "Training the subspace: 35 / 240\n",
      "Training the subspace: 36 / 240\n",
      "Training the subspace: 37 / 240\n",
      "Training the subspace: 38 / 240\n",
      "Training the subspace: 39 / 240\n",
      "Training the subspace: 40 / 240\n",
      "Training the subspace: 41 / 240\n",
      "Training the subspace: 42 / 240\n",
      "Training the subspace: 43 / 240\n",
      "Training the subspace: 44 / 240\n",
      "Training the subspace: 45 / 240\n",
      "Training the subspace: 46 / 240\n",
      "Training the subspace: 47 / 240\n",
      "Training the subspace: 48 / 240\n",
      "Training the subspace: 49 / 240\n",
      "Training the subspace: 50 / 240\n",
      "Training the subspace: 51 / 240\n",
      "Training the subspace: 52 / 240\n",
      "Training the subspace: 53 / 240\n",
      "Training the subspace: 54 / 240\n",
      "Training the subspace: 55 / 240\n",
      "Training the subspace: 56 / 240\n",
      "Training the subspace: 57 / 240\n",
      "Training the subspace: 58 / 240\n",
      "Training the subspace: 59 / 240\n",
      "Training the subspace: 60 / 240\n",
      "Training the subspace: 61 / 240\n",
      "Training the subspace: 62 / 240\n",
      "Training the subspace: 63 / 240\n",
      "Training the subspace: 64 / 240\n",
      "Training the subspace: 65 / 240\n",
      "Training the subspace: 66 / 240\n",
      "Training the subspace: 67 / 240\n",
      "Training the subspace: 68 / 240\n",
      "Training the subspace: 69 / 240\n",
      "Training the subspace: 70 / 240\n",
      "Training the subspace: 71 / 240\n",
      "Training the subspace: 72 / 240\n",
      "Training the subspace: 73 / 240\n",
      "Training the subspace: 74 / 240\n",
      "Training the subspace: 75 / 240\n",
      "Training the subspace: 76 / 240\n",
      "Training the subspace: 77 / 240\n",
      "Training the subspace: 78 / 240\n",
      "Training the subspace: 79 / 240\n",
      "Training the subspace: 80 / 240\n",
      "Training the subspace: 81 / 240\n",
      "Training the subspace: 82 / 240\n",
      "Training the subspace: 83 / 240\n",
      "Training the subspace: 84 / 240\n",
      "Training the subspace: 85 / 240\n",
      "Training the subspace: 86 / 240\n",
      "Training the subspace: 87 / 240\n",
      "Training the subspace: 88 / 240\n",
      "Training the subspace: 89 / 240\n",
      "Training the subspace: 90 / 240\n",
      "Training the subspace: 91 / 240\n",
      "Training the subspace: 92 / 240\n",
      "Training the subspace: 93 / 240\n",
      "Training the subspace: 94 / 240\n",
      "Training the subspace: 95 / 240\n",
      "Training the subspace: 96 / 240\n",
      "Training the subspace: 97 / 240\n",
      "Training the subspace: 98 / 240\n",
      "Training the subspace: 99 / 240\n",
      "Training the subspace: 100 / 240\n",
      "Training the subspace: 101 / 240\n",
      "Training the subspace: 102 / 240\n",
      "Training the subspace: 103 / 240\n",
      "Training the subspace: 104 / 240\n",
      "Training the subspace: 105 / 240\n",
      "Training the subspace: 106 / 240\n",
      "Training the subspace: 107 / 240\n",
      "Training the subspace: 108 / 240\n",
      "Training the subspace: 109 / 240\n",
      "Training the subspace: 110 / 240\n",
      "Training the subspace: 111 / 240\n",
      "Training the subspace: 112 / 240\n",
      "Training the subspace: 113 / 240\n",
      "Training the subspace: 114 / 240\n",
      "Training the subspace: 115 / 240\n",
      "Training the subspace: 116 / 240\n",
      "Training the subspace: 117 / 240\n",
      "Training the subspace: 118 / 240\n",
      "Training the subspace: 119 / 240\n",
      "Training the subspace: 120 / 240\n",
      "Training the subspace: 121 / 240\n",
      "Training the subspace: 122 / 240\n",
      "Training the subspace: 123 / 240\n",
      "Training the subspace: 124 / 240\n",
      "Training the subspace: 125 / 240\n",
      "Training the subspace: 126 / 240\n",
      "Training the subspace: 127 / 240\n",
      "Training the subspace: 128 / 240\n",
      "Training the subspace: 129 / 240\n",
      "Training the subspace: 130 / 240\n",
      "Training the subspace: 131 / 240\n",
      "Training the subspace: 132 / 240\n",
      "Training the subspace: 133 / 240\n",
      "Training the subspace: 134 / 240\n",
      "Training the subspace: 135 / 240\n",
      "Training the subspace: 136 / 240\n",
      "Training the subspace: 137 / 240\n",
      "Training the subspace: 138 / 240\n",
      "Training the subspace: 139 / 240\n",
      "Training the subspace: 140 / 240\n",
      "Training the subspace: 141 / 240\n",
      "Training the subspace: 142 / 240\n",
      "Training the subspace: 143 / 240\n",
      "Training the subspace: 144 / 240\n",
      "Training the subspace: 145 / 240\n",
      "Training the subspace: 146 / 240\n",
      "Training the subspace: 147 / 240\n",
      "Training the subspace: 148 / 240\n",
      "Training the subspace: 149 / 240\n",
      "Training the subspace: 150 / 240\n",
      "Training the subspace: 151 / 240\n",
      "Training the subspace: 152 / 240\n",
      "Training the subspace: 153 / 240\n",
      "Training the subspace: 154 / 240\n",
      "Training the subspace: 155 / 240\n",
      "Training the subspace: 156 / 240\n",
      "Training the subspace: 157 / 240\n",
      "Training the subspace: 158 / 240\n",
      "Training the subspace: 159 / 240\n",
      "Training the subspace: 160 / 240\n",
      "Training the subspace: 161 / 240\n",
      "Training the subspace: 162 / 240\n",
      "Training the subspace: 163 / 240\n",
      "Training the subspace: 164 / 240\n",
      "Training the subspace: 165 / 240\n",
      "Training the subspace: 166 / 240\n",
      "Training the subspace: 167 / 240\n",
      "Training the subspace: 168 / 240\n",
      "Training the subspace: 169 / 240\n",
      "Training the subspace: 170 / 240\n",
      "Training the subspace: 171 / 240\n",
      "Training the subspace: 172 / 240\n",
      "Training the subspace: 173 / 240\n",
      "Training the subspace: 174 / 240\n",
      "Training the subspace: 175 / 240\n",
      "Training the subspace: 176 / 240\n",
      "Training the subspace: 177 / 240\n",
      "Training the subspace: 178 / 240\n",
      "Training the subspace: 179 / 240\n",
      "Training the subspace: 180 / 240\n",
      "Training the subspace: 181 / 240\n",
      "Training the subspace: 182 / 240\n",
      "Training the subspace: 183 / 240\n",
      "Training the subspace: 184 / 240\n",
      "Training the subspace: 185 / 240\n",
      "Training the subspace: 186 / 240\n",
      "Training the subspace: 187 / 240\n",
      "Training the subspace: 188 / 240\n",
      "Training the subspace: 189 / 240\n",
      "Training the subspace: 190 / 240\n",
      "Training the subspace: 191 / 240\n",
      "Training the subspace: 192 / 240\n",
      "Training the subspace: 193 / 240\n",
      "Training the subspace: 194 / 240\n",
      "Training the subspace: 195 / 240\n",
      "Training the subspace: 196 / 240\n",
      "Training the subspace: 197 / 240\n",
      "Training the subspace: 198 / 240\n",
      "Training the subspace: 199 / 240\n",
      "Training the subspace: 200 / 240\n",
      "Training the subspace: 201 / 240\n",
      "Training the subspace: 202 / 240\n",
      "Training the subspace: 203 / 240\n",
      "Training the subspace: 204 / 240\n",
      "Training the subspace: 205 / 240\n",
      "Training the subspace: 206 / 240\n",
      "Training the subspace: 207 / 240\n",
      "Training the subspace: 208 / 240\n",
      "Training the subspace: 209 / 240\n",
      "Training the subspace: 210 / 240\n",
      "Training the subspace: 211 / 240\n",
      "Training the subspace: 212 / 240\n",
      "Training the subspace: 213 / 240\n",
      "Training the subspace: 214 / 240\n",
      "Training the subspace: 215 / 240\n",
      "Training the subspace: 216 / 240\n",
      "Training the subspace: 217 / 240\n",
      "Training the subspace: 218 / 240\n",
      "Training the subspace: 219 / 240\n",
      "Training the subspace: 220 / 240\n",
      "Training the subspace: 221 / 240\n",
      "Training the subspace: 222 / 240\n",
      "Training the subspace: 223 / 240\n",
      "Training the subspace: 224 / 240\n",
      "Training the subspace: 225 / 240\n",
      "Training the subspace: 226 / 240\n",
      "Training the subspace: 227 / 240\n",
      "Training the subspace: 228 / 240\n",
      "Training the subspace: 229 / 240\n",
      "Training the subspace: 230 / 240\n",
      "Training the subspace: 231 / 240\n",
      "Training the subspace: 232 / 240\n",
      "Training the subspace: 233 / 240\n",
      "Training the subspace: 234 / 240\n",
      "Training the subspace: 235 / 240\n",
      "Training the subspace: 236 / 240\n",
      "Training the subspace: 237 / 240\n",
      "Training the subspace: 238 / 240\n",
      "Training the subspace: 239 / 240\n",
      "Encoding the subspace: 0 / 240\n",
      "Encoding the subspace: 1 / 240\n",
      "Encoding the subspace: 2 / 240\n",
      "Encoding the subspace: 3 / 240\n",
      "Encoding the subspace: 4 / 240\n",
      "Encoding the subspace: 5 / 240\n",
      "Encoding the subspace: 6 / 240\n",
      "Encoding the subspace: 7 / 240\n",
      "Encoding the subspace: 8 / 240\n",
      "Encoding the subspace: 9 / 240\n",
      "Encoding the subspace: 10 / 240\n",
      "Encoding the subspace: 11 / 240\n",
      "Encoding the subspace: 12 / 240\n",
      "Encoding the subspace: 13 / 240\n",
      "Encoding the subspace: 14 / 240\n",
      "Encoding the subspace: 15 / 240\n",
      "Encoding the subspace: 16 / 240\n",
      "Encoding the subspace: 17 / 240\n",
      "Encoding the subspace: 18 / 240\n",
      "Encoding the subspace: 19 / 240\n",
      "Encoding the subspace: 20 / 240\n",
      "Encoding the subspace: 21 / 240\n",
      "Encoding the subspace: 22 / 240\n",
      "Encoding the subspace: 23 / 240\n",
      "Encoding the subspace: 24 / 240\n",
      "Encoding the subspace: 25 / 240\n",
      "Encoding the subspace: 26 / 240\n",
      "Encoding the subspace: 27 / 240\n",
      "Encoding the subspace: 28 / 240\n",
      "Encoding the subspace: 29 / 240\n",
      "Encoding the subspace: 30 / 240\n",
      "Encoding the subspace: 31 / 240\n",
      "Encoding the subspace: 32 / 240\n",
      "Encoding the subspace: 33 / 240\n",
      "Encoding the subspace: 34 / 240\n",
      "Encoding the subspace: 35 / 240\n",
      "Encoding the subspace: 36 / 240\n",
      "Encoding the subspace: 37 / 240\n",
      "Encoding the subspace: 38 / 240\n",
      "Encoding the subspace: 39 / 240\n",
      "Encoding the subspace: 40 / 240\n",
      "Encoding the subspace: 41 / 240\n",
      "Encoding the subspace: 42 / 240\n",
      "Encoding the subspace: 43 / 240\n",
      "Encoding the subspace: 44 / 240\n",
      "Encoding the subspace: 45 / 240\n",
      "Encoding the subspace: 46 / 240\n",
      "Encoding the subspace: 47 / 240\n",
      "Encoding the subspace: 48 / 240\n",
      "Encoding the subspace: 49 / 240\n",
      "Encoding the subspace: 50 / 240\n",
      "Encoding the subspace: 51 / 240\n",
      "Encoding the subspace: 52 / 240\n",
      "Encoding the subspace: 53 / 240\n",
      "Encoding the subspace: 54 / 240\n",
      "Encoding the subspace: 55 / 240\n",
      "Encoding the subspace: 56 / 240\n",
      "Encoding the subspace: 57 / 240\n",
      "Encoding the subspace: 58 / 240\n",
      "Encoding the subspace: 59 / 240\n",
      "Encoding the subspace: 60 / 240\n",
      "Encoding the subspace: 61 / 240\n",
      "Encoding the subspace: 62 / 240\n",
      "Encoding the subspace: 63 / 240\n",
      "Encoding the subspace: 64 / 240\n",
      "Encoding the subspace: 65 / 240\n",
      "Encoding the subspace: 66 / 240\n",
      "Encoding the subspace: 67 / 240\n",
      "Encoding the subspace: 68 / 240\n",
      "Encoding the subspace: 69 / 240\n",
      "Encoding the subspace: 70 / 240\n",
      "Encoding the subspace: 71 / 240\n",
      "Encoding the subspace: 72 / 240\n",
      "Encoding the subspace: 73 / 240\n",
      "Encoding the subspace: 74 / 240\n",
      "Encoding the subspace: 75 / 240\n",
      "Encoding the subspace: 76 / 240\n",
      "Encoding the subspace: 77 / 240\n",
      "Encoding the subspace: 78 / 240\n",
      "Encoding the subspace: 79 / 240\n",
      "Encoding the subspace: 80 / 240\n",
      "Encoding the subspace: 81 / 240\n",
      "Encoding the subspace: 82 / 240\n",
      "Encoding the subspace: 83 / 240\n",
      "Encoding the subspace: 84 / 240\n",
      "Encoding the subspace: 85 / 240\n",
      "Encoding the subspace: 86 / 240\n",
      "Encoding the subspace: 87 / 240\n",
      "Encoding the subspace: 88 / 240\n",
      "Encoding the subspace: 89 / 240\n",
      "Encoding the subspace: 90 / 240\n",
      "Encoding the subspace: 91 / 240\n",
      "Encoding the subspace: 92 / 240\n",
      "Encoding the subspace: 93 / 240\n",
      "Encoding the subspace: 94 / 240\n",
      "Encoding the subspace: 95 / 240\n",
      "Encoding the subspace: 96 / 240\n",
      "Encoding the subspace: 97 / 240\n",
      "Encoding the subspace: 98 / 240\n",
      "Encoding the subspace: 99 / 240\n",
      "Encoding the subspace: 100 / 240\n",
      "Encoding the subspace: 101 / 240\n",
      "Encoding the subspace: 102 / 240\n",
      "Encoding the subspace: 103 / 240\n",
      "Encoding the subspace: 104 / 240\n",
      "Encoding the subspace: 105 / 240\n",
      "Encoding the subspace: 106 / 240\n",
      "Encoding the subspace: 107 / 240\n",
      "Encoding the subspace: 108 / 240\n",
      "Encoding the subspace: 109 / 240\n",
      "Encoding the subspace: 110 / 240\n",
      "Encoding the subspace: 111 / 240\n",
      "Encoding the subspace: 112 / 240\n",
      "Encoding the subspace: 113 / 240\n",
      "Encoding the subspace: 114 / 240\n",
      "Encoding the subspace: 115 / 240\n",
      "Encoding the subspace: 116 / 240\n",
      "Encoding the subspace: 117 / 240\n",
      "Encoding the subspace: 118 / 240\n",
      "Encoding the subspace: 119 / 240\n",
      "Encoding the subspace: 120 / 240\n",
      "Encoding the subspace: 121 / 240\n",
      "Encoding the subspace: 122 / 240\n",
      "Encoding the subspace: 123 / 240\n",
      "Encoding the subspace: 124 / 240\n",
      "Encoding the subspace: 125 / 240\n",
      "Encoding the subspace: 126 / 240\n",
      "Encoding the subspace: 127 / 240\n",
      "Encoding the subspace: 128 / 240\n",
      "Encoding the subspace: 129 / 240\n",
      "Encoding the subspace: 130 / 240\n",
      "Encoding the subspace: 131 / 240\n",
      "Encoding the subspace: 132 / 240\n",
      "Encoding the subspace: 133 / 240\n",
      "Encoding the subspace: 134 / 240\n",
      "Encoding the subspace: 135 / 240\n",
      "Encoding the subspace: 136 / 240\n",
      "Encoding the subspace: 137 / 240\n",
      "Encoding the subspace: 138 / 240\n",
      "Encoding the subspace: 139 / 240\n",
      "Encoding the subspace: 140 / 240\n",
      "Encoding the subspace: 141 / 240\n",
      "Encoding the subspace: 142 / 240\n",
      "Encoding the subspace: 143 / 240\n",
      "Encoding the subspace: 144 / 240\n",
      "Encoding the subspace: 145 / 240\n",
      "Encoding the subspace: 146 / 240\n",
      "Encoding the subspace: 147 / 240\n",
      "Encoding the subspace: 148 / 240\n",
      "Encoding the subspace: 149 / 240\n",
      "Encoding the subspace: 150 / 240\n",
      "Encoding the subspace: 151 / 240\n",
      "Encoding the subspace: 152 / 240\n",
      "Encoding the subspace: 153 / 240\n",
      "Encoding the subspace: 154 / 240\n",
      "Encoding the subspace: 155 / 240\n",
      "Encoding the subspace: 156 / 240\n",
      "Encoding the subspace: 157 / 240\n",
      "Encoding the subspace: 158 / 240\n",
      "Encoding the subspace: 159 / 240\n",
      "Encoding the subspace: 160 / 240\n",
      "Encoding the subspace: 161 / 240\n",
      "Encoding the subspace: 162 / 240\n",
      "Encoding the subspace: 163 / 240\n",
      "Encoding the subspace: 164 / 240\n",
      "Encoding the subspace: 165 / 240\n",
      "Encoding the subspace: 166 / 240\n",
      "Encoding the subspace: 167 / 240\n",
      "Encoding the subspace: 168 / 240\n",
      "Encoding the subspace: 169 / 240\n",
      "Encoding the subspace: 170 / 240\n",
      "Encoding the subspace: 171 / 240\n",
      "Encoding the subspace: 172 / 240\n",
      "Encoding the subspace: 173 / 240\n",
      "Encoding the subspace: 174 / 240\n",
      "Encoding the subspace: 175 / 240\n",
      "Encoding the subspace: 176 / 240\n",
      "Encoding the subspace: 177 / 240\n",
      "Encoding the subspace: 178 / 240\n",
      "Encoding the subspace: 179 / 240\n",
      "Encoding the subspace: 180 / 240\n",
      "Encoding the subspace: 181 / 240\n",
      "Encoding the subspace: 182 / 240\n",
      "Encoding the subspace: 183 / 240\n",
      "Encoding the subspace: 184 / 240\n",
      "Encoding the subspace: 185 / 240\n",
      "Encoding the subspace: 186 / 240\n",
      "Encoding the subspace: 187 / 240\n",
      "Encoding the subspace: 188 / 240\n",
      "Encoding the subspace: 189 / 240\n",
      "Encoding the subspace: 190 / 240\n",
      "Encoding the subspace: 191 / 240\n",
      "Encoding the subspace: 192 / 240\n",
      "Encoding the subspace: 193 / 240\n",
      "Encoding the subspace: 194 / 240\n",
      "Encoding the subspace: 195 / 240\n",
      "Encoding the subspace: 196 / 240\n",
      "Encoding the subspace: 197 / 240\n",
      "Encoding the subspace: 198 / 240\n",
      "Encoding the subspace: 199 / 240\n",
      "Encoding the subspace: 200 / 240\n",
      "Encoding the subspace: 201 / 240\n",
      "Encoding the subspace: 202 / 240\n",
      "Encoding the subspace: 203 / 240\n",
      "Encoding the subspace: 204 / 240\n",
      "Encoding the subspace: 205 / 240\n",
      "Encoding the subspace: 206 / 240\n",
      "Encoding the subspace: 207 / 240\n",
      "Encoding the subspace: 208 / 240\n",
      "Encoding the subspace: 209 / 240\n",
      "Encoding the subspace: 210 / 240\n",
      "Encoding the subspace: 211 / 240\n",
      "Encoding the subspace: 212 / 240\n",
      "Encoding the subspace: 213 / 240\n",
      "Encoding the subspace: 214 / 240\n",
      "Encoding the subspace: 215 / 240\n",
      "Encoding the subspace: 216 / 240\n",
      "Encoding the subspace: 217 / 240\n",
      "Encoding the subspace: 218 / 240\n",
      "Encoding the subspace: 219 / 240\n",
      "Encoding the subspace: 220 / 240\n",
      "Encoding the subspace: 221 / 240\n",
      "Encoding the subspace: 222 / 240\n",
      "Encoding the subspace: 223 / 240\n",
      "Encoding the subspace: 224 / 240\n",
      "Encoding the subspace: 225 / 240\n",
      "Encoding the subspace: 226 / 240\n",
      "Encoding the subspace: 227 / 240\n",
      "Encoding the subspace: 228 / 240\n",
      "Encoding the subspace: 229 / 240\n",
      "Encoding the subspace: 230 / 240\n",
      "Encoding the subspace: 231 / 240\n",
      "Encoding the subspace: 232 / 240\n",
      "Encoding the subspace: 233 / 240\n",
      "Encoding the subspace: 234 / 240\n",
      "Encoding the subspace: 235 / 240\n",
      "Encoding the subspace: 236 / 240\n",
      "Encoding the subspace: 237 / 240\n",
      "Encoding the subspace: 238 / 240\n",
      "Encoding the subspace: 239 / 240\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=300, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.170294\n",
      "best loss:  0.17029435895455552\n",
      "Epoch 2 of 500, Train Loss: 0.033858\n",
      "best loss:  0.033857578650394876\n",
      "Epoch 3 of 500, Train Loss: 0.024651\n",
      "best loss:  0.024651082947781643\n",
      "Epoch 4 of 500, Train Loss: 0.020276\n",
      "best loss:  0.02027642118244598\n",
      "Epoch 5 of 500, Train Loss: 0.017129\n",
      "best loss:  0.017128880876621823\n",
      "Epoch 6 of 500, Train Loss: 0.015249\n",
      "best loss:  0.015248897089036326\n",
      "Epoch 7 of 500, Train Loss: 0.013779\n",
      "best loss:  0.01377932249199303\n",
      "Epoch 8 of 500, Train Loss: 0.012599\n",
      "best loss:  0.012598552598072894\n",
      "Epoch 9 of 500, Train Loss: 0.011492\n",
      "best loss:  0.011492279745131197\n",
      "Epoch 10 of 500, Train Loss: 0.010657\n",
      "best loss:  0.010656954421907259\n",
      "Epoch 11 of 500, Train Loss: 0.009906\n",
      "best loss:  0.00990562475917684\n",
      "Epoch 12 of 500, Train Loss: 0.009483\n",
      "best loss:  0.009482661735148227\n",
      "Epoch 13 of 500, Train Loss: 0.008766\n",
      "best loss:  0.008766124821958392\n",
      "Epoch 14 of 500, Train Loss: 0.008445\n",
      "best loss:  0.008445176892677438\n",
      "Epoch 15 of 500, Train Loss: 0.007884\n",
      "best loss:  0.007883812554786162\n",
      "Epoch 16 of 500, Train Loss: 0.007619\n",
      "best loss:  0.007618528304918481\n",
      "Epoch 17 of 500, Train Loss: 0.007241\n",
      "best loss:  0.007241412194972757\n",
      "Epoch 18 of 500, Train Loss: 0.006999\n",
      "best loss:  0.006998985422804027\n",
      "Epoch 19 of 500, Train Loss: 0.006829\n",
      "best loss:  0.006828856754636522\n",
      "Epoch 20 of 500, Train Loss: 0.006605\n",
      "best loss:  0.006604847318538855\n",
      "Epoch 21 of 500, Train Loss: 0.006608\n",
      "Epoch 22 of 500, Train Loss: 0.006464\n",
      "best loss:  0.006464448827183481\n",
      "Epoch 23 of 500, Train Loss: 0.006402\n",
      "best loss:  0.006402004057362806\n",
      "Epoch 24 of 500, Train Loss: 0.006491\n",
      "Epoch 25 of 500, Train Loss: 0.006335\n",
      "best loss:  0.006334852197008071\n",
      "Epoch 26 of 500, Train Loss: 0.006480\n",
      "Epoch 27 of 500, Train Loss: 0.006191\n",
      "best loss:  0.006190847689732585\n",
      "Epoch 28 of 500, Train Loss: 0.006061\n",
      "best loss:  0.006061268420330458\n",
      "Epoch 29 of 500, Train Loss: 0.005819\n",
      "best loss:  0.005818525556309777\n",
      "Epoch 30 of 500, Train Loss: 0.005813\n",
      "best loss:  0.005813288946905401\n",
      "Epoch 31 of 500, Train Loss: 0.005603\n",
      "best loss:  0.005602849381181678\n",
      "Epoch 32 of 500, Train Loss: 0.005594\n",
      "best loss:  0.005594478910852065\n",
      "Epoch 33 of 500, Train Loss: 0.005523\n",
      "best loss:  0.005522804208304697\n",
      "Epoch 34 of 500, Train Loss: 0.005542\n",
      "Epoch 35 of 500, Train Loss: 0.005473\n",
      "best loss:  0.005472580231987878\n",
      "Epoch 36 of 500, Train Loss: 0.005396\n",
      "best loss:  0.005395979248088796\n",
      "Epoch 37 of 500, Train Loss: 0.005185\n",
      "best loss:  0.0051849544878106914\n",
      "Epoch 38 of 500, Train Loss: 0.005029\n",
      "best loss:  0.005029216610482269\n",
      "Epoch 39 of 500, Train Loss: 0.004851\n",
      "best loss:  0.004850779478259966\n",
      "Epoch 40 of 500, Train Loss: 0.004690\n",
      "best loss:  0.004689866750645036\n",
      "Epoch 41 of 500, Train Loss: 0.004593\n",
      "best loss:  0.00459310356325645\n",
      "Epoch 42 of 500, Train Loss: 0.004448\n",
      "best loss:  0.004447980996179791\n",
      "Epoch 43 of 500, Train Loss: 0.004379\n",
      "best loss:  0.004379289524298966\n",
      "Epoch 44 of 500, Train Loss: 0.004298\n",
      "best loss:  0.004297779115693474\n",
      "Epoch 45 of 500, Train Loss: 0.004246\n",
      "best loss:  0.0042464223563086325\n",
      "Epoch 46 of 500, Train Loss: 0.004208\n",
      "best loss:  0.004207998407517773\n",
      "Epoch 47 of 500, Train Loss: 0.004205\n",
      "best loss:  0.004204629472415811\n",
      "Epoch 48 of 500, Train Loss: 0.004174\n",
      "best loss:  0.004174394241153793\n",
      "Epoch 49 of 500, Train Loss: 0.004168\n",
      "best loss:  0.004168134672831665\n",
      "Epoch 50 of 500, Train Loss: 0.004129\n",
      "best loss:  0.004129002555650243\n",
      "Epoch 51 of 500, Train Loss: 0.004115\n",
      "best loss:  0.0041149876925592535\n",
      "Epoch 52 of 500, Train Loss: 0.004022\n",
      "best loss:  0.004022144916234538\n",
      "Epoch 53 of 500, Train Loss: 0.003980\n",
      "best loss:  0.0039804477924607375\n",
      "Epoch 54 of 500, Train Loss: 0.003888\n",
      "best loss:  0.0038883942104930187\n",
      "Epoch 55 of 500, Train Loss: 0.003854\n",
      "best loss:  0.003853502961350767\n",
      "Epoch 56 of 500, Train Loss: 0.003811\n",
      "best loss:  0.0038110927316877276\n",
      "Epoch 57 of 500, Train Loss: 0.003788\n",
      "best loss:  0.0037880701710273287\n",
      "Epoch 58 of 500, Train Loss: 0.003783\n",
      "best loss:  0.0037830031893505756\n",
      "Epoch 59 of 500, Train Loss: 0.003783\n",
      "Epoch 60 of 500, Train Loss: 0.003765\n",
      "best loss:  0.0037646326292265575\n",
      "Epoch 61 of 500, Train Loss: 0.003756\n",
      "best loss:  0.003755777352139834\n",
      "Epoch 62 of 500, Train Loss: 0.003707\n",
      "best loss:  0.0037072397134643113\n",
      "Epoch 63 of 500, Train Loss: 0.003619\n",
      "best loss:  0.0036185768831344394\n",
      "Epoch 64 of 500, Train Loss: 0.003556\n",
      "best loss:  0.0035559250252901633\n",
      "Epoch 65 of 500, Train Loss: 0.003522\n",
      "best loss:  0.0035219698471027107\n",
      "Epoch 66 of 500, Train Loss: 0.003467\n",
      "best loss:  0.003466996306313433\n",
      "Epoch 67 of 500, Train Loss: 0.003491\n",
      "Epoch 68 of 500, Train Loss: 0.003470\n",
      "Epoch 69 of 500, Train Loss: 0.003521\n",
      "Epoch 70 of 500, Train Loss: 0.003574\n",
      "Epoch 71 of 500, Train Loss: 0.003629\n",
      "Epoch 72 of 500, Train Loss: 0.003658\n",
      "Epoch 73 of 500, Train Loss: 0.003678\n",
      "Epoch 74 of 500, Train Loss: 0.003788\n",
      "Epoch 75 of 500, Train Loss: 0.003766\n",
      "Epoch 76 of 500, Train Loss: 0.003806\n",
      "Epoch 77 of 500, Train Loss: 0.003745\n",
      "Epoch 78 of 500, Train Loss: 0.003683\n",
      "Epoch 79 of 500, Train Loss: 0.003703\n",
      "Epoch 80 of 500, Train Loss: 0.003738\n",
      "Epoch 81 of 500, Train Loss: 0.003819\n",
      "Epoch 82 of 500, Train Loss: 0.003875\n",
      "Epoch 83 of 500, Train Loss: 0.003848\n",
      "Epoch 84 of 500, Train Loss: 0.004004\n",
      "Epoch 85 of 500, Train Loss: 0.004153\n",
      "Epoch 86 of 500, Train Loss: 0.004146\n",
      "Epoch 87 of 500, Train Loss: 0.003922\n",
      "Epoch 88 of 500, Train Loss: 0.003843\n",
      "Epoch 89 of 500, Train Loss: 0.003711\n",
      "Epoch 90 of 500, Train Loss: 0.003659\n",
      "Epoch 91 of 500, Train Loss: 0.003673\n",
      "Epoch 92 of 500, Train Loss: 0.003788\n",
      "Epoch 93 of 500, Train Loss: 0.003717\n",
      "Epoch 94 of 500, Train Loss: 0.003662\n",
      "Epoch 95 of 500, Train Loss: 0.003643\n",
      "Epoch 96 of 500, Train Loss: 0.003592\n",
      "Epoch 97 of 500, Train Loss: 0.003697\n",
      "Epoch 98 of 500, Train Loss: 0.003803\n",
      "Epoch 99 of 500, Train Loss: 0.003758\n",
      "Epoch 100 of 500, Train Loss: 0.003853\n",
      "Epoch 101 of 500, Train Loss: 0.003847\n",
      "Epoch 102 of 500, Train Loss: 0.003816\n",
      "Epoch 103 of 500, Train Loss: 0.003748\n",
      "Epoch 104 of 500, Train Loss: 0.003701\n",
      "Epoch 105 of 500, Train Loss: 0.003690\n",
      "Epoch 106 of 500, Train Loss: 0.003664\n",
      "Epoch 107 of 500, Train Loss: 0.003677\n",
      "Epoch 108 of 500, Train Loss: 0.003713\n",
      "Epoch 109 of 500, Train Loss: 0.003719\n",
      "Epoch 110 of 500, Train Loss: 0.003856\n",
      "Epoch 111 of 500, Train Loss: 0.003876\n",
      "Epoch 112 of 500, Train Loss: 0.003933\n",
      "Epoch 113 of 500, Train Loss: 0.003815\n",
      "Epoch 114 of 500, Train Loss: 0.003801\n",
      "Epoch 115 of 500, Train Loss: 0.003745\n",
      "Epoch 116 of 500, Train Loss: 0.003752\n",
      "Epoch 117 of 500, Train Loss: 0.003750\n",
      "Epoch 118 of 500, Train Loss: 0.003773\n",
      "Epoch 119 of 500, Train Loss: 0.003805\n",
      "Epoch 120 of 500, Train Loss: 0.003792\n",
      "Epoch 121 of 500, Train Loss: 0.003866\n",
      "Epoch 122 of 500, Train Loss: 0.003913\n",
      "Epoch 123 of 500, Train Loss: 0.003971\n",
      "Epoch 124 of 500, Train Loss: 0.003922\n",
      "Epoch 125 of 500, Train Loss: 0.003828\n",
      "Epoch 126 of 500, Train Loss: 0.003756\n",
      "Epoch 127 of 500, Train Loss: 0.003716\n",
      "Epoch 128 of 500, Train Loss: 0.003673\n",
      "Epoch 129 of 500, Train Loss: 0.003726\n",
      "Epoch 130 of 500, Train Loss: 0.003774\n",
      "Epoch 131 of 500, Train Loss: 0.003796\n",
      "Epoch 132 of 500, Train Loss: 0.003809\n",
      "Epoch 133 of 500, Train Loss: 0.003807\n",
      "Epoch 134 of 500, Train Loss: 0.003838\n",
      "Epoch 135 of 500, Train Loss: 0.003945\n",
      "Epoch 136 of 500, Train Loss: 0.003829\n",
      "Epoch 137 of 500, Train Loss: 0.003833\n",
      "Epoch 138 of 500, Train Loss: 0.003784\n",
      "Epoch 139 of 500, Train Loss: 0.003780\n",
      "Epoch 140 of 500, Train Loss: 0.003696\n",
      "Epoch 141 of 500, Train Loss: 0.003704\n",
      "Epoch 142 of 500, Train Loss: 0.003717\n",
      "Epoch 143 of 500, Train Loss: 0.003748\n",
      "Epoch 144 of 500, Train Loss: 0.003766\n",
      "Epoch 145 of 500, Train Loss: 0.003815\n",
      "Epoch 146 of 500, Train Loss: 0.003872\n",
      "Epoch 147 of 500, Train Loss: 0.003955\n",
      "Epoch 148 of 500, Train Loss: 0.003939\n",
      "Epoch 149 of 500, Train Loss: 0.003886\n",
      "Epoch 150 of 500, Train Loss: 0.003938\n",
      "Epoch 151 of 500, Train Loss: 0.003866\n",
      "Epoch 152 of 500, Train Loss: 0.003815\n",
      "Epoch 153 of 500, Train Loss: 0.003787\n",
      "Epoch 154 of 500, Train Loss: 0.003833\n",
      "Epoch 155 of 500, Train Loss: 0.003764\n",
      "Epoch 156 of 500, Train Loss: 0.003759\n",
      "Epoch 157 of 500, Train Loss: 0.003760\n",
      "Epoch 158 of 500, Train Loss: 0.003752\n",
      "Epoch 159 of 500, Train Loss: 0.003725\n",
      "Epoch 160 of 500, Train Loss: 0.003705\n",
      "Epoch 161 of 500, Train Loss: 0.003761\n",
      "Epoch 162 of 500, Train Loss: 0.003819\n",
      "Epoch 163 of 500, Train Loss: 0.003896\n",
      "Epoch 164 of 500, Train Loss: 0.003888\n",
      "Epoch 165 of 500, Train Loss: 0.003858\n",
      "Epoch 166 of 500, Train Loss: 0.003830\n",
      "Epoch 167 of 500, Train Loss: 0.003808\n",
      "Epoch 168 of 500, Train Loss: 0.003818\n",
      "Epoch 169 of 500, Train Loss: 0.003808\n",
      "Epoch 170 of 500, Train Loss: 0.003715\n",
      "Epoch 171 of 500, Train Loss: 0.003715\n",
      "Epoch 172 of 500, Train Loss: 0.003732\n",
      "Epoch 173 of 500, Train Loss: 0.003745\n",
      "Epoch 174 of 500, Train Loss: 0.003782\n",
      "Epoch 175 of 500, Train Loss: 0.003829\n",
      "Epoch 176 of 500, Train Loss: 0.003815\n",
      "Epoch 177 of 500, Train Loss: 0.003830\n",
      "Epoch 178 of 500, Train Loss: 0.003860\n",
      "Epoch 179 of 500, Train Loss: 0.003851\n",
      "Epoch 180 of 500, Train Loss: 0.003882\n",
      "Epoch 181 of 500, Train Loss: 0.003886\n",
      "Epoch 182 of 500, Train Loss: 0.003852\n",
      "Epoch 183 of 500, Train Loss: 0.003838\n",
      "Epoch 184 of 500, Train Loss: 0.003785\n",
      "Epoch 185 of 500, Train Loss: 0.003763\n",
      "Epoch 186 of 500, Train Loss: 0.003773\n",
      "Epoch 187 of 500, Train Loss: 0.003814\n",
      "Epoch 188 of 500, Train Loss: 0.003789\n",
      "Epoch 189 of 500, Train Loss: 0.003757\n",
      "Epoch 190 of 500, Train Loss: 0.003751\n",
      "Epoch 191 of 500, Train Loss: 0.003738\n",
      "Epoch 192 of 500, Train Loss: 0.003727\n",
      "Epoch 193 of 500, Train Loss: 0.003755\n",
      "Epoch 194 of 500, Train Loss: 0.003780\n",
      "Epoch 195 of 500, Train Loss: 0.003805\n",
      "Epoch 196 of 500, Train Loss: 0.003825\n",
      "Epoch 197 of 500, Train Loss: 0.003817\n",
      "Epoch 198 of 500, Train Loss: 0.003813\n",
      "Epoch 199 of 500, Train Loss: 0.003814\n",
      "Epoch 200 of 500, Train Loss: 0.003820\n",
      "Epoch 201 of 500, Train Loss: 0.003785\n",
      "Epoch 202 of 500, Train Loss: 0.003766\n",
      "Epoch 203 of 500, Train Loss: 0.003738\n",
      "Epoch 204 of 500, Train Loss: 0.003728\n",
      "Epoch 205 of 500, Train Loss: 0.003726\n",
      "Epoch 206 of 500, Train Loss: 0.003764\n",
      "Epoch 207 of 500, Train Loss: 0.003769\n",
      "Epoch 208 of 500, Train Loss: 0.003786\n",
      "Epoch 209 of 500, Train Loss: 0.003809\n",
      "Epoch 210 of 500, Train Loss: 0.003804\n",
      "Epoch 211 of 500, Train Loss: 0.003828\n",
      "Epoch 212 of 500, Train Loss: 0.003773\n",
      "Epoch 213 of 500, Train Loss: 0.003818\n",
      "Epoch 214 of 500, Train Loss: 0.003854\n",
      "Epoch 215 of 500, Train Loss: 0.003831\n",
      "Epoch 216 of 500, Train Loss: 0.003842\n",
      "Epoch 217 of 500, Train Loss: 0.003811\n",
      "Epoch 218 of 500, Train Loss: 0.003810\n",
      "Epoch 219 of 500, Train Loss: 0.003813\n",
      "Epoch 220 of 500, Train Loss: 0.003817\n",
      "Epoch 221 of 500, Train Loss: 0.003813\n",
      "Epoch 222 of 500, Train Loss: 0.003794\n",
      "Epoch 223 of 500, Train Loss: 0.003814\n",
      "Epoch 224 of 500, Train Loss: 0.003769\n",
      "Epoch 225 of 500, Train Loss: 0.003835\n",
      "Epoch 226 of 500, Train Loss: 0.003866\n",
      "Epoch 227 of 500, Train Loss: 0.003894\n",
      "Epoch 228 of 500, Train Loss: 0.003898\n",
      "Epoch 229 of 500, Train Loss: 0.003926\n",
      "Epoch 230 of 500, Train Loss: 0.003859\n",
      "Epoch 231 of 500, Train Loss: 0.003815\n",
      "Epoch 232 of 500, Train Loss: 0.003756\n",
      "Epoch 233 of 500, Train Loss: 0.003762\n",
      "Epoch 234 of 500, Train Loss: 0.003767\n",
      "Epoch 235 of 500, Train Loss: 0.003809\n",
      "Epoch 236 of 500, Train Loss: 0.003818\n",
      "Epoch 237 of 500, Train Loss: 0.003882\n",
      "Epoch 238 of 500, Train Loss: 0.003879\n",
      "Epoch 239 of 500, Train Loss: 0.003933\n",
      "Epoch 240 of 500, Train Loss: 0.003897\n",
      "Epoch 241 of 500, Train Loss: 0.003913\n",
      "Epoch 242 of 500, Train Loss: 0.003905\n",
      "Epoch 243 of 500, Train Loss: 0.003949\n",
      "Epoch 244 of 500, Train Loss: 0.003935\n",
      "Epoch 245 of 500, Train Loss: 0.003913\n",
      "Epoch 246 of 500, Train Loss: 0.003963\n",
      "Epoch 247 of 500, Train Loss: 0.003936\n",
      "Epoch 248 of 500, Train Loss: 0.003881\n",
      "Epoch 249 of 500, Train Loss: 0.003855\n",
      "Epoch 250 of 500, Train Loss: 0.003893\n",
      "Epoch 251 of 500, Train Loss: 0.003927\n",
      "Epoch 252 of 500, Train Loss: 0.003876\n",
      "Epoch 253 of 500, Train Loss: 0.003879\n",
      "Epoch 254 of 500, Train Loss: 0.003855\n",
      "Epoch 255 of 500, Train Loss: 0.003889\n",
      "Epoch 256 of 500, Train Loss: 0.003941\n",
      "Epoch 257 of 500, Train Loss: 0.003964\n",
      "Epoch 258 of 500, Train Loss: 0.003990\n",
      "Epoch 259 of 500, Train Loss: 0.003974\n",
      "Epoch 260 of 500, Train Loss: 0.003911\n",
      "Epoch 261 of 500, Train Loss: 0.003910\n",
      "Epoch 262 of 500, Train Loss: 0.003872\n",
      "Epoch 263 of 500, Train Loss: 0.003826\n",
      "Epoch 264 of 500, Train Loss: 0.003781\n",
      "Epoch 265 of 500, Train Loss: 0.003800\n",
      "Epoch 266 of 500, Train Loss: 0.003782\n",
      "Epoch 267 of 500, Train Loss: 0.003776\n",
      "Epoch 268 of 500, Train Loss: 0.003792\n",
      "Epoch 269 of 500, Train Loss: 0.003834\n",
      "Epoch 270 of 500, Train Loss: 0.003849\n",
      "Epoch 271 of 500, Train Loss: 0.003889\n",
      "Epoch 272 of 500, Train Loss: 0.003917\n",
      "Epoch 273 of 500, Train Loss: 0.003883\n",
      "Epoch 274 of 500, Train Loss: 0.003893\n",
      "Epoch 275 of 500, Train Loss: 0.003863\n",
      "Epoch 276 of 500, Train Loss: 0.003907\n",
      "Epoch 277 of 500, Train Loss: 0.003917\n",
      "Epoch 278 of 500, Train Loss: 0.003912\n",
      "Epoch 279 of 500, Train Loss: 0.003858\n",
      "Epoch 280 of 500, Train Loss: 0.003810\n",
      "Epoch 281 of 500, Train Loss: 0.003807\n",
      "Epoch 282 of 500, Train Loss: 0.003793\n",
      "Epoch 283 of 500, Train Loss: 0.003786\n",
      "Epoch 284 of 500, Train Loss: 0.003783\n",
      "Epoch 285 of 500, Train Loss: 0.003812\n",
      "Epoch 286 of 500, Train Loss: 0.003788\n",
      "Epoch 287 of 500, Train Loss: 0.003825\n",
      "Epoch 288 of 500, Train Loss: 0.003826\n",
      "Epoch 289 of 500, Train Loss: 0.003851\n",
      "Epoch 290 of 500, Train Loss: 0.003844\n",
      "Epoch 291 of 500, Train Loss: 0.003813\n",
      "Epoch 292 of 500, Train Loss: 0.003818\n",
      "Epoch 293 of 500, Train Loss: 0.003836\n",
      "Epoch 294 of 500, Train Loss: 0.003921\n",
      "Epoch 295 of 500, Train Loss: 0.004007\n",
      "Epoch 296 of 500, Train Loss: 0.004092\n",
      "Epoch 297 of 500, Train Loss: 0.003982\n",
      "Epoch 298 of 500, Train Loss: 0.003879\n",
      "Epoch 299 of 500, Train Loss: 0.003826\n",
      "Epoch 300 of 500, Train Loss: 0.003835\n",
      "Epoch 301 of 500, Train Loss: 0.003772\n",
      "Epoch 302 of 500, Train Loss: 0.003770\n",
      "Epoch 303 of 500, Train Loss: 0.003809\n",
      "Epoch 304 of 500, Train Loss: 0.003794\n",
      "Epoch 305 of 500, Train Loss: 0.003804\n",
      "Epoch 306 of 500, Train Loss: 0.003771\n",
      "Epoch 307 of 500, Train Loss: 0.003820\n",
      "Epoch 308 of 500, Train Loss: 0.003787\n",
      "Epoch 309 of 500, Train Loss: 0.003855\n",
      "Epoch 310 of 500, Train Loss: 0.003849\n",
      "Epoch 311 of 500, Train Loss: 0.003869\n",
      "Epoch 312 of 500, Train Loss: 0.003857\n",
      "Epoch 313 of 500, Train Loss: 0.003859\n",
      "Epoch 314 of 500, Train Loss: 0.003808\n",
      "Epoch 315 of 500, Train Loss: 0.003977\n",
      "Epoch 316 of 500, Train Loss: 0.003961\n",
      "Epoch 317 of 500, Train Loss: 0.003897\n",
      "Epoch 318 of 500, Train Loss: 0.003815\n",
      "Epoch 319 of 500, Train Loss: 0.003744\n",
      "Epoch 320 of 500, Train Loss: 0.003728\n",
      "Epoch 321 of 500, Train Loss: 0.003791\n",
      "Epoch 322 of 500, Train Loss: 0.003759\n",
      "Epoch 323 of 500, Train Loss: 0.003774\n",
      "Epoch 324 of 500, Train Loss: 0.003755\n",
      "Epoch 325 of 500, Train Loss: 0.003833\n",
      "Epoch 326 of 500, Train Loss: 0.003780\n",
      "Epoch 327 of 500, Train Loss: 0.003788\n",
      "Epoch 328 of 500, Train Loss: 0.003742\n",
      "Epoch 329 of 500, Train Loss: 0.003776\n",
      "Epoch 330 of 500, Train Loss: 0.003759\n",
      "Epoch 331 of 500, Train Loss: 0.003783\n",
      "Epoch 332 of 500, Train Loss: 0.003724\n",
      "Epoch 333 of 500, Train Loss: 0.003819\n",
      "Epoch 334 of 500, Train Loss: 0.003814\n",
      "Epoch 335 of 500, Train Loss: 0.003846\n",
      "Epoch 336 of 500, Train Loss: 0.003801\n",
      "Epoch 337 of 500, Train Loss: 0.003844\n",
      "Epoch 338 of 500, Train Loss: 0.003816\n",
      "Epoch 339 of 500, Train Loss: 0.003840\n",
      "Epoch 340 of 500, Train Loss: 0.003755\n",
      "Epoch 341 of 500, Train Loss: 0.003755\n",
      "Epoch 342 of 500, Train Loss: 0.003709\n",
      "Epoch 343 of 500, Train Loss: 0.003781\n",
      "Epoch 344 of 500, Train Loss: 0.003792\n",
      "Epoch 345 of 500, Train Loss: 0.003859\n",
      "Epoch 346 of 500, Train Loss: 0.003785\n",
      "Epoch 347 of 500, Train Loss: 0.003820\n",
      "Epoch 348 of 500, Train Loss: 0.003775\n",
      "Epoch 349 of 500, Train Loss: 0.003779\n",
      "Epoch 350 of 500, Train Loss: 0.003735\n",
      "Epoch 351 of 500, Train Loss: 0.003770\n",
      "Epoch 352 of 500, Train Loss: 0.003724\n",
      "Epoch 353 of 500, Train Loss: 0.003792\n",
      "Epoch 354 of 500, Train Loss: 0.003753\n",
      "Epoch 355 of 500, Train Loss: 0.003787\n",
      "Epoch 356 of 500, Train Loss: 0.003761\n",
      "Epoch 357 of 500, Train Loss: 0.003824\n",
      "Epoch 358 of 500, Train Loss: 0.003752\n",
      "Epoch 359 of 500, Train Loss: 0.003783\n",
      "Epoch 360 of 500, Train Loss: 0.003756\n",
      "Epoch 361 of 500, Train Loss: 0.003776\n",
      "Epoch 362 of 500, Train Loss: 0.003708\n",
      "Epoch 363 of 500, Train Loss: 0.003751\n",
      "Epoch 364 of 500, Train Loss: 0.003719\n",
      "Epoch 365 of 500, Train Loss: 0.003774\n",
      "Epoch 366 of 500, Train Loss: 0.003791\n",
      "Epoch 367 of 500, Train Loss: 0.003884\n",
      "Epoch 368 of 500, Train Loss: 0.003804\n",
      "Epoch 369 of 500, Train Loss: 0.003856\n",
      "Epoch 370 of 500, Train Loss: 0.003822\n",
      "Epoch 371 of 500, Train Loss: 0.003897\n",
      "Epoch 372 of 500, Train Loss: 0.003919\n",
      "Epoch 373 of 500, Train Loss: 0.003833\n",
      "Epoch 374 of 500, Train Loss: 0.003784\n",
      "Epoch 375 of 500, Train Loss: 0.003782\n",
      "Epoch 376 of 500, Train Loss: 0.003744\n",
      "Epoch 377 of 500, Train Loss: 0.003753\n",
      "Epoch 378 of 500, Train Loss: 0.003748\n",
      "Epoch 379 of 500, Train Loss: 0.003777\n",
      "Epoch 380 of 500, Train Loss: 0.003721\n",
      "Epoch 381 of 500, Train Loss: 0.003734\n",
      "Epoch 382 of 500, Train Loss: 0.003744\n",
      "Epoch 383 of 500, Train Loss: 0.003764\n",
      "Epoch 384 of 500, Train Loss: 0.003814\n",
      "Epoch 385 of 500, Train Loss: 0.003833\n",
      "Epoch 386 of 500, Train Loss: 0.003838\n",
      "Epoch 387 of 500, Train Loss: 0.003910\n",
      "Epoch 388 of 500, Train Loss: 0.003925\n",
      "Epoch 389 of 500, Train Loss: 0.003861\n",
      "Epoch 390 of 500, Train Loss: 0.003790\n",
      "Epoch 391 of 500, Train Loss: 0.003765\n",
      "Epoch 392 of 500, Train Loss: 0.003768\n",
      "Epoch 393 of 500, Train Loss: 0.003772\n",
      "Epoch 394 of 500, Train Loss: 0.003767\n",
      "Epoch 395 of 500, Train Loss: 0.003753\n",
      "Epoch 396 of 500, Train Loss: 0.003798\n",
      "Epoch 397 of 500, Train Loss: 0.003839\n",
      "Epoch 398 of 500, Train Loss: 0.003837\n",
      "Epoch 399 of 500, Train Loss: 0.003805\n",
      "Epoch 400 of 500, Train Loss: 0.003814\n",
      "Epoch 401 of 500, Train Loss: 0.003821\n",
      "Epoch 402 of 500, Train Loss: 0.003860\n",
      "Epoch 403 of 500, Train Loss: 0.003837\n",
      "Epoch 404 of 500, Train Loss: 0.003815\n",
      "Epoch 405 of 500, Train Loss: 0.003805\n",
      "Epoch 406 of 500, Train Loss: 0.003859\n",
      "Epoch 407 of 500, Train Loss: 0.003849\n",
      "Epoch 408 of 500, Train Loss: 0.003855\n",
      "Epoch 409 of 500, Train Loss: 0.003802\n",
      "Epoch 410 of 500, Train Loss: 0.003840\n",
      "Epoch 411 of 500, Train Loss: 0.003785\n",
      "Epoch 412 of 500, Train Loss: 0.003766\n",
      "Epoch 413 of 500, Train Loss: 0.003819\n",
      "Epoch 414 of 500, Train Loss: 0.003814\n",
      "Epoch 415 of 500, Train Loss: 0.003821\n",
      "Epoch 416 of 500, Train Loss: 0.003801\n",
      "Epoch 417 of 500, Train Loss: 0.003785\n",
      "Epoch 418 of 500, Train Loss: 0.003792\n",
      "Epoch 419 of 500, Train Loss: 0.003833\n",
      "Epoch 420 of 500, Train Loss: 0.003844\n",
      "Epoch 421 of 500, Train Loss: 0.003838\n",
      "Epoch 422 of 500, Train Loss: 0.003813\n",
      "Epoch 423 of 500, Train Loss: 0.003828\n",
      "Epoch 424 of 500, Train Loss: 0.003776\n",
      "Epoch 425 of 500, Train Loss: 0.003799\n",
      "Epoch 426 of 500, Train Loss: 0.003770\n",
      "Epoch 427 of 500, Train Loss: 0.003784\n",
      "Epoch 428 of 500, Train Loss: 0.003745\n",
      "Epoch 429 of 500, Train Loss: 0.003800\n",
      "Epoch 430 of 500, Train Loss: 0.003801\n",
      "Epoch 431 of 500, Train Loss: 0.003890\n",
      "Epoch 432 of 500, Train Loss: 0.003879\n",
      "Epoch 433 of 500, Train Loss: 0.003836\n",
      "Epoch 434 of 500, Train Loss: 0.003793\n",
      "Epoch 435 of 500, Train Loss: 0.003813\n",
      "Epoch 436 of 500, Train Loss: 0.003797\n",
      "Epoch 437 of 500, Train Loss: 0.003848\n",
      "Epoch 438 of 500, Train Loss: 0.003838\n",
      "Epoch 439 of 500, Train Loss: 0.003850\n",
      "Epoch 440 of 500, Train Loss: 0.003770\n",
      "Epoch 441 of 500, Train Loss: 0.003819\n",
      "Epoch 442 of 500, Train Loss: 0.003798\n",
      "Epoch 443 of 500, Train Loss: 0.003819\n",
      "Epoch 444 of 500, Train Loss: 0.003796\n",
      "Epoch 445 of 500, Train Loss: 0.003834\n",
      "Epoch 446 of 500, Train Loss: 0.003776\n",
      "Epoch 447 of 500, Train Loss: 0.003827\n",
      "Epoch 448 of 500, Train Loss: 0.003854\n",
      "Epoch 449 of 500, Train Loss: 0.003822\n",
      "Epoch 450 of 500, Train Loss: 0.003818\n",
      "Epoch 451 of 500, Train Loss: 0.003847\n",
      "Epoch 452 of 500, Train Loss: 0.003814\n",
      "Epoch 453 of 500, Train Loss: 0.003875\n",
      "Epoch 454 of 500, Train Loss: 0.003840\n",
      "Epoch 455 of 500, Train Loss: 0.003861\n",
      "Epoch 456 of 500, Train Loss: 0.003775\n",
      "Epoch 457 of 500, Train Loss: 0.003774\n",
      "Epoch 458 of 500, Train Loss: 0.003763\n",
      "Epoch 459 of 500, Train Loss: 0.003852\n",
      "Epoch 460 of 500, Train Loss: 0.003882\n",
      "Epoch 461 of 500, Train Loss: 0.003906\n",
      "Epoch 462 of 500, Train Loss: 0.003923\n",
      "Epoch 463 of 500, Train Loss: 0.003898\n",
      "Epoch 464 of 500, Train Loss: 0.003810\n",
      "Epoch 465 of 500, Train Loss: 0.003800\n",
      "Epoch 466 of 500, Train Loss: 0.003756\n",
      "Epoch 467 of 500, Train Loss: 0.003750\n",
      "Epoch 468 of 500, Train Loss: 0.003737\n",
      "Epoch 469 of 500, Train Loss: 0.003818\n",
      "Epoch 470 of 500, Train Loss: 0.003783\n",
      "Epoch 471 of 500, Train Loss: 0.003822\n",
      "Epoch 472 of 500, Train Loss: 0.003821\n",
      "Epoch 473 of 500, Train Loss: 0.003893\n",
      "Epoch 474 of 500, Train Loss: 0.003942\n",
      "Epoch 475 of 500, Train Loss: 0.004043\n",
      "Epoch 476 of 500, Train Loss: 0.003962\n",
      "Epoch 477 of 500, Train Loss: 0.003899\n",
      "Epoch 478 of 500, Train Loss: 0.003778\n",
      "Epoch 479 of 500, Train Loss: 0.003806\n",
      "Epoch 480 of 500, Train Loss: 0.003783\n",
      "Epoch 481 of 500, Train Loss: 0.003811\n",
      "Epoch 482 of 500, Train Loss: 0.003791\n",
      "Epoch 483 of 500, Train Loss: 0.003828\n",
      "Epoch 484 of 500, Train Loss: 0.003806\n",
      "Epoch 485 of 500, Train Loss: 0.003828\n",
      "Epoch 486 of 500, Train Loss: 0.003767\n",
      "Epoch 487 of 500, Train Loss: 0.003807\n",
      "Epoch 488 of 500, Train Loss: 0.003778\n",
      "Epoch 489 of 500, Train Loss: 0.003812\n",
      "Epoch 490 of 500, Train Loss: 0.003796\n",
      "Epoch 491 of 500, Train Loss: 0.003871\n",
      "Epoch 492 of 500, Train Loss: 0.003834\n",
      "Epoch 493 of 500, Train Loss: 0.003864\n",
      "Epoch 494 of 500, Train Loss: 0.003835\n",
      "Epoch 495 of 500, Train Loss: 0.003878\n",
      "Epoch 496 of 500, Train Loss: 0.003885\n",
      "Epoch 497 of 500, Train Loss: 0.003877\n",
      "Epoch 498 of 500, Train Loss: 0.003840\n",
      "Epoch 499 of 500, Train Loss: 0.003850\n",
      "Epoch 500 of 500, Train Loss: 0.003836\n",
      "latent train shape:  (16395, 300)\n",
      "M: 300, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 45\n",
      "Training the subspace: 0 / 300\n",
      "Training the subspace: 1 / 300\n",
      "Training the subspace: 2 / 300\n",
      "Training the subspace: 3 / 300\n",
      "Training the subspace: 4 / 300\n",
      "Training the subspace: 5 / 300\n",
      "Training the subspace: 6 / 300\n",
      "Training the subspace: 7 / 300\n",
      "Training the subspace: 8 / 300\n",
      "Training the subspace: 9 / 300\n",
      "Training the subspace: 10 / 300\n",
      "Training the subspace: 11 / 300\n",
      "Training the subspace: 12 / 300\n",
      "Training the subspace: 13 / 300\n",
      "Training the subspace: 14 / 300\n",
      "Training the subspace: 15 / 300\n",
      "Training the subspace: 16 / 300\n",
      "Training the subspace: 17 / 300\n",
      "Training the subspace: 18 / 300\n",
      "Training the subspace: 19 / 300\n",
      "Training the subspace: 20 / 300\n",
      "Training the subspace: 21 / 300\n",
      "Training the subspace: 22 / 300\n",
      "Training the subspace: 23 / 300\n",
      "Training the subspace: 24 / 300\n",
      "Training the subspace: 25 / 300\n",
      "Training the subspace: 26 / 300\n",
      "Training the subspace: 27 / 300\n",
      "Training the subspace: 28 / 300\n",
      "Training the subspace: 29 / 300\n",
      "Training the subspace: 30 / 300\n",
      "Training the subspace: 31 / 300\n",
      "Training the subspace: 32 / 300\n",
      "Training the subspace: 33 / 300\n",
      "Training the subspace: 34 / 300\n",
      "Training the subspace: 35 / 300\n",
      "Training the subspace: 36 / 300\n",
      "Training the subspace: 37 / 300\n",
      "Training the subspace: 38 / 300\n",
      "Training the subspace: 39 / 300\n",
      "Training the subspace: 40 / 300\n",
      "Training the subspace: 41 / 300\n",
      "Training the subspace: 42 / 300\n",
      "Training the subspace: 43 / 300\n",
      "Training the subspace: 44 / 300\n",
      "Training the subspace: 45 / 300\n",
      "Training the subspace: 46 / 300\n",
      "Training the subspace: 47 / 300\n",
      "Training the subspace: 48 / 300\n",
      "Training the subspace: 49 / 300\n",
      "Training the subspace: 50 / 300\n",
      "Training the subspace: 51 / 300\n",
      "Training the subspace: 52 / 300\n",
      "Training the subspace: 53 / 300\n",
      "Training the subspace: 54 / 300\n",
      "Training the subspace: 55 / 300\n",
      "Training the subspace: 56 / 300\n",
      "Training the subspace: 57 / 300\n",
      "Training the subspace: 58 / 300\n",
      "Training the subspace: 59 / 300\n",
      "Training the subspace: 60 / 300\n",
      "Training the subspace: 61 / 300\n",
      "Training the subspace: 62 / 300\n",
      "Training the subspace: 63 / 300\n",
      "Training the subspace: 64 / 300\n",
      "Training the subspace: 65 / 300\n",
      "Training the subspace: 66 / 300\n",
      "Training the subspace: 67 / 300\n",
      "Training the subspace: 68 / 300\n",
      "Training the subspace: 69 / 300\n",
      "Training the subspace: 70 / 300\n",
      "Training the subspace: 71 / 300\n",
      "Training the subspace: 72 / 300\n",
      "Training the subspace: 73 / 300\n",
      "Training the subspace: 74 / 300\n",
      "Training the subspace: 75 / 300\n",
      "Training the subspace: 76 / 300\n",
      "Training the subspace: 77 / 300\n",
      "Training the subspace: 78 / 300\n",
      "Training the subspace: 79 / 300\n",
      "Training the subspace: 80 / 300\n",
      "Training the subspace: 81 / 300\n",
      "Training the subspace: 82 / 300\n",
      "Training the subspace: 83 / 300\n",
      "Training the subspace: 84 / 300\n",
      "Training the subspace: 85 / 300\n",
      "Training the subspace: 86 / 300\n",
      "Training the subspace: 87 / 300\n",
      "Training the subspace: 88 / 300\n",
      "Training the subspace: 89 / 300\n",
      "Training the subspace: 90 / 300\n",
      "Training the subspace: 91 / 300\n",
      "Training the subspace: 92 / 300\n",
      "Training the subspace: 93 / 300\n",
      "Training the subspace: 94 / 300\n",
      "Training the subspace: 95 / 300\n",
      "Training the subspace: 96 / 300\n",
      "Training the subspace: 97 / 300\n",
      "Training the subspace: 98 / 300\n",
      "Training the subspace: 99 / 300\n",
      "Training the subspace: 100 / 300\n",
      "Training the subspace: 101 / 300\n",
      "Training the subspace: 102 / 300\n",
      "Training the subspace: 103 / 300\n",
      "Training the subspace: 104 / 300\n",
      "Training the subspace: 105 / 300\n",
      "Training the subspace: 106 / 300\n",
      "Training the subspace: 107 / 300\n",
      "Training the subspace: 108 / 300\n",
      "Training the subspace: 109 / 300\n",
      "Training the subspace: 110 / 300\n",
      "Training the subspace: 111 / 300\n",
      "Training the subspace: 112 / 300\n",
      "Training the subspace: 113 / 300\n",
      "Training the subspace: 114 / 300\n",
      "Training the subspace: 115 / 300\n",
      "Training the subspace: 116 / 300\n",
      "Training the subspace: 117 / 300\n",
      "Training the subspace: 118 / 300\n",
      "Training the subspace: 119 / 300\n",
      "Training the subspace: 120 / 300\n",
      "Training the subspace: 121 / 300\n",
      "Training the subspace: 122 / 300\n",
      "Training the subspace: 123 / 300\n",
      "Training the subspace: 124 / 300\n",
      "Training the subspace: 125 / 300\n",
      "Training the subspace: 126 / 300\n",
      "Training the subspace: 127 / 300\n",
      "Training the subspace: 128 / 300\n",
      "Training the subspace: 129 / 300\n",
      "Training the subspace: 130 / 300\n",
      "Training the subspace: 131 / 300\n",
      "Training the subspace: 132 / 300\n",
      "Training the subspace: 133 / 300\n",
      "Training the subspace: 134 / 300\n",
      "Training the subspace: 135 / 300\n",
      "Training the subspace: 136 / 300\n",
      "Training the subspace: 137 / 300\n",
      "Training the subspace: 138 / 300\n",
      "Training the subspace: 139 / 300\n",
      "Training the subspace: 140 / 300\n",
      "Training the subspace: 141 / 300\n",
      "Training the subspace: 142 / 300\n",
      "Training the subspace: 143 / 300\n",
      "Training the subspace: 144 / 300\n",
      "Training the subspace: 145 / 300\n",
      "Training the subspace: 146 / 300\n",
      "Training the subspace: 147 / 300\n",
      "Training the subspace: 148 / 300\n",
      "Training the subspace: 149 / 300\n",
      "Training the subspace: 150 / 300\n",
      "Training the subspace: 151 / 300\n",
      "Training the subspace: 152 / 300\n",
      "Training the subspace: 153 / 300\n",
      "Training the subspace: 154 / 300\n",
      "Training the subspace: 155 / 300\n",
      "Training the subspace: 156 / 300\n",
      "Training the subspace: 157 / 300\n",
      "Training the subspace: 158 / 300\n",
      "Training the subspace: 159 / 300\n",
      "Training the subspace: 160 / 300\n",
      "Training the subspace: 161 / 300\n",
      "Training the subspace: 162 / 300\n",
      "Training the subspace: 163 / 300\n",
      "Training the subspace: 164 / 300\n",
      "Training the subspace: 165 / 300\n",
      "Training the subspace: 166 / 300\n",
      "Training the subspace: 167 / 300\n",
      "Training the subspace: 168 / 300\n",
      "Training the subspace: 169 / 300\n",
      "Training the subspace: 170 / 300\n",
      "Training the subspace: 171 / 300\n",
      "Training the subspace: 172 / 300\n",
      "Training the subspace: 173 / 300\n",
      "Training the subspace: 174 / 300\n",
      "Training the subspace: 175 / 300\n",
      "Training the subspace: 176 / 300\n",
      "Training the subspace: 177 / 300\n",
      "Training the subspace: 178 / 300\n",
      "Training the subspace: 179 / 300\n",
      "Training the subspace: 180 / 300\n",
      "Training the subspace: 181 / 300\n",
      "Training the subspace: 182 / 300\n",
      "Training the subspace: 183 / 300\n",
      "Training the subspace: 184 / 300\n",
      "Training the subspace: 185 / 300\n",
      "Training the subspace: 186 / 300\n",
      "Training the subspace: 187 / 300\n",
      "Training the subspace: 188 / 300\n",
      "Training the subspace: 189 / 300\n",
      "Training the subspace: 190 / 300\n",
      "Training the subspace: 191 / 300\n",
      "Training the subspace: 192 / 300\n",
      "Training the subspace: 193 / 300\n",
      "Training the subspace: 194 / 300\n",
      "Training the subspace: 195 / 300\n",
      "Training the subspace: 196 / 300\n",
      "Training the subspace: 197 / 300\n",
      "Training the subspace: 198 / 300\n",
      "Training the subspace: 199 / 300\n",
      "Training the subspace: 200 / 300\n",
      "Training the subspace: 201 / 300\n",
      "Training the subspace: 202 / 300\n",
      "Training the subspace: 203 / 300\n",
      "Training the subspace: 204 / 300\n",
      "Training the subspace: 205 / 300\n",
      "Training the subspace: 206 / 300\n",
      "Training the subspace: 207 / 300\n",
      "Training the subspace: 208 / 300\n",
      "Training the subspace: 209 / 300\n",
      "Training the subspace: 210 / 300\n",
      "Training the subspace: 211 / 300\n",
      "Training the subspace: 212 / 300\n",
      "Training the subspace: 213 / 300\n",
      "Training the subspace: 214 / 300\n",
      "Training the subspace: 215 / 300\n",
      "Training the subspace: 216 / 300\n",
      "Training the subspace: 217 / 300\n",
      "Training the subspace: 218 / 300\n",
      "Training the subspace: 219 / 300\n",
      "Training the subspace: 220 / 300\n",
      "Training the subspace: 221 / 300\n",
      "Training the subspace: 222 / 300\n",
      "Training the subspace: 223 / 300\n",
      "Training the subspace: 224 / 300\n",
      "Training the subspace: 225 / 300\n",
      "Training the subspace: 226 / 300\n",
      "Training the subspace: 227 / 300\n",
      "Training the subspace: 228 / 300\n",
      "Training the subspace: 229 / 300\n",
      "Training the subspace: 230 / 300\n",
      "Training the subspace: 231 / 300\n",
      "Training the subspace: 232 / 300\n",
      "Training the subspace: 233 / 300\n",
      "Training the subspace: 234 / 300\n",
      "Training the subspace: 235 / 300\n",
      "Training the subspace: 236 / 300\n",
      "Training the subspace: 237 / 300\n",
      "Training the subspace: 238 / 300\n",
      "Training the subspace: 239 / 300\n",
      "Training the subspace: 240 / 300\n",
      "Training the subspace: 241 / 300\n",
      "Training the subspace: 242 / 300\n",
      "Training the subspace: 243 / 300\n",
      "Training the subspace: 244 / 300\n",
      "Training the subspace: 245 / 300\n",
      "Training the subspace: 246 / 300\n",
      "Training the subspace: 247 / 300\n",
      "Training the subspace: 248 / 300\n",
      "Training the subspace: 249 / 300\n",
      "Training the subspace: 250 / 300\n",
      "Training the subspace: 251 / 300\n",
      "Training the subspace: 252 / 300\n",
      "Training the subspace: 253 / 300\n",
      "Training the subspace: 254 / 300\n",
      "Training the subspace: 255 / 300\n",
      "Training the subspace: 256 / 300\n",
      "Training the subspace: 257 / 300\n",
      "Training the subspace: 258 / 300\n",
      "Training the subspace: 259 / 300\n",
      "Training the subspace: 260 / 300\n",
      "Training the subspace: 261 / 300\n",
      "Training the subspace: 262 / 300\n",
      "Training the subspace: 263 / 300\n",
      "Training the subspace: 264 / 300\n",
      "Training the subspace: 265 / 300\n",
      "Training the subspace: 266 / 300\n",
      "Training the subspace: 267 / 300\n",
      "Training the subspace: 268 / 300\n",
      "Training the subspace: 269 / 300\n",
      "Training the subspace: 270 / 300\n",
      "Training the subspace: 271 / 300\n",
      "Training the subspace: 272 / 300\n",
      "Training the subspace: 273 / 300\n",
      "Training the subspace: 274 / 300\n",
      "Training the subspace: 275 / 300\n",
      "Training the subspace: 276 / 300\n",
      "Training the subspace: 277 / 300\n",
      "Training the subspace: 278 / 300\n",
      "Training the subspace: 279 / 300\n",
      "Training the subspace: 280 / 300\n",
      "Training the subspace: 281 / 300\n",
      "Training the subspace: 282 / 300\n",
      "Training the subspace: 283 / 300\n",
      "Training the subspace: 284 / 300\n",
      "Training the subspace: 285 / 300\n",
      "Training the subspace: 286 / 300\n",
      "Training the subspace: 287 / 300\n",
      "Training the subspace: 288 / 300\n",
      "Training the subspace: 289 / 300\n",
      "Training the subspace: 290 / 300\n",
      "Training the subspace: 291 / 300\n",
      "Training the subspace: 292 / 300\n",
      "Training the subspace: 293 / 300\n",
      "Training the subspace: 294 / 300\n",
      "Training the subspace: 295 / 300\n",
      "Training the subspace: 296 / 300\n",
      "Training the subspace: 297 / 300\n",
      "Training the subspace: 298 / 300\n",
      "Training the subspace: 299 / 300\n",
      "Encoding the subspace: 0 / 300\n",
      "Encoding the subspace: 1 / 300\n",
      "Encoding the subspace: 2 / 300\n",
      "Encoding the subspace: 3 / 300\n",
      "Encoding the subspace: 4 / 300\n",
      "Encoding the subspace: 5 / 300\n",
      "Encoding the subspace: 6 / 300\n",
      "Encoding the subspace: 7 / 300\n",
      "Encoding the subspace: 8 / 300\n",
      "Encoding the subspace: 9 / 300\n",
      "Encoding the subspace: 10 / 300\n",
      "Encoding the subspace: 11 / 300\n",
      "Encoding the subspace: 12 / 300\n",
      "Encoding the subspace: 13 / 300\n",
      "Encoding the subspace: 14 / 300\n",
      "Encoding the subspace: 15 / 300\n",
      "Encoding the subspace: 16 / 300\n",
      "Encoding the subspace: 17 / 300\n",
      "Encoding the subspace: 18 / 300\n",
      "Encoding the subspace: 19 / 300\n",
      "Encoding the subspace: 20 / 300\n",
      "Encoding the subspace: 21 / 300\n",
      "Encoding the subspace: 22 / 300\n",
      "Encoding the subspace: 23 / 300\n",
      "Encoding the subspace: 24 / 300\n",
      "Encoding the subspace: 25 / 300\n",
      "Encoding the subspace: 26 / 300\n",
      "Encoding the subspace: 27 / 300\n",
      "Encoding the subspace: 28 / 300\n",
      "Encoding the subspace: 29 / 300\n",
      "Encoding the subspace: 30 / 300\n",
      "Encoding the subspace: 31 / 300\n",
      "Encoding the subspace: 32 / 300\n",
      "Encoding the subspace: 33 / 300\n",
      "Encoding the subspace: 34 / 300\n",
      "Encoding the subspace: 35 / 300\n",
      "Encoding the subspace: 36 / 300\n",
      "Encoding the subspace: 37 / 300\n",
      "Encoding the subspace: 38 / 300\n",
      "Encoding the subspace: 39 / 300\n",
      "Encoding the subspace: 40 / 300\n",
      "Encoding the subspace: 41 / 300\n",
      "Encoding the subspace: 42 / 300\n",
      "Encoding the subspace: 43 / 300\n",
      "Encoding the subspace: 44 / 300\n",
      "Encoding the subspace: 45 / 300\n",
      "Encoding the subspace: 46 / 300\n",
      "Encoding the subspace: 47 / 300\n",
      "Encoding the subspace: 48 / 300\n",
      "Encoding the subspace: 49 / 300\n",
      "Encoding the subspace: 50 / 300\n",
      "Encoding the subspace: 51 / 300\n",
      "Encoding the subspace: 52 / 300\n",
      "Encoding the subspace: 53 / 300\n",
      "Encoding the subspace: 54 / 300\n",
      "Encoding the subspace: 55 / 300\n",
      "Encoding the subspace: 56 / 300\n",
      "Encoding the subspace: 57 / 300\n",
      "Encoding the subspace: 58 / 300\n",
      "Encoding the subspace: 59 / 300\n",
      "Encoding the subspace: 60 / 300\n",
      "Encoding the subspace: 61 / 300\n",
      "Encoding the subspace: 62 / 300\n",
      "Encoding the subspace: 63 / 300\n",
      "Encoding the subspace: 64 / 300\n",
      "Encoding the subspace: 65 / 300\n",
      "Encoding the subspace: 66 / 300\n",
      "Encoding the subspace: 67 / 300\n",
      "Encoding the subspace: 68 / 300\n",
      "Encoding the subspace: 69 / 300\n",
      "Encoding the subspace: 70 / 300\n",
      "Encoding the subspace: 71 / 300\n",
      "Encoding the subspace: 72 / 300\n",
      "Encoding the subspace: 73 / 300\n",
      "Encoding the subspace: 74 / 300\n",
      "Encoding the subspace: 75 / 300\n",
      "Encoding the subspace: 76 / 300\n",
      "Encoding the subspace: 77 / 300\n",
      "Encoding the subspace: 78 / 300\n",
      "Encoding the subspace: 79 / 300\n",
      "Encoding the subspace: 80 / 300\n",
      "Encoding the subspace: 81 / 300\n",
      "Encoding the subspace: 82 / 300\n",
      "Encoding the subspace: 83 / 300\n",
      "Encoding the subspace: 84 / 300\n",
      "Encoding the subspace: 85 / 300\n",
      "Encoding the subspace: 86 / 300\n",
      "Encoding the subspace: 87 / 300\n",
      "Encoding the subspace: 88 / 300\n",
      "Encoding the subspace: 89 / 300\n",
      "Encoding the subspace: 90 / 300\n",
      "Encoding the subspace: 91 / 300\n",
      "Encoding the subspace: 92 / 300\n",
      "Encoding the subspace: 93 / 300\n",
      "Encoding the subspace: 94 / 300\n",
      "Encoding the subspace: 95 / 300\n",
      "Encoding the subspace: 96 / 300\n",
      "Encoding the subspace: 97 / 300\n",
      "Encoding the subspace: 98 / 300\n",
      "Encoding the subspace: 99 / 300\n",
      "Encoding the subspace: 100 / 300\n",
      "Encoding the subspace: 101 / 300\n",
      "Encoding the subspace: 102 / 300\n",
      "Encoding the subspace: 103 / 300\n",
      "Encoding the subspace: 104 / 300\n",
      "Encoding the subspace: 105 / 300\n",
      "Encoding the subspace: 106 / 300\n",
      "Encoding the subspace: 107 / 300\n",
      "Encoding the subspace: 108 / 300\n",
      "Encoding the subspace: 109 / 300\n",
      "Encoding the subspace: 110 / 300\n",
      "Encoding the subspace: 111 / 300\n",
      "Encoding the subspace: 112 / 300\n",
      "Encoding the subspace: 113 / 300\n",
      "Encoding the subspace: 114 / 300\n",
      "Encoding the subspace: 115 / 300\n",
      "Encoding the subspace: 116 / 300\n",
      "Encoding the subspace: 117 / 300\n",
      "Encoding the subspace: 118 / 300\n",
      "Encoding the subspace: 119 / 300\n",
      "Encoding the subspace: 120 / 300\n",
      "Encoding the subspace: 121 / 300\n",
      "Encoding the subspace: 122 / 300\n",
      "Encoding the subspace: 123 / 300\n",
      "Encoding the subspace: 124 / 300\n",
      "Encoding the subspace: 125 / 300\n",
      "Encoding the subspace: 126 / 300\n",
      "Encoding the subspace: 127 / 300\n",
      "Encoding the subspace: 128 / 300\n",
      "Encoding the subspace: 129 / 300\n",
      "Encoding the subspace: 130 / 300\n",
      "Encoding the subspace: 131 / 300\n",
      "Encoding the subspace: 132 / 300\n",
      "Encoding the subspace: 133 / 300\n",
      "Encoding the subspace: 134 / 300\n",
      "Encoding the subspace: 135 / 300\n",
      "Encoding the subspace: 136 / 300\n",
      "Encoding the subspace: 137 / 300\n",
      "Encoding the subspace: 138 / 300\n",
      "Encoding the subspace: 139 / 300\n",
      "Encoding the subspace: 140 / 300\n",
      "Encoding the subspace: 141 / 300\n",
      "Encoding the subspace: 142 / 300\n",
      "Encoding the subspace: 143 / 300\n",
      "Encoding the subspace: 144 / 300\n",
      "Encoding the subspace: 145 / 300\n",
      "Encoding the subspace: 146 / 300\n",
      "Encoding the subspace: 147 / 300\n",
      "Encoding the subspace: 148 / 300\n",
      "Encoding the subspace: 149 / 300\n",
      "Encoding the subspace: 150 / 300\n",
      "Encoding the subspace: 151 / 300\n",
      "Encoding the subspace: 152 / 300\n",
      "Encoding the subspace: 153 / 300\n",
      "Encoding the subspace: 154 / 300\n",
      "Encoding the subspace: 155 / 300\n",
      "Encoding the subspace: 156 / 300\n",
      "Encoding the subspace: 157 / 300\n",
      "Encoding the subspace: 158 / 300\n",
      "Encoding the subspace: 159 / 300\n",
      "Encoding the subspace: 160 / 300\n",
      "Encoding the subspace: 161 / 300\n",
      "Encoding the subspace: 162 / 300\n",
      "Encoding the subspace: 163 / 300\n",
      "Encoding the subspace: 164 / 300\n",
      "Encoding the subspace: 165 / 300\n",
      "Encoding the subspace: 166 / 300\n",
      "Encoding the subspace: 167 / 300\n",
      "Encoding the subspace: 168 / 300\n",
      "Encoding the subspace: 169 / 300\n",
      "Encoding the subspace: 170 / 300\n",
      "Encoding the subspace: 171 / 300\n",
      "Encoding the subspace: 172 / 300\n",
      "Encoding the subspace: 173 / 300\n",
      "Encoding the subspace: 174 / 300\n",
      "Encoding the subspace: 175 / 300\n",
      "Encoding the subspace: 176 / 300\n",
      "Encoding the subspace: 177 / 300\n",
      "Encoding the subspace: 178 / 300\n",
      "Encoding the subspace: 179 / 300\n",
      "Encoding the subspace: 180 / 300\n",
      "Encoding the subspace: 181 / 300\n",
      "Encoding the subspace: 182 / 300\n",
      "Encoding the subspace: 183 / 300\n",
      "Encoding the subspace: 184 / 300\n",
      "Encoding the subspace: 185 / 300\n",
      "Encoding the subspace: 186 / 300\n",
      "Encoding the subspace: 187 / 300\n",
      "Encoding the subspace: 188 / 300\n",
      "Encoding the subspace: 189 / 300\n",
      "Encoding the subspace: 190 / 300\n",
      "Encoding the subspace: 191 / 300\n",
      "Encoding the subspace: 192 / 300\n",
      "Encoding the subspace: 193 / 300\n",
      "Encoding the subspace: 194 / 300\n",
      "Encoding the subspace: 195 / 300\n",
      "Encoding the subspace: 196 / 300\n",
      "Encoding the subspace: 197 / 300\n",
      "Encoding the subspace: 198 / 300\n",
      "Encoding the subspace: 199 / 300\n",
      "Encoding the subspace: 200 / 300\n",
      "Encoding the subspace: 201 / 300\n",
      "Encoding the subspace: 202 / 300\n",
      "Encoding the subspace: 203 / 300\n",
      "Encoding the subspace: 204 / 300\n",
      "Encoding the subspace: 205 / 300\n",
      "Encoding the subspace: 206 / 300\n",
      "Encoding the subspace: 207 / 300\n",
      "Encoding the subspace: 208 / 300\n",
      "Encoding the subspace: 209 / 300\n",
      "Encoding the subspace: 210 / 300\n",
      "Encoding the subspace: 211 / 300\n",
      "Encoding the subspace: 212 / 300\n",
      "Encoding the subspace: 213 / 300\n",
      "Encoding the subspace: 214 / 300\n",
      "Encoding the subspace: 215 / 300\n",
      "Encoding the subspace: 216 / 300\n",
      "Encoding the subspace: 217 / 300\n",
      "Encoding the subspace: 218 / 300\n",
      "Encoding the subspace: 219 / 300\n",
      "Encoding the subspace: 220 / 300\n",
      "Encoding the subspace: 221 / 300\n",
      "Encoding the subspace: 222 / 300\n",
      "Encoding the subspace: 223 / 300\n",
      "Encoding the subspace: 224 / 300\n",
      "Encoding the subspace: 225 / 300\n",
      "Encoding the subspace: 226 / 300\n",
      "Encoding the subspace: 227 / 300\n",
      "Encoding the subspace: 228 / 300\n",
      "Encoding the subspace: 229 / 300\n",
      "Encoding the subspace: 230 / 300\n",
      "Encoding the subspace: 231 / 300\n",
      "Encoding the subspace: 232 / 300\n",
      "Encoding the subspace: 233 / 300\n",
      "Encoding the subspace: 234 / 300\n",
      "Encoding the subspace: 235 / 300\n",
      "Encoding the subspace: 236 / 300\n",
      "Encoding the subspace: 237 / 300\n",
      "Encoding the subspace: 238 / 300\n",
      "Encoding the subspace: 239 / 300\n",
      "Encoding the subspace: 240 / 300\n",
      "Encoding the subspace: 241 / 300\n",
      "Encoding the subspace: 242 / 300\n",
      "Encoding the subspace: 243 / 300\n",
      "Encoding the subspace: 244 / 300\n",
      "Encoding the subspace: 245 / 300\n",
      "Encoding the subspace: 246 / 300\n",
      "Encoding the subspace: 247 / 300\n",
      "Encoding the subspace: 248 / 300\n",
      "Encoding the subspace: 249 / 300\n",
      "Encoding the subspace: 250 / 300\n",
      "Encoding the subspace: 251 / 300\n",
      "Encoding the subspace: 252 / 300\n",
      "Encoding the subspace: 253 / 300\n",
      "Encoding the subspace: 254 / 300\n",
      "Encoding the subspace: 255 / 300\n",
      "Encoding the subspace: 256 / 300\n",
      "Encoding the subspace: 257 / 300\n",
      "Encoding the subspace: 258 / 300\n",
      "Encoding the subspace: 259 / 300\n",
      "Encoding the subspace: 260 / 300\n",
      "Encoding the subspace: 261 / 300\n",
      "Encoding the subspace: 262 / 300\n",
      "Encoding the subspace: 263 / 300\n",
      "Encoding the subspace: 264 / 300\n",
      "Encoding the subspace: 265 / 300\n",
      "Encoding the subspace: 266 / 300\n",
      "Encoding the subspace: 267 / 300\n",
      "Encoding the subspace: 268 / 300\n",
      "Encoding the subspace: 269 / 300\n",
      "Encoding the subspace: 270 / 300\n",
      "Encoding the subspace: 271 / 300\n",
      "Encoding the subspace: 272 / 300\n",
      "Encoding the subspace: 273 / 300\n",
      "Encoding the subspace: 274 / 300\n",
      "Encoding the subspace: 275 / 300\n",
      "Encoding the subspace: 276 / 300\n",
      "Encoding the subspace: 277 / 300\n",
      "Encoding the subspace: 278 / 300\n",
      "Encoding the subspace: 279 / 300\n",
      "Encoding the subspace: 280 / 300\n",
      "Encoding the subspace: 281 / 300\n",
      "Encoding the subspace: 282 / 300\n",
      "Encoding the subspace: 283 / 300\n",
      "Encoding the subspace: 284 / 300\n",
      "Encoding the subspace: 285 / 300\n",
      "Encoding the subspace: 286 / 300\n",
      "Encoding the subspace: 287 / 300\n",
      "Encoding the subspace: 288 / 300\n",
      "Encoding the subspace: 289 / 300\n",
      "Encoding the subspace: 290 / 300\n",
      "Encoding the subspace: 291 / 300\n",
      "Encoding the subspace: 292 / 300\n",
      "Encoding the subspace: 293 / 300\n",
      "Encoding the subspace: 294 / 300\n",
      "Encoding the subspace: 295 / 300\n",
      "Encoding the subspace: 296 / 300\n",
      "Encoding the subspace: 297 / 300\n",
      "Encoding the subspace: 298 / 300\n",
      "Encoding the subspace: 299 / 300\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "data timestep:  1000\n",
      "plane idx:  1\n",
      "tucker_coeffs: 1521\n",
      "basis product shape:  (1521, 1521)\n",
      "X_train_tucker shape:  (16395, 1521)\n",
      "X_train max avg:  36.71844473160444 0.014225448629870378\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=5, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.852061\n",
      "best loss:  0.8520608844683152\n",
      "Epoch 2 of 500, Train Loss: 0.319035\n",
      "best loss:  0.31903504025797513\n",
      "Epoch 3 of 500, Train Loss: 0.095586\n",
      "best loss:  0.09558597116103006\n",
      "Epoch 4 of 500, Train Loss: 0.058766\n",
      "best loss:  0.058766089013395036\n",
      "Epoch 5 of 500, Train Loss: 0.056509\n",
      "best loss:  0.05650866208977044\n",
      "Epoch 6 of 500, Train Loss: 0.056373\n",
      "best loss:  0.056373143320011566\n",
      "Epoch 7 of 500, Train Loss: 0.056271\n",
      "best loss:  0.05627072536406403\n",
      "Epoch 8 of 500, Train Loss: 0.056136\n",
      "best loss:  0.05613585438375369\n",
      "Epoch 9 of 500, Train Loss: 0.055952\n",
      "best loss:  0.055952351489790185\n",
      "Epoch 10 of 500, Train Loss: 0.055704\n",
      "best loss:  0.05570364779794531\n",
      "Epoch 11 of 500, Train Loss: 0.055371\n",
      "best loss:  0.05537138197728064\n",
      "Epoch 12 of 500, Train Loss: 0.054936\n",
      "best loss:  0.054936447444772565\n",
      "Epoch 13 of 500, Train Loss: 0.054381\n",
      "best loss:  0.054381478074739524\n",
      "Epoch 14 of 500, Train Loss: 0.053694\n",
      "best loss:  0.05369410053384671\n",
      "Epoch 15 of 500, Train Loss: 0.052871\n",
      "best loss:  0.052870587554540084\n",
      "Epoch 16 of 500, Train Loss: 0.051919\n",
      "best loss:  0.051919030647385546\n",
      "Epoch 17 of 500, Train Loss: 0.050860\n",
      "best loss:  0.0508601917639779\n",
      "Epoch 18 of 500, Train Loss: 0.049724\n",
      "best loss:  0.049724100123325635\n",
      "Epoch 19 of 500, Train Loss: 0.048543\n",
      "best loss:  0.04854317773413274\n",
      "Epoch 20 of 500, Train Loss: 0.047345\n",
      "best loss:  0.04734497247155496\n",
      "Epoch 21 of 500, Train Loss: 0.046147\n",
      "best loss:  0.04614670020789847\n",
      "Epoch 22 of 500, Train Loss: 0.044954\n",
      "best loss:  0.0449539738316486\n",
      "Epoch 23 of 500, Train Loss: 0.043764\n",
      "best loss:  0.043764142171559176\n",
      "Epoch 24 of 500, Train Loss: 0.042572\n",
      "best loss:  0.04257209860907449\n",
      "Epoch 25 of 500, Train Loss: 0.041377\n",
      "best loss:  0.0413773687621362\n",
      "Epoch 26 of 500, Train Loss: 0.040186\n",
      "best loss:  0.040185540075262274\n",
      "Epoch 27 of 500, Train Loss: 0.039009\n",
      "best loss:  0.03900939746815783\n",
      "Epoch 28 of 500, Train Loss: 0.037866\n",
      "best loss:  0.03786576798460049\n",
      "Epoch 29 of 500, Train Loss: 0.036772\n",
      "best loss:  0.03677165622609963\n",
      "Epoch 30 of 500, Train Loss: 0.035743\n",
      "best loss:  0.03574285397573208\n",
      "Epoch 31 of 500, Train Loss: 0.034791\n",
      "best loss:  0.03479075708874148\n",
      "Epoch 32 of 500, Train Loss: 0.033922\n",
      "best loss:  0.03392192421800479\n",
      "Epoch 33 of 500, Train Loss: 0.033138\n",
      "best loss:  0.03313844817800649\n",
      "Epoch 34 of 500, Train Loss: 0.032439\n",
      "best loss:  0.03243871749100788\n",
      "Epoch 35 of 500, Train Loss: 0.031819\n",
      "best loss:  0.03181897059927673\n",
      "Epoch 36 of 500, Train Loss: 0.031275\n",
      "best loss:  0.03127462946892853\n",
      "Epoch 37 of 500, Train Loss: 0.030801\n",
      "best loss:  0.03080106553009733\n",
      "Epoch 38 of 500, Train Loss: 0.030394\n",
      "best loss:  0.030393767956185855\n",
      "Epoch 39 of 500, Train Loss: 0.030047\n",
      "best loss:  0.030046879220934398\n",
      "Epoch 40 of 500, Train Loss: 0.029755\n",
      "best loss:  0.029755166710116143\n",
      "Epoch 41 of 500, Train Loss: 0.029512\n",
      "best loss:  0.029511962744295454\n",
      "Epoch 42 of 500, Train Loss: 0.029311\n",
      "best loss:  0.0293111854495459\n",
      "Epoch 43 of 500, Train Loss: 0.029146\n",
      "best loss:  0.029145910064852256\n",
      "Epoch 44 of 500, Train Loss: 0.029010\n",
      "best loss:  0.029010343217370352\n",
      "Epoch 45 of 500, Train Loss: 0.028899\n",
      "best loss:  0.028898705851535724\n",
      "Epoch 46 of 500, Train Loss: 0.028806\n",
      "best loss:  0.02880625827589917\n",
      "Epoch 47 of 500, Train Loss: 0.028729\n",
      "best loss:  0.02872930293949682\n",
      "Epoch 48 of 500, Train Loss: 0.028664\n",
      "best loss:  0.028664395055199375\n",
      "Epoch 49 of 500, Train Loss: 0.028608\n",
      "best loss:  0.028608479950840534\n",
      "Epoch 50 of 500, Train Loss: 0.028560\n",
      "best loss:  0.028560075783910763\n",
      "Epoch 51 of 500, Train Loss: 0.028517\n",
      "best loss:  0.02851730584331189\n",
      "Epoch 52 of 500, Train Loss: 0.028479\n",
      "best loss:  0.028479156665081634\n",
      "Epoch 53 of 500, Train Loss: 0.028444\n",
      "best loss:  0.028444405672656617\n",
      "Epoch 54 of 500, Train Loss: 0.028412\n",
      "best loss:  0.028411839916298615\n",
      "Epoch 55 of 500, Train Loss: 0.028381\n",
      "best loss:  0.028381053721550274\n",
      "Epoch 56 of 500, Train Loss: 0.028352\n",
      "best loss:  0.02835150207789947\n",
      "Epoch 57 of 500, Train Loss: 0.028322\n",
      "best loss:  0.028322408361587847\n",
      "Epoch 58 of 500, Train Loss: 0.028294\n",
      "best loss:  0.028293561142537196\n",
      "Epoch 59 of 500, Train Loss: 0.028264\n",
      "best loss:  0.028264404661671887\n",
      "Epoch 60 of 500, Train Loss: 0.028235\n",
      "best loss:  0.028234639699578054\n",
      "Epoch 61 of 500, Train Loss: 0.028204\n",
      "best loss:  0.028203960544657136\n",
      "Epoch 62 of 500, Train Loss: 0.028172\n",
      "best loss:  0.028172229958586252\n",
      "Epoch 63 of 500, Train Loss: 0.028138\n",
      "best loss:  0.02813840343241486\n",
      "Epoch 64 of 500, Train Loss: 0.028103\n",
      "best loss:  0.028103027104710538\n",
      "Epoch 65 of 500, Train Loss: 0.028066\n",
      "best loss:  0.02806565635151757\n",
      "Epoch 66 of 500, Train Loss: 0.028026\n",
      "best loss:  0.0280258063531062\n",
      "Epoch 67 of 500, Train Loss: 0.027984\n",
      "best loss:  0.027983974408657946\n",
      "Epoch 68 of 500, Train Loss: 0.027940\n",
      "best loss:  0.027939608123860964\n",
      "Epoch 69 of 500, Train Loss: 0.027893\n",
      "best loss:  0.02789250665476055\n",
      "Epoch 70 of 500, Train Loss: 0.027843\n",
      "best loss:  0.027843152240998034\n",
      "Epoch 71 of 500, Train Loss: 0.027792\n",
      "best loss:  0.027791684329177637\n",
      "Epoch 72 of 500, Train Loss: 0.027738\n",
      "best loss:  0.027738293713633722\n",
      "Epoch 73 of 500, Train Loss: 0.027683\n",
      "best loss:  0.027682651504838132\n",
      "Epoch 74 of 500, Train Loss: 0.027626\n",
      "best loss:  0.027626034187770255\n",
      "Epoch 75 of 500, Train Loss: 0.027569\n",
      "best loss:  0.027568706035588746\n",
      "Epoch 76 of 500, Train Loss: 0.027511\n",
      "best loss:  0.02751117788053067\n",
      "Epoch 77 of 500, Train Loss: 0.027454\n",
      "best loss:  0.027454366400754875\n",
      "Epoch 78 of 500, Train Loss: 0.027399\n",
      "best loss:  0.027398614270627036\n",
      "Epoch 79 of 500, Train Loss: 0.027345\n",
      "best loss:  0.027344741771456085\n",
      "Epoch 80 of 500, Train Loss: 0.027293\n",
      "best loss:  0.02729327900266607\n",
      "Epoch 81 of 500, Train Loss: 0.027245\n",
      "best loss:  0.027244939062701062\n",
      "Epoch 82 of 500, Train Loss: 0.027200\n",
      "best loss:  0.02719953596906\n",
      "Epoch 83 of 500, Train Loss: 0.027158\n",
      "best loss:  0.02715774866582667\n",
      "Epoch 84 of 500, Train Loss: 0.027120\n",
      "best loss:  0.02711950741345496\n",
      "Epoch 85 of 500, Train Loss: 0.027085\n",
      "best loss:  0.02708502324900857\n",
      "Epoch 86 of 500, Train Loss: 0.027054\n",
      "best loss:  0.02705393673882769\n",
      "Epoch 87 of 500, Train Loss: 0.027026\n",
      "best loss:  0.027026027711758088\n",
      "Epoch 88 of 500, Train Loss: 0.027001\n",
      "best loss:  0.02700115795664186\n",
      "Epoch 89 of 500, Train Loss: 0.026979\n",
      "best loss:  0.026979186085265765\n",
      "Epoch 90 of 500, Train Loss: 0.026959\n",
      "best loss:  0.026959412148251205\n",
      "Epoch 91 of 500, Train Loss: 0.026942\n",
      "best loss:  0.02694205804042114\n",
      "Epoch 92 of 500, Train Loss: 0.026926\n",
      "best loss:  0.02692642637939994\n",
      "Epoch 93 of 500, Train Loss: 0.026912\n",
      "best loss:  0.026912426265577476\n",
      "Epoch 94 of 500, Train Loss: 0.026900\n",
      "best loss:  0.026899842325644928\n",
      "Epoch 95 of 500, Train Loss: 0.026888\n",
      "best loss:  0.02688846277035123\n",
      "Epoch 96 of 500, Train Loss: 0.026878\n",
      "best loss:  0.026878214036944764\n",
      "Epoch 97 of 500, Train Loss: 0.026869\n",
      "best loss:  0.026868855270697888\n",
      "Epoch 98 of 500, Train Loss: 0.026860\n",
      "best loss:  0.026860296150016085\n",
      "Epoch 99 of 500, Train Loss: 0.026853\n",
      "best loss:  0.026852582698659033\n",
      "Epoch 100 of 500, Train Loss: 0.026845\n",
      "best loss:  0.026845352566266766\n",
      "Epoch 101 of 500, Train Loss: 0.026839\n",
      "best loss:  0.026838624161801685\n",
      "Epoch 102 of 500, Train Loss: 0.026833\n",
      "best loss:  0.026832626213846626\n",
      "Epoch 103 of 500, Train Loss: 0.026827\n",
      "best loss:  0.02682713396186659\n",
      "Epoch 104 of 500, Train Loss: 0.026822\n",
      "best loss:  0.02682204237848506\n",
      "Epoch 105 of 500, Train Loss: 0.026818\n",
      "best loss:  0.02681750848676374\n",
      "Epoch 106 of 500, Train Loss: 0.026813\n",
      "best loss:  0.02681317315806902\n",
      "Epoch 107 of 500, Train Loss: 0.026809\n",
      "best loss:  0.0268093974089481\n",
      "Epoch 108 of 500, Train Loss: 0.026806\n",
      "best loss:  0.026805658841888284\n",
      "Epoch 109 of 500, Train Loss: 0.026802\n",
      "best loss:  0.026801706406796827\n",
      "Epoch 110 of 500, Train Loss: 0.026798\n",
      "best loss:  0.026798391882095544\n",
      "Epoch 111 of 500, Train Loss: 0.026795\n",
      "best loss:  0.026794890943073572\n",
      "Epoch 112 of 500, Train Loss: 0.026792\n",
      "best loss:  0.02679208453064207\n",
      "Epoch 113 of 500, Train Loss: 0.026789\n",
      "best loss:  0.02678851956229065\n",
      "Epoch 114 of 500, Train Loss: 0.026785\n",
      "best loss:  0.026785245041142745\n",
      "Epoch 115 of 500, Train Loss: 0.026782\n",
      "best loss:  0.026781990654430852\n",
      "Epoch 116 of 500, Train Loss: 0.026779\n",
      "best loss:  0.02677874765476183\n",
      "Epoch 117 of 500, Train Loss: 0.026775\n",
      "best loss:  0.02677545298338514\n",
      "Epoch 118 of 500, Train Loss: 0.026772\n",
      "best loss:  0.0267720994667706\n",
      "Epoch 119 of 500, Train Loss: 0.026768\n",
      "best loss:  0.026768249098135723\n",
      "Epoch 120 of 500, Train Loss: 0.026765\n",
      "best loss:  0.026764687422803756\n",
      "Epoch 121 of 500, Train Loss: 0.026761\n",
      "best loss:  0.026760707774953988\n",
      "Epoch 122 of 500, Train Loss: 0.026757\n",
      "best loss:  0.02675699459275144\n",
      "Epoch 123 of 500, Train Loss: 0.026753\n",
      "best loss:  0.02675276418855457\n",
      "Epoch 124 of 500, Train Loss: 0.026749\n",
      "best loss:  0.02674887901579947\n",
      "Epoch 125 of 500, Train Loss: 0.026744\n",
      "best loss:  0.026744473166041827\n",
      "Epoch 126 of 500, Train Loss: 0.026741\n",
      "best loss:  0.026740518553803366\n",
      "Epoch 127 of 500, Train Loss: 0.026736\n",
      "best loss:  0.0267359608402584\n",
      "Epoch 128 of 500, Train Loss: 0.026732\n",
      "best loss:  0.02673179083350367\n",
      "Epoch 129 of 500, Train Loss: 0.026727\n",
      "best loss:  0.026727215034522606\n",
      "Epoch 130 of 500, Train Loss: 0.026723\n",
      "best loss:  0.026723024585598342\n",
      "Epoch 131 of 500, Train Loss: 0.026718\n",
      "best loss:  0.026718378160419592\n",
      "Epoch 132 of 500, Train Loss: 0.026714\n",
      "best loss:  0.026714357178362246\n",
      "Epoch 133 of 500, Train Loss: 0.026709\n",
      "best loss:  0.026709470742241326\n",
      "Epoch 134 of 500, Train Loss: 0.026705\n",
      "best loss:  0.026705238929328818\n",
      "Epoch 135 of 500, Train Loss: 0.026700\n",
      "best loss:  0.026700448136968005\n",
      "Epoch 136 of 500, Train Loss: 0.026696\n",
      "best loss:  0.02669616247565204\n",
      "Epoch 137 of 500, Train Loss: 0.026691\n",
      "best loss:  0.02669130822114805\n",
      "Epoch 138 of 500, Train Loss: 0.026687\n",
      "best loss:  0.026687065090229663\n",
      "Epoch 139 of 500, Train Loss: 0.026682\n",
      "best loss:  0.026682078926787936\n",
      "Epoch 140 of 500, Train Loss: 0.026678\n",
      "best loss:  0.02667763391365261\n",
      "Epoch 141 of 500, Train Loss: 0.026673\n",
      "best loss:  0.026672611048250177\n",
      "Epoch 142 of 500, Train Loss: 0.026668\n",
      "best loss:  0.026668215384381976\n",
      "Epoch 143 of 500, Train Loss: 0.026663\n",
      "best loss:  0.02666291378189398\n",
      "Epoch 144 of 500, Train Loss: 0.026658\n",
      "best loss:  0.026658112427775653\n",
      "Epoch 145 of 500, Train Loss: 0.026653\n",
      "best loss:  0.026652822098171325\n",
      "Epoch 146 of 500, Train Loss: 0.026648\n",
      "best loss:  0.026647845055734695\n",
      "Epoch 147 of 500, Train Loss: 0.026642\n",
      "best loss:  0.026642373584792045\n",
      "Epoch 148 of 500, Train Loss: 0.026637\n",
      "best loss:  0.026637106901937158\n",
      "Epoch 149 of 500, Train Loss: 0.026632\n",
      "best loss:  0.026631549789025197\n",
      "Epoch 150 of 500, Train Loss: 0.026626\n",
      "best loss:  0.0266261504044352\n",
      "Epoch 151 of 500, Train Loss: 0.026620\n",
      "best loss:  0.026620229965169703\n",
      "Epoch 152 of 500, Train Loss: 0.026615\n",
      "best loss:  0.02661462199410366\n",
      "Epoch 153 of 500, Train Loss: 0.026608\n",
      "best loss:  0.026608480615252558\n",
      "Epoch 154 of 500, Train Loss: 0.026603\n",
      "best loss:  0.02660276893437876\n",
      "Epoch 155 of 500, Train Loss: 0.026596\n",
      "best loss:  0.026596473839022076\n",
      "Epoch 156 of 500, Train Loss: 0.026590\n",
      "best loss:  0.026590464668269018\n",
      "Epoch 157 of 500, Train Loss: 0.026584\n",
      "best loss:  0.0265840590049787\n",
      "Epoch 158 of 500, Train Loss: 0.026578\n",
      "best loss:  0.0265779757521173\n",
      "Epoch 159 of 500, Train Loss: 0.026572\n",
      "best loss:  0.026571511523346886\n",
      "Epoch 160 of 500, Train Loss: 0.026565\n",
      "best loss:  0.026565322659227477\n",
      "Epoch 161 of 500, Train Loss: 0.026559\n",
      "best loss:  0.026558722111273757\n",
      "Epoch 162 of 500, Train Loss: 0.026552\n",
      "best loss:  0.02655246573032869\n",
      "Epoch 163 of 500, Train Loss: 0.026546\n",
      "best loss:  0.02654576766107939\n",
      "Epoch 164 of 500, Train Loss: 0.026539\n",
      "best loss:  0.026539459575938652\n",
      "Epoch 165 of 500, Train Loss: 0.026533\n",
      "best loss:  0.02653273267539709\n",
      "Epoch 166 of 500, Train Loss: 0.026526\n",
      "best loss:  0.026526429634017856\n",
      "Epoch 167 of 500, Train Loss: 0.026520\n",
      "best loss:  0.026519814193476165\n",
      "Epoch 168 of 500, Train Loss: 0.026513\n",
      "best loss:  0.026513484506073676\n",
      "Epoch 169 of 500, Train Loss: 0.026507\n",
      "best loss:  0.026506927388705775\n",
      "Epoch 170 of 500, Train Loss: 0.026501\n",
      "best loss:  0.026500606759588503\n",
      "Epoch 171 of 500, Train Loss: 0.026494\n",
      "best loss:  0.02649412514252025\n",
      "Epoch 172 of 500, Train Loss: 0.026488\n",
      "best loss:  0.026488122320914454\n",
      "Epoch 173 of 500, Train Loss: 0.026482\n",
      "best loss:  0.026481731063343384\n",
      "Epoch 174 of 500, Train Loss: 0.026476\n",
      "best loss:  0.026475643522517626\n",
      "Epoch 175 of 500, Train Loss: 0.026469\n",
      "best loss:  0.02646943713924595\n",
      "Epoch 176 of 500, Train Loss: 0.026464\n",
      "best loss:  0.02646351157274285\n",
      "Epoch 177 of 500, Train Loss: 0.026458\n",
      "best loss:  0.026457612969923418\n",
      "Epoch 178 of 500, Train Loss: 0.026452\n",
      "best loss:  0.026451911731233256\n",
      "Epoch 179 of 500, Train Loss: 0.026446\n",
      "best loss:  0.02644633528029243\n",
      "Epoch 180 of 500, Train Loss: 0.026441\n",
      "best loss:  0.026440910730590364\n",
      "Epoch 181 of 500, Train Loss: 0.026436\n",
      "best loss:  0.02643556446838734\n",
      "Epoch 182 of 500, Train Loss: 0.026430\n",
      "best loss:  0.02643033453577393\n",
      "Epoch 183 of 500, Train Loss: 0.026425\n",
      "best loss:  0.026425214126966027\n",
      "Epoch 184 of 500, Train Loss: 0.026420\n",
      "best loss:  0.026420308162400485\n",
      "Epoch 185 of 500, Train Loss: 0.026415\n",
      "best loss:  0.026415450158240202\n",
      "Epoch 186 of 500, Train Loss: 0.026411\n",
      "best loss:  0.02641072863841054\n",
      "Epoch 187 of 500, Train Loss: 0.026406\n",
      "best loss:  0.026406204274696723\n",
      "Epoch 188 of 500, Train Loss: 0.026402\n",
      "best loss:  0.026401944797352504\n",
      "Epoch 189 of 500, Train Loss: 0.026398\n",
      "best loss:  0.026397736222382145\n",
      "Epoch 190 of 500, Train Loss: 0.026394\n",
      "best loss:  0.026393763308572158\n",
      "Epoch 191 of 500, Train Loss: 0.026390\n",
      "best loss:  0.026389816837284225\n",
      "Epoch 192 of 500, Train Loss: 0.026386\n",
      "best loss:  0.026386061629150494\n",
      "Epoch 193 of 500, Train Loss: 0.026383\n",
      "best loss:  0.026382509070311404\n",
      "Epoch 194 of 500, Train Loss: 0.026379\n",
      "best loss:  0.02637910702475166\n",
      "Epoch 195 of 500, Train Loss: 0.026376\n",
      "best loss:  0.026375872096544954\n",
      "Epoch 196 of 500, Train Loss: 0.026373\n",
      "best loss:  0.02637280138162535\n",
      "Epoch 197 of 500, Train Loss: 0.026370\n",
      "best loss:  0.026369821993623276\n",
      "Epoch 198 of 500, Train Loss: 0.026367\n",
      "best loss:  0.026366995572646747\n",
      "Epoch 199 of 500, Train Loss: 0.026364\n",
      "best loss:  0.026364334957320386\n",
      "Epoch 200 of 500, Train Loss: 0.026362\n",
      "best loss:  0.026361749822891124\n",
      "Epoch 201 of 500, Train Loss: 0.026359\n",
      "best loss:  0.026359295134086077\n",
      "Epoch 202 of 500, Train Loss: 0.026357\n",
      "best loss:  0.026357025523532845\n",
      "Epoch 203 of 500, Train Loss: 0.026355\n",
      "best loss:  0.026354892642775102\n",
      "Epoch 204 of 500, Train Loss: 0.026353\n",
      "best loss:  0.026352869560165953\n",
      "Epoch 205 of 500, Train Loss: 0.026351\n",
      "best loss:  0.026350928359491706\n",
      "Epoch 206 of 500, Train Loss: 0.026349\n",
      "best loss:  0.026349193556865953\n",
      "Epoch 207 of 500, Train Loss: 0.026347\n",
      "best loss:  0.026347457030571477\n",
      "Epoch 208 of 500, Train Loss: 0.026346\n",
      "best loss:  0.02634586328496189\n",
      "Epoch 209 of 500, Train Loss: 0.026344\n",
      "best loss:  0.02634446148610811\n",
      "Epoch 210 of 500, Train Loss: 0.026343\n",
      "best loss:  0.02634313957433599\n",
      "Epoch 211 of 500, Train Loss: 0.026342\n",
      "best loss:  0.026341814561765063\n",
      "Epoch 212 of 500, Train Loss: 0.026341\n",
      "best loss:  0.02634064175861765\n",
      "Epoch 213 of 500, Train Loss: 0.026339\n",
      "best loss:  0.026339293107870504\n",
      "Epoch 214 of 500, Train Loss: 0.026338\n",
      "best loss:  0.026338185373668437\n",
      "Epoch 215 of 500, Train Loss: 0.026337\n",
      "best loss:  0.02633718669400081\n",
      "Epoch 216 of 500, Train Loss: 0.026336\n",
      "best loss:  0.0263363934891811\n",
      "Epoch 217 of 500, Train Loss: 0.026335\n",
      "best loss:  0.026335415572501977\n",
      "Epoch 218 of 500, Train Loss: 0.026335\n",
      "best loss:  0.026334561980167013\n",
      "Epoch 219 of 500, Train Loss: 0.026334\n",
      "best loss:  0.026333850370756126\n",
      "Epoch 220 of 500, Train Loss: 0.026333\n",
      "best loss:  0.026333108540288672\n",
      "Epoch 221 of 500, Train Loss: 0.026332\n",
      "best loss:  0.026332320100375155\n",
      "Epoch 222 of 500, Train Loss: 0.026332\n",
      "best loss:  0.026331641630977103\n",
      "Epoch 223 of 500, Train Loss: 0.026331\n",
      "best loss:  0.02633094993715947\n",
      "Epoch 224 of 500, Train Loss: 0.026330\n",
      "best loss:  0.02633030126060814\n",
      "Epoch 225 of 500, Train Loss: 0.026330\n",
      "best loss:  0.026329688728306905\n",
      "Epoch 226 of 500, Train Loss: 0.026329\n",
      "best loss:  0.02632908654839609\n",
      "Epoch 227 of 500, Train Loss: 0.026328\n",
      "best loss:  0.026328493621698472\n",
      "Epoch 228 of 500, Train Loss: 0.026328\n",
      "best loss:  0.02632795357372785\n",
      "Epoch 229 of 500, Train Loss: 0.026327\n",
      "best loss:  0.026327393703566966\n",
      "Epoch 230 of 500, Train Loss: 0.026327\n",
      "best loss:  0.026326883525602057\n",
      "Epoch 231 of 500, Train Loss: 0.026326\n",
      "best loss:  0.026326348127024755\n",
      "Epoch 232 of 500, Train Loss: 0.026326\n",
      "best loss:  0.026325876581349026\n",
      "Epoch 233 of 500, Train Loss: 0.026325\n",
      "best loss:  0.02632541722814776\n",
      "Epoch 234 of 500, Train Loss: 0.026325\n",
      "best loss:  0.026324870295880376\n",
      "Epoch 235 of 500, Train Loss: 0.026324\n",
      "best loss:  0.026324359928853378\n",
      "Epoch 236 of 500, Train Loss: 0.026324\n",
      "best loss:  0.02632391267652847\n",
      "Epoch 237 of 500, Train Loss: 0.026323\n",
      "best loss:  0.026323424232582195\n",
      "Epoch 238 of 500, Train Loss: 0.026323\n",
      "best loss:  0.02632288412234291\n",
      "Epoch 239 of 500, Train Loss: 0.026322\n",
      "best loss:  0.02632233384516552\n",
      "Epoch 240 of 500, Train Loss: 0.026322\n",
      "best loss:  0.02632185273538452\n",
      "Epoch 241 of 500, Train Loss: 0.026321\n",
      "best loss:  0.026321335068033906\n",
      "Epoch 242 of 500, Train Loss: 0.026321\n",
      "best loss:  0.02632085290058812\n",
      "Epoch 243 of 500, Train Loss: 0.026320\n",
      "best loss:  0.0263203411456017\n",
      "Epoch 244 of 500, Train Loss: 0.026320\n",
      "best loss:  0.026319861261340134\n",
      "Epoch 245 of 500, Train Loss: 0.026319\n",
      "best loss:  0.026319360538284736\n",
      "Epoch 246 of 500, Train Loss: 0.026319\n",
      "best loss:  0.02631889201760249\n",
      "Epoch 247 of 500, Train Loss: 0.026318\n",
      "best loss:  0.026318479471482595\n",
      "Epoch 248 of 500, Train Loss: 0.026318\n",
      "best loss:  0.0263179856823584\n",
      "Epoch 249 of 500, Train Loss: 0.026317\n",
      "best loss:  0.026317486283640095\n",
      "Epoch 250 of 500, Train Loss: 0.026317\n",
      "best loss:  0.026316995760462022\n",
      "Epoch 251 of 500, Train Loss: 0.026316\n",
      "best loss:  0.026316487109884188\n",
      "Epoch 252 of 500, Train Loss: 0.026316\n",
      "best loss:  0.02631600521226926\n",
      "Epoch 253 of 500, Train Loss: 0.026316\n",
      "best loss:  0.026315546127545192\n",
      "Epoch 254 of 500, Train Loss: 0.026315\n",
      "best loss:  0.026315080295695903\n",
      "Epoch 255 of 500, Train Loss: 0.026315\n",
      "best loss:  0.02631460090642506\n",
      "Epoch 256 of 500, Train Loss: 0.026314\n",
      "best loss:  0.026314137339711104\n",
      "Epoch 257 of 500, Train Loss: 0.026314\n",
      "best loss:  0.026313689179978398\n",
      "Epoch 258 of 500, Train Loss: 0.026313\n",
      "best loss:  0.02631326752819496\n",
      "Epoch 259 of 500, Train Loss: 0.026313\n",
      "best loss:  0.02631285854176683\n",
      "Epoch 260 of 500, Train Loss: 0.026312\n",
      "best loss:  0.026312444040952468\n",
      "Epoch 261 of 500, Train Loss: 0.026312\n",
      "best loss:  0.02631196716769649\n",
      "Epoch 262 of 500, Train Loss: 0.026312\n",
      "best loss:  0.026311520996951145\n",
      "Epoch 263 of 500, Train Loss: 0.026311\n",
      "best loss:  0.026311074441764504\n",
      "Epoch 264 of 500, Train Loss: 0.026311\n",
      "best loss:  0.02631063793349087\n",
      "Epoch 265 of 500, Train Loss: 0.026310\n",
      "best loss:  0.02631021577724104\n",
      "Epoch 266 of 500, Train Loss: 0.026310\n",
      "best loss:  0.026309809874916493\n",
      "Epoch 267 of 500, Train Loss: 0.026309\n",
      "best loss:  0.026309403853018068\n",
      "Epoch 268 of 500, Train Loss: 0.026309\n",
      "best loss:  0.026309003989399924\n",
      "Epoch 269 of 500, Train Loss: 0.026309\n",
      "best loss:  0.026308600553907022\n",
      "Epoch 270 of 500, Train Loss: 0.026308\n",
      "best loss:  0.026308208713470418\n",
      "Epoch 271 of 500, Train Loss: 0.026308\n",
      "best loss:  0.02630783345499762\n",
      "Epoch 272 of 500, Train Loss: 0.026307\n",
      "best loss:  0.026307488350349285\n",
      "Epoch 273 of 500, Train Loss: 0.026307\n",
      "best loss:  0.02630707867129511\n",
      "Epoch 274 of 500, Train Loss: 0.026307\n",
      "best loss:  0.026306685320617876\n",
      "Epoch 275 of 500, Train Loss: 0.026306\n",
      "best loss:  0.026306302494246713\n",
      "Epoch 276 of 500, Train Loss: 0.026306\n",
      "best loss:  0.026305888674777837\n",
      "Epoch 277 of 500, Train Loss: 0.026306\n",
      "best loss:  0.02630550687538835\n",
      "Epoch 278 of 500, Train Loss: 0.026305\n",
      "best loss:  0.02630513718138745\n",
      "Epoch 279 of 500, Train Loss: 0.026305\n",
      "best loss:  0.026304772879491567\n",
      "Epoch 280 of 500, Train Loss: 0.026304\n",
      "best loss:  0.02630441419576302\n",
      "Epoch 281 of 500, Train Loss: 0.026304\n",
      "best loss:  0.026304050510236633\n",
      "Epoch 282 of 500, Train Loss: 0.026304\n",
      "best loss:  0.026303689800069283\n",
      "Epoch 283 of 500, Train Loss: 0.026303\n",
      "best loss:  0.026303317683907732\n",
      "Epoch 284 of 500, Train Loss: 0.026303\n",
      "best loss:  0.026302952489945606\n",
      "Epoch 285 of 500, Train Loss: 0.026303\n",
      "best loss:  0.026302588344623817\n",
      "Epoch 286 of 500, Train Loss: 0.026302\n",
      "best loss:  0.02630223525108683\n",
      "Epoch 287 of 500, Train Loss: 0.026302\n",
      "best loss:  0.026301882330819155\n",
      "Epoch 288 of 500, Train Loss: 0.026302\n",
      "best loss:  0.026301538217054724\n",
      "Epoch 289 of 500, Train Loss: 0.026301\n",
      "best loss:  0.026301188046535\n",
      "Epoch 290 of 500, Train Loss: 0.026301\n",
      "best loss:  0.02630084808356338\n",
      "Epoch 291 of 500, Train Loss: 0.026300\n",
      "best loss:  0.026300498888586005\n",
      "Epoch 292 of 500, Train Loss: 0.026300\n",
      "best loss:  0.026300161667240335\n",
      "Epoch 293 of 500, Train Loss: 0.026300\n",
      "best loss:  0.02629981379389272\n",
      "Epoch 294 of 500, Train Loss: 0.026299\n",
      "best loss:  0.026299479246038454\n",
      "Epoch 295 of 500, Train Loss: 0.026299\n",
      "best loss:  0.02629913296054168\n",
      "Epoch 296 of 500, Train Loss: 0.026299\n",
      "best loss:  0.026298801662840758\n",
      "Epoch 297 of 500, Train Loss: 0.026298\n",
      "best loss:  0.026298456679119966\n",
      "Epoch 298 of 500, Train Loss: 0.026298\n",
      "best loss:  0.02629812457643868\n",
      "Epoch 299 of 500, Train Loss: 0.026298\n",
      "best loss:  0.026297771074996843\n",
      "Epoch 300 of 500, Train Loss: 0.026297\n",
      "best loss:  0.026297433316696782\n",
      "Epoch 301 of 500, Train Loss: 0.026297\n",
      "best loss:  0.026297085185701303\n",
      "Epoch 302 of 500, Train Loss: 0.026297\n",
      "best loss:  0.02629676172257105\n",
      "Epoch 303 of 500, Train Loss: 0.026296\n",
      "best loss:  0.02629642788900149\n",
      "Epoch 304 of 500, Train Loss: 0.026296\n",
      "best loss:  0.026296118065866448\n",
      "Epoch 305 of 500, Train Loss: 0.026296\n",
      "best loss:  0.02629580458873421\n",
      "Epoch 306 of 500, Train Loss: 0.026296\n",
      "best loss:  0.02629551805543186\n",
      "Epoch 307 of 500, Train Loss: 0.026295\n",
      "best loss:  0.026295179797177628\n",
      "Epoch 308 of 500, Train Loss: 0.026295\n",
      "best loss:  0.02629495382240374\n",
      "Epoch 309 of 500, Train Loss: 0.026295\n",
      "best loss:  0.026294565557902896\n",
      "Epoch 310 of 500, Train Loss: 0.026294\n",
      "best loss:  0.026294186642725568\n",
      "Epoch 311 of 500, Train Loss: 0.026294\n",
      "best loss:  0.026293818802796593\n",
      "Epoch 312 of 500, Train Loss: 0.026293\n",
      "best loss:  0.02629349674432098\n",
      "Epoch 313 of 500, Train Loss: 0.026293\n",
      "best loss:  0.026293168802359393\n",
      "Epoch 314 of 500, Train Loss: 0.026293\n",
      "best loss:  0.02629286370622788\n",
      "Epoch 315 of 500, Train Loss: 0.026293\n",
      "best loss:  0.026292535820217847\n",
      "Epoch 316 of 500, Train Loss: 0.026292\n",
      "best loss:  0.02629221788816905\n",
      "Epoch 317 of 500, Train Loss: 0.026292\n",
      "best loss:  0.026291874692641746\n",
      "Epoch 318 of 500, Train Loss: 0.026292\n",
      "best loss:  0.026291544740936583\n",
      "Epoch 319 of 500, Train Loss: 0.026291\n",
      "best loss:  0.0262911957187773\n",
      "Epoch 320 of 500, Train Loss: 0.026291\n",
      "best loss:  0.026290871040505724\n",
      "Epoch 321 of 500, Train Loss: 0.026291\n",
      "best loss:  0.02629053511507015\n",
      "Epoch 322 of 500, Train Loss: 0.026290\n",
      "best loss:  0.02629022298483611\n",
      "Epoch 323 of 500, Train Loss: 0.026290\n",
      "best loss:  0.026289893348084025\n",
      "Epoch 324 of 500, Train Loss: 0.026290\n",
      "best loss:  0.026289584474320445\n",
      "Epoch 325 of 500, Train Loss: 0.026289\n",
      "best loss:  0.026289278666160338\n",
      "Epoch 326 of 500, Train Loss: 0.026289\n",
      "best loss:  0.026289005343740908\n",
      "Epoch 327 of 500, Train Loss: 0.026289\n",
      "best loss:  0.026288613211814776\n",
      "Epoch 328 of 500, Train Loss: 0.026288\n",
      "best loss:  0.026288306874822278\n",
      "Epoch 329 of 500, Train Loss: 0.026288\n",
      "best loss:  0.026287946494949564\n",
      "Epoch 330 of 500, Train Loss: 0.026288\n",
      "best loss:  0.02628757948481817\n",
      "Epoch 331 of 500, Train Loss: 0.026287\n",
      "best loss:  0.026287220981578496\n",
      "Epoch 332 of 500, Train Loss: 0.026287\n",
      "best loss:  0.026286887319473363\n",
      "Epoch 333 of 500, Train Loss: 0.026287\n",
      "best loss:  0.026286541087220807\n",
      "Epoch 334 of 500, Train Loss: 0.026286\n",
      "best loss:  0.026286212184155982\n",
      "Epoch 335 of 500, Train Loss: 0.026286\n",
      "best loss:  0.02628586548894947\n",
      "Epoch 336 of 500, Train Loss: 0.026286\n",
      "best loss:  0.026285534333834758\n",
      "Epoch 337 of 500, Train Loss: 0.026285\n",
      "best loss:  0.026285184594232215\n",
      "Epoch 338 of 500, Train Loss: 0.026285\n",
      "best loss:  0.026284850114963718\n",
      "Epoch 339 of 500, Train Loss: 0.026284\n",
      "best loss:  0.026284497278623368\n",
      "Epoch 340 of 500, Train Loss: 0.026284\n",
      "best loss:  0.02628415880739227\n",
      "Epoch 341 of 500, Train Loss: 0.026284\n",
      "best loss:  0.026283803149559644\n",
      "Epoch 342 of 500, Train Loss: 0.026283\n",
      "best loss:  0.02628345854892648\n",
      "Epoch 343 of 500, Train Loss: 0.026283\n",
      "best loss:  0.02628309147878254\n",
      "Epoch 344 of 500, Train Loss: 0.026283\n",
      "best loss:  0.026282741238774603\n",
      "Epoch 345 of 500, Train Loss: 0.026282\n",
      "best loss:  0.02628237683941473\n",
      "Epoch 346 of 500, Train Loss: 0.026282\n",
      "best loss:  0.026282037833034125\n",
      "Epoch 347 of 500, Train Loss: 0.026282\n",
      "best loss:  0.026281689462890893\n",
      "Epoch 348 of 500, Train Loss: 0.026281\n",
      "best loss:  0.026281354137129592\n",
      "Epoch 349 of 500, Train Loss: 0.026281\n",
      "best loss:  0.02628102345309083\n",
      "Epoch 350 of 500, Train Loss: 0.026281\n",
      "best loss:  0.02628064408939866\n",
      "Epoch 351 of 500, Train Loss: 0.026280\n",
      "best loss:  0.026280236348343738\n",
      "Epoch 352 of 500, Train Loss: 0.026280\n",
      "best loss:  0.026279875642688612\n",
      "Epoch 353 of 500, Train Loss: 0.026280\n",
      "best loss:  0.02627950233530051\n",
      "Epoch 354 of 500, Train Loss: 0.026279\n",
      "best loss:  0.026279149323434736\n",
      "Epoch 355 of 500, Train Loss: 0.026279\n",
      "best loss:  0.026278776966320538\n",
      "Epoch 356 of 500, Train Loss: 0.026278\n",
      "best loss:  0.026278417849869923\n",
      "Epoch 357 of 500, Train Loss: 0.026278\n",
      "best loss:  0.026278026688522697\n",
      "Epoch 358 of 500, Train Loss: 0.026278\n",
      "best loss:  0.02627765121256081\n",
      "Epoch 359 of 500, Train Loss: 0.026277\n",
      "best loss:  0.02627725462481567\n",
      "Epoch 360 of 500, Train Loss: 0.026277\n",
      "best loss:  0.026276883477879355\n",
      "Epoch 361 of 500, Train Loss: 0.026276\n",
      "best loss:  0.026276490920901934\n",
      "Epoch 362 of 500, Train Loss: 0.026276\n",
      "best loss:  0.026276123277254603\n",
      "Epoch 363 of 500, Train Loss: 0.026276\n",
      "best loss:  0.026275733807990545\n",
      "Epoch 364 of 500, Train Loss: 0.026275\n",
      "best loss:  0.02627536814611102\n",
      "Epoch 365 of 500, Train Loss: 0.026275\n",
      "best loss:  0.026274973618646346\n",
      "Epoch 366 of 500, Train Loss: 0.026275\n",
      "best loss:  0.026274598152611347\n",
      "Epoch 367 of 500, Train Loss: 0.026274\n",
      "best loss:  0.02627419309813327\n",
      "Epoch 368 of 500, Train Loss: 0.026274\n",
      "best loss:  0.026273810298384218\n",
      "Epoch 369 of 500, Train Loss: 0.026273\n",
      "best loss:  0.0262733981524984\n",
      "Epoch 370 of 500, Train Loss: 0.026273\n",
      "best loss:  0.026273016105387938\n",
      "Epoch 371 of 500, Train Loss: 0.026273\n",
      "best loss:  0.02627260819286859\n",
      "Epoch 372 of 500, Train Loss: 0.026272\n",
      "best loss:  0.02627223670571075\n",
      "Epoch 373 of 500, Train Loss: 0.026272\n",
      "best loss:  0.026271837830269176\n",
      "Epoch 374 of 500, Train Loss: 0.026271\n",
      "best loss:  0.026271461933316996\n",
      "Epoch 375 of 500, Train Loss: 0.026271\n",
      "best loss:  0.026271031420883658\n",
      "Epoch 376 of 500, Train Loss: 0.026271\n",
      "best loss:  0.026270635427849354\n",
      "Epoch 377 of 500, Train Loss: 0.026270\n",
      "best loss:  0.02627021706863415\n",
      "Epoch 378 of 500, Train Loss: 0.026270\n",
      "best loss:  0.026269841183864967\n",
      "Epoch 379 of 500, Train Loss: 0.026269\n",
      "best loss:  0.026269430628988545\n",
      "Epoch 380 of 500, Train Loss: 0.026269\n",
      "best loss:  0.02626906378851669\n",
      "Epoch 381 of 500, Train Loss: 0.026269\n",
      "best loss:  0.026268666786098377\n",
      "Epoch 382 of 500, Train Loss: 0.026268\n",
      "best loss:  0.026268300331872708\n",
      "Epoch 383 of 500, Train Loss: 0.026268\n",
      "best loss:  0.026267882958126737\n",
      "Epoch 384 of 500, Train Loss: 0.026267\n",
      "best loss:  0.0262674949724812\n",
      "Epoch 385 of 500, Train Loss: 0.026267\n",
      "best loss:  0.02626707056418148\n",
      "Epoch 386 of 500, Train Loss: 0.026267\n",
      "best loss:  0.02626670626724904\n",
      "Epoch 387 of 500, Train Loss: 0.026266\n",
      "best loss:  0.02626630553186965\n",
      "Epoch 388 of 500, Train Loss: 0.026266\n",
      "best loss:  0.026265951753377394\n",
      "Epoch 389 of 500, Train Loss: 0.026266\n",
      "best loss:  0.026265559141350866\n",
      "Epoch 390 of 500, Train Loss: 0.026265\n",
      "best loss:  0.026265202729525983\n",
      "Epoch 391 of 500, Train Loss: 0.026265\n",
      "best loss:  0.02626484261948413\n",
      "Epoch 392 of 500, Train Loss: 0.026264\n",
      "best loss:  0.026264463665050475\n",
      "Epoch 393 of 500, Train Loss: 0.026264\n",
      "best loss:  0.026264078316798563\n",
      "Epoch 394 of 500, Train Loss: 0.026264\n",
      "best loss:  0.026263780531819995\n",
      "Epoch 395 of 500, Train Loss: 0.026263\n",
      "best loss:  0.026263409087979486\n",
      "Epoch 396 of 500, Train Loss: 0.026263\n",
      "best loss:  0.0262630871296752\n",
      "Epoch 397 of 500, Train Loss: 0.026263\n",
      "best loss:  0.02626268939898429\n",
      "Epoch 398 of 500, Train Loss: 0.026262\n",
      "best loss:  0.026262343506953254\n",
      "Epoch 399 of 500, Train Loss: 0.026262\n",
      "best loss:  0.026261959916662945\n",
      "Epoch 400 of 500, Train Loss: 0.026262\n",
      "best loss:  0.02626167421451185\n",
      "Epoch 401 of 500, Train Loss: 0.026261\n",
      "best loss:  0.026261324191993\n",
      "Epoch 402 of 500, Train Loss: 0.026261\n",
      "best loss:  0.026261063659014976\n",
      "Epoch 403 of 500, Train Loss: 0.026261\n",
      "best loss:  0.026260727314393995\n",
      "Epoch 404 of 500, Train Loss: 0.026260\n",
      "best loss:  0.02626048113885444\n",
      "Epoch 405 of 500, Train Loss: 0.026260\n",
      "best loss:  0.026260149959824952\n",
      "Epoch 406 of 500, Train Loss: 0.026260\n",
      "best loss:  0.0262599221770038\n",
      "Epoch 407 of 500, Train Loss: 0.026260\n",
      "best loss:  0.026259612279868318\n",
      "Epoch 408 of 500, Train Loss: 0.026259\n",
      "best loss:  0.026259421241873977\n",
      "Epoch 409 of 500, Train Loss: 0.026259\n",
      "best loss:  0.02625914338467166\n",
      "Epoch 410 of 500, Train Loss: 0.026259\n",
      "best loss:  0.02625898889567371\n",
      "Epoch 411 of 500, Train Loss: 0.026259\n",
      "best loss:  0.02625872880173486\n",
      "Epoch 412 of 500, Train Loss: 0.026259\n",
      "best loss:  0.026258596558441486\n",
      "Epoch 413 of 500, Train Loss: 0.026258\n",
      "best loss:  0.026258356912538063\n",
      "Epoch 414 of 500, Train Loss: 0.026258\n",
      "best loss:  0.026258262797518698\n",
      "Epoch 415 of 500, Train Loss: 0.026258\n",
      "best loss:  0.026258056427445983\n",
      "Epoch 416 of 500, Train Loss: 0.026258\n",
      "best loss:  0.02625800849727261\n",
      "Epoch 417 of 500, Train Loss: 0.026258\n",
      "best loss:  0.02625783782158148\n",
      "Epoch 418 of 500, Train Loss: 0.026258\n",
      "best loss:  0.026257837759764042\n",
      "Epoch 419 of 500, Train Loss: 0.026258\n",
      "best loss:  0.02625770568026432\n",
      "Epoch 420 of 500, Train Loss: 0.026258\n",
      "Epoch 421 of 500, Train Loss: 0.026258\n",
      "best loss:  0.026257668597923316\n",
      "Epoch 422 of 500, Train Loss: 0.026258\n",
      "Epoch 423 of 500, Train Loss: 0.026258\n",
      "Epoch 424 of 500, Train Loss: 0.026258\n",
      "Epoch 425 of 500, Train Loss: 0.026258\n",
      "Epoch 426 of 500, Train Loss: 0.026258\n",
      "Epoch 427 of 500, Train Loss: 0.026258\n",
      "Epoch 428 of 500, Train Loss: 0.026258\n",
      "Epoch 429 of 500, Train Loss: 0.026259\n",
      "Epoch 430 of 500, Train Loss: 0.026259\n",
      "Epoch 431 of 500, Train Loss: 0.026259\n",
      "Epoch 432 of 500, Train Loss: 0.026260\n",
      "Epoch 433 of 500, Train Loss: 0.026260\n",
      "Epoch 434 of 500, Train Loss: 0.026260\n",
      "Epoch 435 of 500, Train Loss: 0.026261\n",
      "Epoch 436 of 500, Train Loss: 0.026261\n",
      "Epoch 437 of 500, Train Loss: 0.026262\n",
      "Epoch 438 of 500, Train Loss: 0.026262\n",
      "Epoch 439 of 500, Train Loss: 0.026263\n",
      "Epoch 440 of 500, Train Loss: 0.026264\n",
      "Epoch 441 of 500, Train Loss: 0.026264\n",
      "Epoch 442 of 500, Train Loss: 0.026265\n",
      "Epoch 443 of 500, Train Loss: 0.026266\n",
      "Epoch 444 of 500, Train Loss: 0.026267\n",
      "Epoch 445 of 500, Train Loss: 0.026268\n",
      "Epoch 446 of 500, Train Loss: 0.026269\n",
      "Epoch 447 of 500, Train Loss: 0.026270\n",
      "Epoch 448 of 500, Train Loss: 0.026271\n",
      "Epoch 449 of 500, Train Loss: 0.026272\n",
      "Epoch 450 of 500, Train Loss: 0.026274\n",
      "Epoch 451 of 500, Train Loss: 0.026275\n",
      "Epoch 452 of 500, Train Loss: 0.026276\n",
      "Epoch 453 of 500, Train Loss: 0.026277\n",
      "Epoch 454 of 500, Train Loss: 0.026279\n",
      "Epoch 455 of 500, Train Loss: 0.026280\n",
      "Epoch 456 of 500, Train Loss: 0.026282\n",
      "Epoch 457 of 500, Train Loss: 0.026284\n",
      "Epoch 458 of 500, Train Loss: 0.026286\n",
      "Epoch 459 of 500, Train Loss: 0.026287\n",
      "Epoch 460 of 500, Train Loss: 0.026289\n",
      "Epoch 461 of 500, Train Loss: 0.026291\n",
      "Epoch 462 of 500, Train Loss: 0.026293\n",
      "Epoch 463 of 500, Train Loss: 0.026294\n",
      "Epoch 464 of 500, Train Loss: 0.026297\n",
      "Epoch 465 of 500, Train Loss: 0.026298\n",
      "Epoch 466 of 500, Train Loss: 0.026301\n",
      "Epoch 467 of 500, Train Loss: 0.026302\n",
      "Epoch 468 of 500, Train Loss: 0.026305\n",
      "Epoch 469 of 500, Train Loss: 0.026306\n",
      "Epoch 470 of 500, Train Loss: 0.026308\n",
      "Epoch 471 of 500, Train Loss: 0.026310\n",
      "Epoch 472 of 500, Train Loss: 0.026312\n",
      "Epoch 473 of 500, Train Loss: 0.026313\n",
      "Epoch 474 of 500, Train Loss: 0.026315\n",
      "Epoch 475 of 500, Train Loss: 0.026316\n",
      "Epoch 476 of 500, Train Loss: 0.026319\n",
      "Epoch 477 of 500, Train Loss: 0.026320\n",
      "Epoch 478 of 500, Train Loss: 0.026323\n",
      "Epoch 479 of 500, Train Loss: 0.026323\n",
      "Epoch 480 of 500, Train Loss: 0.026325\n",
      "Epoch 481 of 500, Train Loss: 0.026325\n",
      "Epoch 482 of 500, Train Loss: 0.026328\n",
      "Epoch 483 of 500, Train Loss: 0.026328\n",
      "Epoch 484 of 500, Train Loss: 0.026330\n",
      "Epoch 485 of 500, Train Loss: 0.026331\n",
      "Epoch 486 of 500, Train Loss: 0.026332\n",
      "Epoch 487 of 500, Train Loss: 0.026332\n",
      "Epoch 488 of 500, Train Loss: 0.026333\n",
      "Epoch 489 of 500, Train Loss: 0.026334\n",
      "Epoch 490 of 500, Train Loss: 0.026336\n",
      "Epoch 491 of 500, Train Loss: 0.026335\n",
      "Epoch 492 of 500, Train Loss: 0.026337\n",
      "Epoch 493 of 500, Train Loss: 0.026336\n",
      "Epoch 494 of 500, Train Loss: 0.026338\n",
      "Epoch 495 of 500, Train Loss: 0.026337\n",
      "Epoch 496 of 500, Train Loss: 0.026339\n",
      "Epoch 497 of 500, Train Loss: 0.026338\n",
      "Epoch 498 of 500, Train Loss: 0.026340\n",
      "Epoch 499 of 500, Train Loss: 0.026339\n",
      "Epoch 500 of 500, Train Loss: 0.026340\n",
      "latent train shape:  (16395, 5)\n",
      "M: 5, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 34\n",
      "Training the subspace: 0 / 5\n",
      "Training the subspace: 1 / 5\n",
      "Training the subspace: 2 / 5\n",
      "Training the subspace: 3 / 5\n",
      "Training the subspace: 4 / 5\n",
      "Encoding the subspace: 0 / 5\n",
      "Encoding the subspace: 1 / 5\n",
      "Encoding the subspace: 2 / 5\n",
      "Encoding the subspace: 3 / 5\n",
      "Encoding the subspace: 4 / 5\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.747354\n",
      "best loss:  0.7473538901454718\n",
      "Epoch 2 of 500, Train Loss: 0.160504\n",
      "best loss:  0.16050411991832791\n",
      "Epoch 3 of 500, Train Loss: 0.059519\n",
      "best loss:  0.05951855918789615\n",
      "Epoch 4 of 500, Train Loss: 0.056530\n",
      "best loss:  0.05653025621012243\n",
      "Epoch 5 of 500, Train Loss: 0.056373\n",
      "best loss:  0.056372910016938296\n",
      "Epoch 6 of 500, Train Loss: 0.056179\n",
      "best loss:  0.05617866892766482\n",
      "Epoch 7 of 500, Train Loss: 0.055895\n",
      "best loss:  0.05589489097835121\n",
      "Epoch 8 of 500, Train Loss: 0.055476\n",
      "best loss:  0.055476220197842076\n",
      "Epoch 9 of 500, Train Loss: 0.054866\n",
      "best loss:  0.05486591515995837\n",
      "Epoch 10 of 500, Train Loss: 0.053996\n",
      "best loss:  0.05399555619625562\n",
      "Epoch 11 of 500, Train Loss: 0.052790\n",
      "best loss:  0.05278982315618588\n",
      "Epoch 12 of 500, Train Loss: 0.051182\n",
      "best loss:  0.05118169771261918\n",
      "Epoch 13 of 500, Train Loss: 0.049144\n",
      "best loss:  0.04914409233583859\n",
      "Epoch 14 of 500, Train Loss: 0.046728\n",
      "best loss:  0.04672784766032498\n",
      "Epoch 15 of 500, Train Loss: 0.044082\n",
      "best loss:  0.04408235490085673\n",
      "Epoch 16 of 500, Train Loss: 0.041422\n",
      "best loss:  0.041421602744529704\n",
      "Epoch 17 of 500, Train Loss: 0.038938\n",
      "best loss:  0.0389378518403089\n",
      "Epoch 18 of 500, Train Loss: 0.036734\n",
      "best loss:  0.036733872703883015\n",
      "Epoch 19 of 500, Train Loss: 0.034830\n",
      "best loss:  0.03482950118987626\n",
      "Epoch 20 of 500, Train Loss: 0.033205\n",
      "best loss:  0.03320484798331203\n",
      "Epoch 21 of 500, Train Loss: 0.031831\n",
      "best loss:  0.03183090680435863\n",
      "Epoch 22 of 500, Train Loss: 0.030681\n",
      "best loss:  0.030681032384655534\n",
      "Epoch 23 of 500, Train Loss: 0.029727\n",
      "best loss:  0.029726692303011643\n",
      "Epoch 24 of 500, Train Loss: 0.028938\n",
      "best loss:  0.02893772833103444\n",
      "Epoch 25 of 500, Train Loss: 0.028282\n",
      "best loss:  0.02828151685665831\n",
      "Epoch 26 of 500, Train Loss: 0.027726\n",
      "best loss:  0.02772646432051204\n",
      "Epoch 27 of 500, Train Loss: 0.027245\n",
      "best loss:  0.027245283647090224\n",
      "Epoch 28 of 500, Train Loss: 0.026818\n",
      "best loss:  0.026817798438976526\n",
      "Epoch 29 of 500, Train Loss: 0.026430\n",
      "best loss:  0.02642972371811697\n",
      "Epoch 30 of 500, Train Loss: 0.026072\n",
      "best loss:  0.026072353747305134\n",
      "Epoch 31 of 500, Train Loss: 0.025740\n",
      "best loss:  0.025739947518481795\n",
      "Epoch 32 of 500, Train Loss: 0.025430\n",
      "best loss:  0.025429733497787325\n",
      "Epoch 33 of 500, Train Loss: 0.025140\n",
      "best loss:  0.025140363265951562\n",
      "Epoch 34 of 500, Train Loss: 0.024871\n",
      "best loss:  0.024870724909101402\n",
      "Epoch 35 of 500, Train Loss: 0.024619\n",
      "best loss:  0.02461927732436506\n",
      "Epoch 36 of 500, Train Loss: 0.024386\n",
      "best loss:  0.02438595922584393\n",
      "Epoch 37 of 500, Train Loss: 0.024170\n",
      "best loss:  0.024169775910040204\n",
      "Epoch 38 of 500, Train Loss: 0.023969\n",
      "best loss:  0.02396936159239822\n",
      "Epoch 39 of 500, Train Loss: 0.023783\n",
      "best loss:  0.023783434646713092\n",
      "Epoch 40 of 500, Train Loss: 0.023610\n",
      "best loss:  0.023610484212377044\n",
      "Epoch 41 of 500, Train Loss: 0.023450\n",
      "best loss:  0.02344952926198526\n",
      "Epoch 42 of 500, Train Loss: 0.023299\n",
      "best loss:  0.02329943250492937\n",
      "Epoch 43 of 500, Train Loss: 0.023159\n",
      "best loss:  0.023158892452655462\n",
      "Epoch 44 of 500, Train Loss: 0.023027\n",
      "best loss:  0.02302703167250481\n",
      "Epoch 45 of 500, Train Loss: 0.022903\n",
      "best loss:  0.022903049650760874\n",
      "Epoch 46 of 500, Train Loss: 0.022787\n",
      "best loss:  0.022787175248665806\n",
      "Epoch 47 of 500, Train Loss: 0.022679\n",
      "best loss:  0.022678616260405803\n",
      "Epoch 48 of 500, Train Loss: 0.022577\n",
      "best loss:  0.022577084923549304\n",
      "Epoch 49 of 500, Train Loss: 0.022481\n",
      "best loss:  0.022480941311701728\n",
      "Epoch 50 of 500, Train Loss: 0.022390\n",
      "best loss:  0.022390147025404542\n",
      "Epoch 51 of 500, Train Loss: 0.022305\n",
      "best loss:  0.022304855403490365\n",
      "Epoch 52 of 500, Train Loss: 0.022224\n",
      "best loss:  0.022223819827375747\n",
      "Epoch 53 of 500, Train Loss: 0.022146\n",
      "best loss:  0.02214608737944387\n",
      "Epoch 54 of 500, Train Loss: 0.022071\n",
      "best loss:  0.022070733769868983\n",
      "Epoch 55 of 500, Train Loss: 0.021998\n",
      "best loss:  0.02199762042651545\n",
      "Epoch 56 of 500, Train Loss: 0.021926\n",
      "best loss:  0.02192630254223134\n",
      "Epoch 57 of 500, Train Loss: 0.021856\n",
      "best loss:  0.021856392609215406\n",
      "Epoch 58 of 500, Train Loss: 0.021788\n",
      "best loss:  0.021787944845422062\n",
      "Epoch 59 of 500, Train Loss: 0.021720\n",
      "best loss:  0.021719957428872443\n",
      "Epoch 60 of 500, Train Loss: 0.021653\n",
      "best loss:  0.021652511223544214\n",
      "Epoch 61 of 500, Train Loss: 0.021586\n",
      "best loss:  0.02158578643313602\n",
      "Epoch 62 of 500, Train Loss: 0.021519\n",
      "best loss:  0.021519230465023857\n",
      "Epoch 63 of 500, Train Loss: 0.021453\n",
      "best loss:  0.021453417473483484\n",
      "Epoch 64 of 500, Train Loss: 0.021388\n",
      "best loss:  0.02138815204028127\n",
      "Epoch 65 of 500, Train Loss: 0.021324\n",
      "best loss:  0.021323558181351007\n",
      "Epoch 66 of 500, Train Loss: 0.021260\n",
      "best loss:  0.021260408087193157\n",
      "Epoch 67 of 500, Train Loss: 0.021197\n",
      "best loss:  0.02119737582903016\n",
      "Epoch 68 of 500, Train Loss: 0.021136\n",
      "best loss:  0.021136162040913984\n",
      "Epoch 69 of 500, Train Loss: 0.021077\n",
      "best loss:  0.021076633179354546\n",
      "Epoch 70 of 500, Train Loss: 0.021019\n",
      "best loss:  0.021018642734541276\n",
      "Epoch 71 of 500, Train Loss: 0.020963\n",
      "best loss:  0.02096277017704962\n",
      "Epoch 72 of 500, Train Loss: 0.020909\n",
      "best loss:  0.02090896716135619\n",
      "Epoch 73 of 500, Train Loss: 0.020857\n",
      "best loss:  0.020857032989927877\n",
      "Epoch 74 of 500, Train Loss: 0.020808\n",
      "best loss:  0.020808173136428257\n",
      "Epoch 75 of 500, Train Loss: 0.020761\n",
      "best loss:  0.020761173591198213\n",
      "Epoch 76 of 500, Train Loss: 0.020716\n",
      "best loss:  0.020716313921346496\n",
      "Epoch 77 of 500, Train Loss: 0.020673\n",
      "best loss:  0.020673247708265792\n",
      "Epoch 78 of 500, Train Loss: 0.020633\n",
      "best loss:  0.020632603138508347\n",
      "Epoch 79 of 500, Train Loss: 0.020593\n",
      "best loss:  0.020593080471103736\n",
      "Epoch 80 of 500, Train Loss: 0.020555\n",
      "best loss:  0.02055539523590674\n",
      "Epoch 81 of 500, Train Loss: 0.020519\n",
      "best loss:  0.020519184223881583\n",
      "Epoch 82 of 500, Train Loss: 0.020484\n",
      "best loss:  0.020483990161961247\n",
      "Epoch 83 of 500, Train Loss: 0.020450\n",
      "best loss:  0.02044964123060638\n",
      "Epoch 84 of 500, Train Loss: 0.020417\n",
      "best loss:  0.020417006608042795\n",
      "Epoch 85 of 500, Train Loss: 0.020384\n",
      "best loss:  0.020384449485144326\n",
      "Epoch 86 of 500, Train Loss: 0.020352\n",
      "best loss:  0.020352180254855013\n",
      "Epoch 87 of 500, Train Loss: 0.020320\n",
      "best loss:  0.020319944848631356\n",
      "Epoch 88 of 500, Train Loss: 0.020289\n",
      "best loss:  0.02028882049895866\n",
      "Epoch 89 of 500, Train Loss: 0.020258\n",
      "best loss:  0.020257983896601585\n",
      "Epoch 90 of 500, Train Loss: 0.020227\n",
      "best loss:  0.020226763878351567\n",
      "Epoch 91 of 500, Train Loss: 0.020196\n",
      "best loss:  0.02019553929892217\n",
      "Epoch 92 of 500, Train Loss: 0.020164\n",
      "best loss:  0.020164466488072217\n",
      "Epoch 93 of 500, Train Loss: 0.020134\n",
      "best loss:  0.020133595070765578\n",
      "Epoch 94 of 500, Train Loss: 0.020102\n",
      "best loss:  0.020102229911713072\n",
      "Epoch 95 of 500, Train Loss: 0.020072\n",
      "best loss:  0.02007161105338614\n",
      "Epoch 96 of 500, Train Loss: 0.020040\n",
      "best loss:  0.02004025880853797\n",
      "Epoch 97 of 500, Train Loss: 0.020009\n",
      "best loss:  0.020009116832998965\n",
      "Epoch 98 of 500, Train Loss: 0.019978\n",
      "best loss:  0.019978274569157222\n",
      "Epoch 99 of 500, Train Loss: 0.019948\n",
      "best loss:  0.019947674545520815\n",
      "Epoch 100 of 500, Train Loss: 0.019917\n",
      "best loss:  0.019917485912649913\n",
      "Epoch 101 of 500, Train Loss: 0.019888\n",
      "best loss:  0.01988806083839921\n",
      "Epoch 102 of 500, Train Loss: 0.019860\n",
      "best loss:  0.019859599132522176\n",
      "Epoch 103 of 500, Train Loss: 0.019832\n",
      "best loss:  0.01983195727162222\n",
      "Epoch 104 of 500, Train Loss: 0.019806\n",
      "best loss:  0.019805518721001107\n",
      "Epoch 105 of 500, Train Loss: 0.019781\n",
      "best loss:  0.019780707302595336\n",
      "Epoch 106 of 500, Train Loss: 0.019758\n",
      "best loss:  0.019757691807792226\n",
      "Epoch 107 of 500, Train Loss: 0.019736\n",
      "best loss:  0.01973602483062572\n",
      "Epoch 108 of 500, Train Loss: 0.019716\n",
      "best loss:  0.019715637180461484\n",
      "Epoch 109 of 500, Train Loss: 0.019697\n",
      "best loss:  0.019697280756338652\n",
      "Epoch 110 of 500, Train Loss: 0.019681\n",
      "best loss:  0.0196807102441008\n",
      "Epoch 111 of 500, Train Loss: 0.019665\n",
      "best loss:  0.019665335634652916\n",
      "Epoch 112 of 500, Train Loss: 0.019652\n",
      "best loss:  0.019652260010322022\n",
      "Epoch 113 of 500, Train Loss: 0.019639\n",
      "best loss:  0.01963939983713578\n",
      "Epoch 114 of 500, Train Loss: 0.019628\n",
      "best loss:  0.019627906995520444\n",
      "Epoch 115 of 500, Train Loss: 0.019619\n",
      "best loss:  0.019618553366889283\n",
      "Epoch 116 of 500, Train Loss: 0.019609\n",
      "best loss:  0.019609205545487545\n",
      "Epoch 117 of 500, Train Loss: 0.019600\n",
      "best loss:  0.019599570763927286\n",
      "Epoch 118 of 500, Train Loss: 0.019591\n",
      "best loss:  0.019591489015778802\n",
      "Epoch 119 of 500, Train Loss: 0.019584\n",
      "best loss:  0.019583711532159355\n",
      "Epoch 120 of 500, Train Loss: 0.019576\n",
      "best loss:  0.019576200653389168\n",
      "Epoch 121 of 500, Train Loss: 0.019569\n",
      "best loss:  0.01956876623760115\n",
      "Epoch 122 of 500, Train Loss: 0.019562\n",
      "best loss:  0.019562401189270263\n",
      "Epoch 123 of 500, Train Loss: 0.019557\n",
      "best loss:  0.01955689246113802\n",
      "Epoch 124 of 500, Train Loss: 0.019551\n",
      "best loss:  0.01955112551953535\n",
      "Epoch 125 of 500, Train Loss: 0.019546\n",
      "best loss:  0.019546185725940707\n",
      "Epoch 126 of 500, Train Loss: 0.019542\n",
      "best loss:  0.0195415767582942\n",
      "Epoch 127 of 500, Train Loss: 0.019536\n",
      "best loss:  0.019536436566101142\n",
      "Epoch 128 of 500, Train Loss: 0.019531\n",
      "best loss:  0.019530741676107022\n",
      "Epoch 129 of 500, Train Loss: 0.019525\n",
      "best loss:  0.019525263915804048\n",
      "Epoch 130 of 500, Train Loss: 0.019521\n",
      "best loss:  0.01952056628615819\n",
      "Epoch 131 of 500, Train Loss: 0.019515\n",
      "best loss:  0.01951511656277084\n",
      "Epoch 132 of 500, Train Loss: 0.019509\n",
      "best loss:  0.01950904593194741\n",
      "Epoch 133 of 500, Train Loss: 0.019503\n",
      "best loss:  0.01950313585003578\n",
      "Epoch 134 of 500, Train Loss: 0.019497\n",
      "best loss:  0.01949719697473455\n",
      "Epoch 135 of 500, Train Loss: 0.019490\n",
      "best loss:  0.01949036176237025\n",
      "Epoch 136 of 500, Train Loss: 0.019484\n",
      "best loss:  0.019483528507858977\n",
      "Epoch 137 of 500, Train Loss: 0.019476\n",
      "best loss:  0.019476101037282714\n",
      "Epoch 138 of 500, Train Loss: 0.019469\n",
      "best loss:  0.019468554306880013\n",
      "Epoch 139 of 500, Train Loss: 0.019461\n",
      "best loss:  0.019461385348483257\n",
      "Epoch 140 of 500, Train Loss: 0.019455\n",
      "best loss:  0.019454572616459064\n",
      "Epoch 141 of 500, Train Loss: 0.019447\n",
      "best loss:  0.019447181673446137\n",
      "Epoch 142 of 500, Train Loss: 0.019440\n",
      "best loss:  0.01944019225770752\n",
      "Epoch 143 of 500, Train Loss: 0.019433\n",
      "best loss:  0.019432915682144565\n",
      "Epoch 144 of 500, Train Loss: 0.019426\n",
      "best loss:  0.019425546191587626\n",
      "Epoch 145 of 500, Train Loss: 0.019418\n",
      "best loss:  0.01941831114922123\n",
      "Epoch 146 of 500, Train Loss: 0.019412\n",
      "best loss:  0.019411550387839403\n",
      "Epoch 147 of 500, Train Loss: 0.019405\n",
      "best loss:  0.01940489577827975\n",
      "Epoch 148 of 500, Train Loss: 0.019399\n",
      "best loss:  0.019398541091589342\n",
      "Epoch 149 of 500, Train Loss: 0.019392\n",
      "best loss:  0.019392243152179273\n",
      "Epoch 150 of 500, Train Loss: 0.019386\n",
      "best loss:  0.0193864542268065\n",
      "Epoch 151 of 500, Train Loss: 0.019381\n",
      "best loss:  0.019380696257305614\n",
      "Epoch 152 of 500, Train Loss: 0.019376\n",
      "best loss:  0.01937572452868696\n",
      "Epoch 153 of 500, Train Loss: 0.019371\n",
      "best loss:  0.019370915087313544\n",
      "Epoch 154 of 500, Train Loss: 0.019366\n",
      "best loss:  0.019366167329816695\n",
      "Epoch 155 of 500, Train Loss: 0.019362\n",
      "best loss:  0.019362038826703478\n",
      "Epoch 156 of 500, Train Loss: 0.019358\n",
      "best loss:  0.019358084351410947\n",
      "Epoch 157 of 500, Train Loss: 0.019354\n",
      "best loss:  0.019354207410302027\n",
      "Epoch 158 of 500, Train Loss: 0.019351\n",
      "best loss:  0.019350777863653003\n",
      "Epoch 159 of 500, Train Loss: 0.019347\n",
      "best loss:  0.019347221242009424\n",
      "Epoch 160 of 500, Train Loss: 0.019344\n",
      "best loss:  0.01934406732893982\n",
      "Epoch 161 of 500, Train Loss: 0.019341\n",
      "best loss:  0.019341014893707466\n",
      "Epoch 162 of 500, Train Loss: 0.019338\n",
      "best loss:  0.01933840136161045\n",
      "Epoch 163 of 500, Train Loss: 0.019335\n",
      "best loss:  0.019335454080322424\n",
      "Epoch 164 of 500, Train Loss: 0.019333\n",
      "best loss:  0.01933312920385127\n",
      "Epoch 165 of 500, Train Loss: 0.019331\n",
      "best loss:  0.019330812323939545\n",
      "Epoch 166 of 500, Train Loss: 0.019329\n",
      "best loss:  0.01932860784843571\n",
      "Epoch 167 of 500, Train Loss: 0.019326\n",
      "best loss:  0.019326074839643628\n",
      "Epoch 168 of 500, Train Loss: 0.019325\n",
      "best loss:  0.019324808258765428\n",
      "Epoch 169 of 500, Train Loss: 0.019323\n",
      "best loss:  0.019322785791397717\n",
      "Epoch 170 of 500, Train Loss: 0.019321\n",
      "best loss:  0.019321099323014793\n",
      "Epoch 171 of 500, Train Loss: 0.019319\n",
      "best loss:  0.019319405692382247\n",
      "Epoch 172 of 500, Train Loss: 0.019318\n",
      "best loss:  0.019317930074933872\n",
      "Epoch 173 of 500, Train Loss: 0.019317\n",
      "best loss:  0.01931666986734332\n",
      "Epoch 174 of 500, Train Loss: 0.019315\n",
      "best loss:  0.0193154242910831\n",
      "Epoch 175 of 500, Train Loss: 0.019314\n",
      "best loss:  0.019313782109254465\n",
      "Epoch 176 of 500, Train Loss: 0.019313\n",
      "best loss:  0.019312861821092594\n",
      "Epoch 177 of 500, Train Loss: 0.019311\n",
      "best loss:  0.019311463148080448\n",
      "Epoch 178 of 500, Train Loss: 0.019311\n",
      "best loss:  0.019310627556803038\n",
      "Epoch 179 of 500, Train Loss: 0.019310\n",
      "best loss:  0.01930950272782316\n",
      "Epoch 180 of 500, Train Loss: 0.019308\n",
      "best loss:  0.019307771957770004\n",
      "Epoch 181 of 500, Train Loss: 0.019307\n",
      "best loss:  0.019306820624168646\n",
      "Epoch 182 of 500, Train Loss: 0.019306\n",
      "best loss:  0.019305865736741413\n",
      "Epoch 183 of 500, Train Loss: 0.019305\n",
      "best loss:  0.019304718988967424\n",
      "Epoch 184 of 500, Train Loss: 0.019304\n",
      "best loss:  0.019303849796528112\n",
      "Epoch 185 of 500, Train Loss: 0.019304\n",
      "best loss:  0.019303708098099618\n",
      "Epoch 186 of 500, Train Loss: 0.019303\n",
      "best loss:  0.019303388300698335\n",
      "Epoch 187 of 500, Train Loss: 0.019302\n",
      "best loss:  0.019302279612838195\n",
      "Epoch 188 of 500, Train Loss: 0.019302\n",
      "best loss:  0.019301559006057088\n",
      "Epoch 189 of 500, Train Loss: 0.019301\n",
      "best loss:  0.019300804542947934\n",
      "Epoch 190 of 500, Train Loss: 0.019301\n",
      "Epoch 191 of 500, Train Loss: 0.019300\n",
      "best loss:  0.019300154050678562\n",
      "Epoch 192 of 500, Train Loss: 0.019300\n",
      "best loss:  0.01929952294088344\n",
      "Epoch 193 of 500, Train Loss: 0.019299\n",
      "best loss:  0.01929918408814399\n",
      "Epoch 194 of 500, Train Loss: 0.019299\n",
      "best loss:  0.019298682087452997\n",
      "Epoch 195 of 500, Train Loss: 0.019298\n",
      "best loss:  0.019297778189485502\n",
      "Epoch 196 of 500, Train Loss: 0.019297\n",
      "best loss:  0.019296964192122235\n",
      "Epoch 197 of 500, Train Loss: 0.019296\n",
      "best loss:  0.01929616096994457\n",
      "Epoch 198 of 500, Train Loss: 0.019296\n",
      "best loss:  0.019295733584932147\n",
      "Epoch 199 of 500, Train Loss: 0.019295\n",
      "best loss:  0.01929490561998637\n",
      "Epoch 200 of 500, Train Loss: 0.019294\n",
      "best loss:  0.01929449410355549\n",
      "Epoch 201 of 500, Train Loss: 0.019294\n",
      "best loss:  0.019293668385696155\n",
      "Epoch 202 of 500, Train Loss: 0.019293\n",
      "best loss:  0.019293118590875277\n",
      "Epoch 203 of 500, Train Loss: 0.019293\n",
      "best loss:  0.019292560118826307\n",
      "Epoch 204 of 500, Train Loss: 0.019292\n",
      "best loss:  0.019291812729527236\n",
      "Epoch 205 of 500, Train Loss: 0.019291\n",
      "best loss:  0.019290861295980163\n",
      "Epoch 206 of 500, Train Loss: 0.019291\n",
      "best loss:  0.019290788475037804\n",
      "Epoch 207 of 500, Train Loss: 0.019290\n",
      "best loss:  0.019290379818001864\n",
      "Epoch 208 of 500, Train Loss: 0.019290\n",
      "best loss:  0.01928989419893203\n",
      "Epoch 209 of 500, Train Loss: 0.019289\n",
      "best loss:  0.0192894215565608\n",
      "Epoch 210 of 500, Train Loss: 0.019289\n",
      "best loss:  0.019289323701812026\n",
      "Epoch 211 of 500, Train Loss: 0.019289\n",
      "best loss:  0.01928888700379947\n",
      "Epoch 212 of 500, Train Loss: 0.019289\n",
      "best loss:  0.01928888623559371\n",
      "Epoch 213 of 500, Train Loss: 0.019289\n",
      "best loss:  0.019288828321227212\n",
      "Epoch 214 of 500, Train Loss: 0.019289\n",
      "Epoch 215 of 500, Train Loss: 0.019289\n",
      "Epoch 216 of 500, Train Loss: 0.019290\n",
      "Epoch 217 of 500, Train Loss: 0.019290\n",
      "Epoch 218 of 500, Train Loss: 0.019291\n",
      "Epoch 219 of 500, Train Loss: 0.019291\n",
      "Epoch 220 of 500, Train Loss: 0.019292\n",
      "Epoch 221 of 500, Train Loss: 0.019292\n",
      "Epoch 222 of 500, Train Loss: 0.019293\n",
      "Epoch 223 of 500, Train Loss: 0.019293\n",
      "Epoch 224 of 500, Train Loss: 0.019293\n",
      "Epoch 225 of 500, Train Loss: 0.019293\n",
      "Epoch 226 of 500, Train Loss: 0.019294\n",
      "Epoch 227 of 500, Train Loss: 0.019294\n",
      "Epoch 228 of 500, Train Loss: 0.019294\n",
      "Epoch 229 of 500, Train Loss: 0.019294\n",
      "Epoch 230 of 500, Train Loss: 0.019295\n",
      "Epoch 231 of 500, Train Loss: 0.019295\n",
      "Epoch 232 of 500, Train Loss: 0.019294\n",
      "Epoch 233 of 500, Train Loss: 0.019295\n",
      "Epoch 234 of 500, Train Loss: 0.019295\n",
      "Epoch 235 of 500, Train Loss: 0.019295\n",
      "Epoch 236 of 500, Train Loss: 0.019294\n",
      "Epoch 237 of 500, Train Loss: 0.019295\n",
      "Epoch 238 of 500, Train Loss: 0.019294\n",
      "Epoch 239 of 500, Train Loss: 0.019294\n",
      "Epoch 240 of 500, Train Loss: 0.019293\n",
      "Epoch 241 of 500, Train Loss: 0.019293\n",
      "Epoch 242 of 500, Train Loss: 0.019292\n",
      "Epoch 243 of 500, Train Loss: 0.019293\n",
      "Epoch 244 of 500, Train Loss: 0.019292\n",
      "Epoch 245 of 500, Train Loss: 0.019292\n",
      "Epoch 246 of 500, Train Loss: 0.019291\n",
      "Epoch 247 of 500, Train Loss: 0.019291\n",
      "Epoch 248 of 500, Train Loss: 0.019290\n",
      "Epoch 249 of 500, Train Loss: 0.019290\n",
      "Epoch 250 of 500, Train Loss: 0.019288\n",
      "best loss:  0.019288498971903652\n",
      "Epoch 251 of 500, Train Loss: 0.019288\n",
      "best loss:  0.0192883852090914\n",
      "Epoch 252 of 500, Train Loss: 0.019288\n",
      "best loss:  0.0192878889804345\n",
      "Epoch 253 of 500, Train Loss: 0.019287\n",
      "best loss:  0.01928739133403752\n",
      "Epoch 254 of 500, Train Loss: 0.019287\n",
      "best loss:  0.01928702626167977\n",
      "Epoch 255 of 500, Train Loss: 0.019286\n",
      "best loss:  0.01928643887666733\n",
      "Epoch 256 of 500, Train Loss: 0.019286\n",
      "best loss:  0.019286412011124483\n",
      "Epoch 257 of 500, Train Loss: 0.019286\n",
      "best loss:  0.019286333297671706\n",
      "Epoch 258 of 500, Train Loss: 0.019286\n",
      "best loss:  0.01928577585350686\n",
      "Epoch 259 of 500, Train Loss: 0.019285\n",
      "best loss:  0.01928486459407515\n",
      "Epoch 260 of 500, Train Loss: 0.019284\n",
      "best loss:  0.019284074204204995\n",
      "Epoch 261 of 500, Train Loss: 0.019283\n",
      "best loss:  0.019283478896859167\n",
      "Epoch 262 of 500, Train Loss: 0.019283\n",
      "best loss:  0.019283474567382372\n",
      "Epoch 263 of 500, Train Loss: 0.019283\n",
      "best loss:  0.019282883267115095\n",
      "Epoch 264 of 500, Train Loss: 0.019283\n",
      "best loss:  0.01928262213626768\n",
      "Epoch 265 of 500, Train Loss: 0.019282\n",
      "best loss:  0.019282336782008885\n",
      "Epoch 266 of 500, Train Loss: 0.019281\n",
      "best loss:  0.019281262315741142\n",
      "Epoch 267 of 500, Train Loss: 0.019281\n",
      "best loss:  0.019280810496370964\n",
      "Epoch 268 of 500, Train Loss: 0.019280\n",
      "best loss:  0.019280378415038116\n",
      "Epoch 269 of 500, Train Loss: 0.019280\n",
      "best loss:  0.019280252997382622\n",
      "Epoch 270 of 500, Train Loss: 0.019279\n",
      "best loss:  0.01927937989600559\n",
      "Epoch 271 of 500, Train Loss: 0.019279\n",
      "best loss:  0.019279223412154385\n",
      "Epoch 272 of 500, Train Loss: 0.019279\n",
      "best loss:  0.01927902526492903\n",
      "Epoch 273 of 500, Train Loss: 0.019279\n",
      "best loss:  0.019278852815257905\n",
      "Epoch 274 of 500, Train Loss: 0.019279\n",
      "Epoch 275 of 500, Train Loss: 0.019279\n",
      "best loss:  0.019278741137543333\n",
      "Epoch 276 of 500, Train Loss: 0.019278\n",
      "best loss:  0.01927828168913419\n",
      "Epoch 277 of 500, Train Loss: 0.019279\n",
      "Epoch 278 of 500, Train Loss: 0.019278\n",
      "best loss:  0.0192780623450702\n",
      "Epoch 279 of 500, Train Loss: 0.019278\n",
      "Epoch 280 of 500, Train Loss: 0.019278\n",
      "best loss:  0.019277590467971787\n",
      "Epoch 281 of 500, Train Loss: 0.019278\n",
      "Epoch 282 of 500, Train Loss: 0.019277\n",
      "best loss:  0.019277243882637875\n",
      "Epoch 283 of 500, Train Loss: 0.019277\n",
      "best loss:  0.019277025975265302\n",
      "Epoch 284 of 500, Train Loss: 0.019277\n",
      "Epoch 285 of 500, Train Loss: 0.019277\n",
      "Epoch 286 of 500, Train Loss: 0.019277\n",
      "best loss:  0.019277001591444434\n",
      "Epoch 287 of 500, Train Loss: 0.019276\n",
      "best loss:  0.019276388387724597\n",
      "Epoch 288 of 500, Train Loss: 0.019277\n",
      "Epoch 289 of 500, Train Loss: 0.019276\n",
      "best loss:  0.019275893586283448\n",
      "Epoch 290 of 500, Train Loss: 0.019276\n",
      "best loss:  0.019275856546582943\n",
      "Epoch 291 of 500, Train Loss: 0.019276\n",
      "best loss:  0.01927552772067711\n",
      "Epoch 292 of 500, Train Loss: 0.019275\n",
      "best loss:  0.019275470680779193\n",
      "Epoch 293 of 500, Train Loss: 0.019276\n",
      "Epoch 294 of 500, Train Loss: 0.019275\n",
      "best loss:  0.01927509331216588\n",
      "Epoch 295 of 500, Train Loss: 0.019275\n",
      "Epoch 296 of 500, Train Loss: 0.019275\n",
      "best loss:  0.01927456927378373\n",
      "Epoch 297 of 500, Train Loss: 0.019275\n",
      "Epoch 298 of 500, Train Loss: 0.019274\n",
      "best loss:  0.01927427809199051\n",
      "Epoch 299 of 500, Train Loss: 0.019274\n",
      "best loss:  0.019274255815828437\n",
      "Epoch 300 of 500, Train Loss: 0.019273\n",
      "best loss:  0.019273406107388365\n",
      "Epoch 301 of 500, Train Loss: 0.019273\n",
      "best loss:  0.019273071178025746\n",
      "Epoch 302 of 500, Train Loss: 0.019272\n",
      "best loss:  0.019272380862917476\n",
      "Epoch 303 of 500, Train Loss: 0.019272\n",
      "best loss:  0.01927154418848123\n",
      "Epoch 304 of 500, Train Loss: 0.019272\n",
      "Epoch 305 of 500, Train Loss: 0.019271\n",
      "best loss:  0.01927078022579157\n",
      "Epoch 306 of 500, Train Loss: 0.019271\n",
      "Epoch 307 of 500, Train Loss: 0.019271\n",
      "Epoch 308 of 500, Train Loss: 0.019270\n",
      "best loss:  0.019270324459355396\n",
      "Epoch 309 of 500, Train Loss: 0.019270\n",
      "best loss:  0.019270050869135604\n",
      "Epoch 310 of 500, Train Loss: 0.019269\n",
      "best loss:  0.019269360635247326\n",
      "Epoch 311 of 500, Train Loss: 0.019270\n",
      "Epoch 312 of 500, Train Loss: 0.019268\n",
      "best loss:  0.019268452191442006\n",
      "Epoch 313 of 500, Train Loss: 0.019269\n",
      "Epoch 314 of 500, Train Loss: 0.019268\n",
      "best loss:  0.019268335764839623\n",
      "Epoch 315 of 500, Train Loss: 0.019268\n",
      "Epoch 316 of 500, Train Loss: 0.019268\n",
      "best loss:  0.01926794874210221\n",
      "Epoch 317 of 500, Train Loss: 0.019268\n",
      "best loss:  0.01926782636433053\n",
      "Epoch 318 of 500, Train Loss: 0.019267\n",
      "best loss:  0.019266558034063946\n",
      "Epoch 319 of 500, Train Loss: 0.019267\n",
      "Epoch 320 of 500, Train Loss: 0.019267\n",
      "Epoch 321 of 500, Train Loss: 0.019266\n",
      "best loss:  0.01926642791038675\n",
      "Epoch 322 of 500, Train Loss: 0.019268\n",
      "Epoch 323 of 500, Train Loss: 0.019266\n",
      "best loss:  0.019266118998269282\n",
      "Epoch 324 of 500, Train Loss: 0.019268\n",
      "Epoch 325 of 500, Train Loss: 0.019265\n",
      "best loss:  0.019265453047579328\n",
      "Epoch 326 of 500, Train Loss: 0.019267\n",
      "Epoch 327 of 500, Train Loss: 0.019265\n",
      "best loss:  0.019264615649157095\n",
      "Epoch 328 of 500, Train Loss: 0.019266\n",
      "Epoch 329 of 500, Train Loss: 0.019264\n",
      "best loss:  0.01926404073547442\n",
      "Epoch 330 of 500, Train Loss: 0.019264\n",
      "Epoch 331 of 500, Train Loss: 0.019263\n",
      "best loss:  0.01926346071531036\n",
      "Epoch 332 of 500, Train Loss: 0.019263\n",
      "best loss:  0.01926345836579648\n",
      "Epoch 333 of 500, Train Loss: 0.019265\n",
      "Epoch 334 of 500, Train Loss: 0.019263\n",
      "best loss:  0.019263392886690173\n",
      "Epoch 335 of 500, Train Loss: 0.019266\n",
      "Epoch 336 of 500, Train Loss: 0.019263\n",
      "Epoch 337 of 500, Train Loss: 0.019266\n",
      "Epoch 338 of 500, Train Loss: 0.019262\n",
      "best loss:  0.019261947959569436\n",
      "Epoch 339 of 500, Train Loss: 0.019265\n",
      "Epoch 340 of 500, Train Loss: 0.019261\n",
      "best loss:  0.019261068431886144\n",
      "Epoch 341 of 500, Train Loss: 0.019265\n",
      "Epoch 342 of 500, Train Loss: 0.019262\n",
      "Epoch 343 of 500, Train Loss: 0.019265\n",
      "Epoch 344 of 500, Train Loss: 0.019262\n",
      "Epoch 345 of 500, Train Loss: 0.019263\n",
      "Epoch 346 of 500, Train Loss: 0.019262\n",
      "Epoch 347 of 500, Train Loss: 0.019262\n",
      "Epoch 348 of 500, Train Loss: 0.019263\n",
      "Epoch 349 of 500, Train Loss: 0.019261\n",
      "best loss:  0.019260572734092138\n",
      "Epoch 350 of 500, Train Loss: 0.019265\n",
      "Epoch 351 of 500, Train Loss: 0.019260\n",
      "best loss:  0.01926007842380633\n",
      "Epoch 352 of 500, Train Loss: 0.019266\n",
      "Epoch 353 of 500, Train Loss: 0.019259\n",
      "best loss:  0.019259003068856504\n",
      "Epoch 354 of 500, Train Loss: 0.019265\n",
      "Epoch 355 of 500, Train Loss: 0.019258\n",
      "best loss:  0.019258048834347764\n",
      "Epoch 356 of 500, Train Loss: 0.019264\n",
      "Epoch 357 of 500, Train Loss: 0.019259\n",
      "Epoch 358 of 500, Train Loss: 0.019264\n",
      "Epoch 359 of 500, Train Loss: 0.019261\n",
      "Epoch 360 of 500, Train Loss: 0.019262\n",
      "Epoch 361 of 500, Train Loss: 0.019261\n",
      "Epoch 362 of 500, Train Loss: 0.019260\n",
      "Epoch 363 of 500, Train Loss: 0.019263\n",
      "Epoch 364 of 500, Train Loss: 0.019259\n",
      "Epoch 365 of 500, Train Loss: 0.019264\n",
      "Epoch 366 of 500, Train Loss: 0.019257\n",
      "best loss:  0.019257321059902365\n",
      "Epoch 367 of 500, Train Loss: 0.019265\n",
      "Epoch 368 of 500, Train Loss: 0.019257\n",
      "best loss:  0.019257042286267807\n",
      "Epoch 369 of 500, Train Loss: 0.019265\n",
      "Epoch 370 of 500, Train Loss: 0.019258\n",
      "Epoch 371 of 500, Train Loss: 0.019265\n",
      "Epoch 372 of 500, Train Loss: 0.019259\n",
      "Epoch 373 of 500, Train Loss: 0.019264\n",
      "Epoch 374 of 500, Train Loss: 0.019260\n",
      "Epoch 375 of 500, Train Loss: 0.019263\n",
      "Epoch 376 of 500, Train Loss: 0.019262\n",
      "Epoch 377 of 500, Train Loss: 0.019261\n",
      "Epoch 378 of 500, Train Loss: 0.019263\n",
      "Epoch 379 of 500, Train Loss: 0.019261\n",
      "Epoch 380 of 500, Train Loss: 0.019264\n",
      "Epoch 381 of 500, Train Loss: 0.019260\n",
      "Epoch 382 of 500, Train Loss: 0.019264\n",
      "Epoch 383 of 500, Train Loss: 0.019260\n",
      "Epoch 384 of 500, Train Loss: 0.019263\n",
      "Epoch 385 of 500, Train Loss: 0.019260\n",
      "Epoch 386 of 500, Train Loss: 0.019262\n",
      "Epoch 387 of 500, Train Loss: 0.019260\n",
      "Epoch 388 of 500, Train Loss: 0.019261\n",
      "Epoch 389 of 500, Train Loss: 0.019262\n",
      "Epoch 390 of 500, Train Loss: 0.019261\n",
      "Epoch 391 of 500, Train Loss: 0.019263\n",
      "Epoch 392 of 500, Train Loss: 0.019261\n",
      "Epoch 393 of 500, Train Loss: 0.019264\n",
      "Epoch 394 of 500, Train Loss: 0.019260\n",
      "Epoch 395 of 500, Train Loss: 0.019263\n",
      "Epoch 396 of 500, Train Loss: 0.019260\n",
      "Epoch 397 of 500, Train Loss: 0.019263\n",
      "Epoch 398 of 500, Train Loss: 0.019258\n",
      "Epoch 399 of 500, Train Loss: 0.019262\n",
      "Epoch 400 of 500, Train Loss: 0.019259\n",
      "Epoch 401 of 500, Train Loss: 0.019261\n",
      "Epoch 402 of 500, Train Loss: 0.019260\n",
      "Epoch 403 of 500, Train Loss: 0.019262\n",
      "Epoch 404 of 500, Train Loss: 0.019263\n",
      "Epoch 405 of 500, Train Loss: 0.019263\n",
      "Epoch 406 of 500, Train Loss: 0.019265\n",
      "Epoch 407 of 500, Train Loss: 0.019262\n",
      "Epoch 408 of 500, Train Loss: 0.019266\n",
      "Epoch 409 of 500, Train Loss: 0.019262\n",
      "Epoch 410 of 500, Train Loss: 0.019265\n",
      "Epoch 411 of 500, Train Loss: 0.019261\n",
      "Epoch 412 of 500, Train Loss: 0.019264\n",
      "Epoch 413 of 500, Train Loss: 0.019261\n",
      "Epoch 414 of 500, Train Loss: 0.019263\n",
      "Epoch 415 of 500, Train Loss: 0.019261\n",
      "Epoch 416 of 500, Train Loss: 0.019263\n",
      "Epoch 417 of 500, Train Loss: 0.019263\n",
      "Epoch 418 of 500, Train Loss: 0.019263\n",
      "Epoch 419 of 500, Train Loss: 0.019265\n",
      "Epoch 420 of 500, Train Loss: 0.019264\n",
      "Epoch 421 of 500, Train Loss: 0.019265\n",
      "Epoch 422 of 500, Train Loss: 0.019264\n",
      "Epoch 423 of 500, Train Loss: 0.019265\n",
      "Epoch 424 of 500, Train Loss: 0.019265\n",
      "Epoch 425 of 500, Train Loss: 0.019263\n",
      "Epoch 426 of 500, Train Loss: 0.019266\n",
      "Epoch 427 of 500, Train Loss: 0.019263\n",
      "Epoch 428 of 500, Train Loss: 0.019266\n",
      "Epoch 429 of 500, Train Loss: 0.019263\n",
      "Epoch 430 of 500, Train Loss: 0.019268\n",
      "Epoch 431 of 500, Train Loss: 0.019264\n",
      "Epoch 432 of 500, Train Loss: 0.019269\n",
      "Epoch 433 of 500, Train Loss: 0.019263\n",
      "Epoch 434 of 500, Train Loss: 0.019269\n",
      "Epoch 435 of 500, Train Loss: 0.019263\n",
      "Epoch 436 of 500, Train Loss: 0.019268\n",
      "Epoch 437 of 500, Train Loss: 0.019263\n",
      "Epoch 438 of 500, Train Loss: 0.019267\n",
      "Epoch 439 of 500, Train Loss: 0.019264\n",
      "Epoch 440 of 500, Train Loss: 0.019267\n",
      "Epoch 441 of 500, Train Loss: 0.019266\n",
      "Epoch 442 of 500, Train Loss: 0.019268\n",
      "Epoch 443 of 500, Train Loss: 0.019268\n",
      "Epoch 444 of 500, Train Loss: 0.019267\n",
      "Epoch 445 of 500, Train Loss: 0.019268\n",
      "Epoch 446 of 500, Train Loss: 0.019267\n",
      "Epoch 447 of 500, Train Loss: 0.019269\n",
      "Epoch 448 of 500, Train Loss: 0.019265\n",
      "Epoch 449 of 500, Train Loss: 0.019269\n",
      "Epoch 450 of 500, Train Loss: 0.019264\n",
      "Epoch 451 of 500, Train Loss: 0.019270\n",
      "Epoch 452 of 500, Train Loss: 0.019264\n",
      "Epoch 453 of 500, Train Loss: 0.019271\n",
      "Epoch 454 of 500, Train Loss: 0.019264\n",
      "Epoch 455 of 500, Train Loss: 0.019272\n",
      "Epoch 456 of 500, Train Loss: 0.019265\n",
      "Epoch 457 of 500, Train Loss: 0.019273\n",
      "Epoch 458 of 500, Train Loss: 0.019267\n",
      "Epoch 459 of 500, Train Loss: 0.019272\n",
      "Epoch 460 of 500, Train Loss: 0.019267\n",
      "Epoch 461 of 500, Train Loss: 0.019270\n",
      "Epoch 462 of 500, Train Loss: 0.019268\n",
      "Epoch 463 of 500, Train Loss: 0.019268\n",
      "Epoch 464 of 500, Train Loss: 0.019270\n",
      "Epoch 465 of 500, Train Loss: 0.019267\n",
      "Epoch 466 of 500, Train Loss: 0.019272\n",
      "Epoch 467 of 500, Train Loss: 0.019267\n",
      "Epoch 468 of 500, Train Loss: 0.019275\n",
      "Epoch 469 of 500, Train Loss: 0.019267\n",
      "Epoch 470 of 500, Train Loss: 0.019276\n",
      "Epoch 471 of 500, Train Loss: 0.019268\n",
      "Epoch 472 of 500, Train Loss: 0.019276\n",
      "Epoch 473 of 500, Train Loss: 0.019269\n",
      "Epoch 474 of 500, Train Loss: 0.019275\n",
      "Epoch 475 of 500, Train Loss: 0.019272\n",
      "Epoch 476 of 500, Train Loss: 0.019275\n",
      "Epoch 477 of 500, Train Loss: 0.019276\n",
      "Epoch 478 of 500, Train Loss: 0.019276\n",
      "Epoch 479 of 500, Train Loss: 0.019278\n",
      "Epoch 480 of 500, Train Loss: 0.019275\n",
      "Epoch 481 of 500, Train Loss: 0.019279\n",
      "Epoch 482 of 500, Train Loss: 0.019274\n",
      "Epoch 483 of 500, Train Loss: 0.019279\n",
      "Epoch 484 of 500, Train Loss: 0.019275\n",
      "Epoch 485 of 500, Train Loss: 0.019277\n",
      "Epoch 486 of 500, Train Loss: 0.019275\n",
      "Epoch 487 of 500, Train Loss: 0.019277\n",
      "Epoch 488 of 500, Train Loss: 0.019275\n",
      "Epoch 489 of 500, Train Loss: 0.019278\n",
      "Epoch 490 of 500, Train Loss: 0.019277\n",
      "Epoch 491 of 500, Train Loss: 0.019280\n",
      "Epoch 492 of 500, Train Loss: 0.019280\n",
      "Epoch 493 of 500, Train Loss: 0.019282\n",
      "Epoch 494 of 500, Train Loss: 0.019282\n",
      "Epoch 495 of 500, Train Loss: 0.019282\n",
      "Epoch 496 of 500, Train Loss: 0.019281\n",
      "Epoch 497 of 500, Train Loss: 0.019281\n",
      "Epoch 498 of 500, Train Loss: 0.019279\n",
      "Epoch 499 of 500, Train Loss: 0.019280\n",
      "Epoch 500 of 500, Train Loss: 0.019276\n",
      "latent train shape:  (16395, 10)\n",
      "M: 10, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 67\n",
      "Training the subspace: 0 / 10\n",
      "Training the subspace: 1 / 10\n",
      "Training the subspace: 2 / 10\n",
      "Training the subspace: 3 / 10\n",
      "Training the subspace: 4 / 10\n",
      "Training the subspace: 5 / 10\n",
      "Training the subspace: 6 / 10\n",
      "Training the subspace: 7 / 10\n",
      "Training the subspace: 8 / 10\n",
      "Training the subspace: 9 / 10\n",
      "Encoding the subspace: 0 / 10\n",
      "Encoding the subspace: 1 / 10\n",
      "Encoding the subspace: 2 / 10\n",
      "Encoding the subspace: 3 / 10\n",
      "Encoding the subspace: 4 / 10\n",
      "Encoding the subspace: 5 / 10\n",
      "Encoding the subspace: 6 / 10\n",
      "Encoding the subspace: 7 / 10\n",
      "Encoding the subspace: 8 / 10\n",
      "Encoding the subspace: 9 / 10\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=20, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.605888\n",
      "best loss:  0.6058879906470461\n",
      "Epoch 2 of 500, Train Loss: 0.078215\n",
      "best loss:  0.07821459736487316\n",
      "Epoch 3 of 500, Train Loss: 0.056112\n",
      "best loss:  0.05611203116086824\n",
      "Epoch 4 of 500, Train Loss: 0.055334\n",
      "best loss:  0.05533408131558708\n",
      "Epoch 5 of 500, Train Loss: 0.054095\n",
      "best loss:  0.05409511380698965\n",
      "Epoch 6 of 500, Train Loss: 0.052161\n",
      "best loss:  0.0521614656295418\n",
      "Epoch 7 of 500, Train Loss: 0.049414\n",
      "best loss:  0.04941434295245179\n",
      "Epoch 8 of 500, Train Loss: 0.045990\n",
      "best loss:  0.045990101119066595\n",
      "Epoch 9 of 500, Train Loss: 0.042342\n",
      "best loss:  0.04234212517870099\n",
      "Epoch 10 of 500, Train Loss: 0.038988\n",
      "best loss:  0.038987744087894814\n",
      "Epoch 11 of 500, Train Loss: 0.036176\n",
      "best loss:  0.03617609372496056\n",
      "Epoch 12 of 500, Train Loss: 0.033882\n",
      "best loss:  0.0338819835419846\n",
      "Epoch 13 of 500, Train Loss: 0.031994\n",
      "best loss:  0.031994017978585074\n",
      "Epoch 14 of 500, Train Loss: 0.030427\n",
      "best loss:  0.030427432022068303\n",
      "Epoch 15 of 500, Train Loss: 0.029127\n",
      "best loss:  0.02912726839679445\n",
      "Epoch 16 of 500, Train Loss: 0.028047\n",
      "best loss:  0.028047024168601158\n",
      "Epoch 17 of 500, Train Loss: 0.027138\n",
      "best loss:  0.027137632429863275\n",
      "Epoch 18 of 500, Train Loss: 0.026352\n",
      "best loss:  0.026351692544408285\n",
      "Epoch 19 of 500, Train Loss: 0.025652\n",
      "best loss:  0.02565176668531774\n",
      "Epoch 20 of 500, Train Loss: 0.025010\n",
      "best loss:  0.025010426524690852\n",
      "Epoch 21 of 500, Train Loss: 0.024414\n",
      "best loss:  0.024413985158713868\n",
      "Epoch 22 of 500, Train Loss: 0.023855\n",
      "best loss:  0.023854628683594885\n",
      "Epoch 23 of 500, Train Loss: 0.023328\n",
      "best loss:  0.02332803854121386\n",
      "Epoch 24 of 500, Train Loss: 0.022835\n",
      "best loss:  0.022834849842578577\n",
      "Epoch 25 of 500, Train Loss: 0.022376\n",
      "best loss:  0.02237627245557978\n",
      "Epoch 26 of 500, Train Loss: 0.021954\n",
      "best loss:  0.021953821880291685\n",
      "Epoch 27 of 500, Train Loss: 0.021567\n",
      "best loss:  0.021566903782654696\n",
      "Epoch 28 of 500, Train Loss: 0.021217\n",
      "best loss:  0.021216833834489447\n",
      "Epoch 29 of 500, Train Loss: 0.020899\n",
      "best loss:  0.02089856716519034\n",
      "Epoch 30 of 500, Train Loss: 0.020607\n",
      "best loss:  0.02060710959828661\n",
      "Epoch 31 of 500, Train Loss: 0.020339\n",
      "best loss:  0.020338591512846723\n",
      "Epoch 32 of 500, Train Loss: 0.020089\n",
      "best loss:  0.0200888607849533\n",
      "Epoch 33 of 500, Train Loss: 0.019856\n",
      "best loss:  0.019856236102790463\n",
      "Epoch 34 of 500, Train Loss: 0.019637\n",
      "best loss:  0.019637270671515834\n",
      "Epoch 35 of 500, Train Loss: 0.019428\n",
      "best loss:  0.0194279629197477\n",
      "Epoch 36 of 500, Train Loss: 0.019231\n",
      "best loss:  0.019231040133822118\n",
      "Epoch 37 of 500, Train Loss: 0.019046\n",
      "best loss:  0.0190460848177119\n",
      "Epoch 38 of 500, Train Loss: 0.018872\n",
      "best loss:  0.018871993535783057\n",
      "Epoch 39 of 500, Train Loss: 0.018710\n",
      "best loss:  0.018709766539535665\n",
      "Epoch 40 of 500, Train Loss: 0.018557\n",
      "best loss:  0.01855680094819783\n",
      "Epoch 41 of 500, Train Loss: 0.018414\n",
      "best loss:  0.018414302260701565\n",
      "Epoch 42 of 500, Train Loss: 0.018282\n",
      "best loss:  0.01828154610413986\n",
      "Epoch 43 of 500, Train Loss: 0.018156\n",
      "best loss:  0.01815610552249953\n",
      "Epoch 44 of 500, Train Loss: 0.018036\n",
      "best loss:  0.018036002542303746\n",
      "Epoch 45 of 500, Train Loss: 0.017919\n",
      "best loss:  0.01791948696906144\n",
      "Epoch 46 of 500, Train Loss: 0.017807\n",
      "best loss:  0.01780729517841517\n",
      "Epoch 47 of 500, Train Loss: 0.017697\n",
      "best loss:  0.017696763028286024\n",
      "Epoch 48 of 500, Train Loss: 0.017589\n",
      "best loss:  0.01758898399763943\n",
      "Epoch 49 of 500, Train Loss: 0.017485\n",
      "best loss:  0.017484721745266905\n",
      "Epoch 50 of 500, Train Loss: 0.017381\n",
      "best loss:  0.0173814369944491\n",
      "Epoch 51 of 500, Train Loss: 0.017282\n",
      "best loss:  0.017281578350162436\n",
      "Epoch 52 of 500, Train Loss: 0.017186\n",
      "best loss:  0.01718566194402285\n",
      "Epoch 53 of 500, Train Loss: 0.017094\n",
      "best loss:  0.017093766968004232\n",
      "Epoch 54 of 500, Train Loss: 0.017007\n",
      "best loss:  0.017006999451816877\n",
      "Epoch 55 of 500, Train Loss: 0.016923\n",
      "best loss:  0.01692272390621577\n",
      "Epoch 56 of 500, Train Loss: 0.016845\n",
      "best loss:  0.016844520027218604\n",
      "Epoch 57 of 500, Train Loss: 0.016771\n",
      "best loss:  0.016771012087363403\n",
      "Epoch 58 of 500, Train Loss: 0.016702\n",
      "best loss:  0.01670230325183275\n",
      "Epoch 59 of 500, Train Loss: 0.016640\n",
      "best loss:  0.016639619802626557\n",
      "Epoch 60 of 500, Train Loss: 0.016579\n",
      "best loss:  0.016578661123120998\n",
      "Epoch 61 of 500, Train Loss: 0.016524\n",
      "best loss:  0.016524237374388864\n",
      "Epoch 62 of 500, Train Loss: 0.016473\n",
      "best loss:  0.016473193708653813\n",
      "Epoch 63 of 500, Train Loss: 0.016426\n",
      "best loss:  0.016425662614899694\n",
      "Epoch 64 of 500, Train Loss: 0.016381\n",
      "best loss:  0.016381094779082856\n",
      "Epoch 65 of 500, Train Loss: 0.016339\n",
      "best loss:  0.01633934420037134\n",
      "Epoch 66 of 500, Train Loss: 0.016300\n",
      "best loss:  0.016300001656122203\n",
      "Epoch 67 of 500, Train Loss: 0.016266\n",
      "best loss:  0.016265748124190627\n",
      "Epoch 68 of 500, Train Loss: 0.016231\n",
      "best loss:  0.01623064234479179\n",
      "Epoch 69 of 500, Train Loss: 0.016201\n",
      "best loss:  0.016200534850241644\n",
      "Epoch 70 of 500, Train Loss: 0.016168\n",
      "best loss:  0.016168496525907787\n",
      "Epoch 71 of 500, Train Loss: 0.016141\n",
      "best loss:  0.016140909067512123\n",
      "Epoch 72 of 500, Train Loss: 0.016114\n",
      "best loss:  0.016114397232317865\n",
      "Epoch 73 of 500, Train Loss: 0.016090\n",
      "best loss:  0.01608981802119231\n",
      "Epoch 74 of 500, Train Loss: 0.016064\n",
      "best loss:  0.016064045135411903\n",
      "Epoch 75 of 500, Train Loss: 0.016040\n",
      "best loss:  0.016040458842072378\n",
      "Epoch 76 of 500, Train Loss: 0.016019\n",
      "best loss:  0.016019172876912652\n",
      "Epoch 77 of 500, Train Loss: 0.015999\n",
      "best loss:  0.015998585751124967\n",
      "Epoch 78 of 500, Train Loss: 0.015976\n",
      "best loss:  0.01597615655591935\n",
      "Epoch 79 of 500, Train Loss: 0.015957\n",
      "best loss:  0.015956878136435537\n",
      "Epoch 80 of 500, Train Loss: 0.015939\n",
      "best loss:  0.015938907347993147\n",
      "Epoch 81 of 500, Train Loss: 0.015921\n",
      "best loss:  0.015921216846133273\n",
      "Epoch 82 of 500, Train Loss: 0.015903\n",
      "best loss:  0.01590285838519353\n",
      "Epoch 83 of 500, Train Loss: 0.015884\n",
      "best loss:  0.015884331299367456\n",
      "Epoch 84 of 500, Train Loss: 0.015869\n",
      "best loss:  0.015868845469518673\n",
      "Epoch 85 of 500, Train Loss: 0.015853\n",
      "best loss:  0.01585273622343851\n",
      "Epoch 86 of 500, Train Loss: 0.015836\n",
      "best loss:  0.015836208507593708\n",
      "Epoch 87 of 500, Train Loss: 0.015821\n",
      "best loss:  0.015821016380967674\n",
      "Epoch 88 of 500, Train Loss: 0.015808\n",
      "best loss:  0.015807923963049773\n",
      "Epoch 89 of 500, Train Loss: 0.015792\n",
      "best loss:  0.0157921525043735\n",
      "Epoch 90 of 500, Train Loss: 0.015778\n",
      "best loss:  0.01577783166543696\n",
      "Epoch 91 of 500, Train Loss: 0.015765\n",
      "best loss:  0.015765004617824444\n",
      "Epoch 92 of 500, Train Loss: 0.015752\n",
      "best loss:  0.015751704623265726\n",
      "Epoch 93 of 500, Train Loss: 0.015738\n",
      "best loss:  0.01573832216582355\n",
      "Epoch 94 of 500, Train Loss: 0.015726\n",
      "best loss:  0.015725844690184534\n",
      "Epoch 95 of 500, Train Loss: 0.015713\n",
      "best loss:  0.01571329403307664\n",
      "Epoch 96 of 500, Train Loss: 0.015701\n",
      "best loss:  0.015700638266018585\n",
      "Epoch 97 of 500, Train Loss: 0.015690\n",
      "best loss:  0.01568951726374\n",
      "Epoch 98 of 500, Train Loss: 0.015677\n",
      "best loss:  0.01567743061937919\n",
      "Epoch 99 of 500, Train Loss: 0.015667\n",
      "best loss:  0.015666512074625813\n",
      "Epoch 100 of 500, Train Loss: 0.015656\n",
      "best loss:  0.01565587500587662\n",
      "Epoch 101 of 500, Train Loss: 0.015644\n",
      "best loss:  0.015644211211519734\n",
      "Epoch 102 of 500, Train Loss: 0.015635\n",
      "best loss:  0.015635267204186308\n",
      "Epoch 103 of 500, Train Loss: 0.015628\n",
      "best loss:  0.01562759748280099\n",
      "Epoch 104 of 500, Train Loss: 0.015618\n",
      "best loss:  0.015617621819806365\n",
      "Epoch 105 of 500, Train Loss: 0.015607\n",
      "best loss:  0.015606973090573654\n",
      "Epoch 106 of 500, Train Loss: 0.015597\n",
      "best loss:  0.01559708284100431\n",
      "Epoch 107 of 500, Train Loss: 0.015589\n",
      "best loss:  0.015589304604069439\n",
      "Epoch 108 of 500, Train Loss: 0.015580\n",
      "best loss:  0.015580017101371574\n",
      "Epoch 109 of 500, Train Loss: 0.015572\n",
      "best loss:  0.015571688639375475\n",
      "Epoch 110 of 500, Train Loss: 0.015566\n",
      "best loss:  0.015566112802096809\n",
      "Epoch 111 of 500, Train Loss: 0.015559\n",
      "best loss:  0.015559018276182415\n",
      "Epoch 112 of 500, Train Loss: 0.015550\n",
      "best loss:  0.015549771422511256\n",
      "Epoch 113 of 500, Train Loss: 0.015544\n",
      "best loss:  0.015543694545193611\n",
      "Epoch 114 of 500, Train Loss: 0.015536\n",
      "best loss:  0.015536270144056211\n",
      "Epoch 115 of 500, Train Loss: 0.015530\n",
      "best loss:  0.015530151105215124\n",
      "Epoch 116 of 500, Train Loss: 0.015527\n",
      "best loss:  0.015526608453635776\n",
      "Epoch 117 of 500, Train Loss: 0.015523\n",
      "best loss:  0.015522995877283619\n",
      "Epoch 118 of 500, Train Loss: 0.015519\n",
      "best loss:  0.01551884765034657\n",
      "Epoch 119 of 500, Train Loss: 0.015516\n",
      "best loss:  0.015515614636449952\n",
      "Epoch 120 of 500, Train Loss: 0.015510\n",
      "best loss:  0.01550971252131709\n",
      "Epoch 121 of 500, Train Loss: 0.015502\n",
      "best loss:  0.015502045704223316\n",
      "Epoch 122 of 500, Train Loss: 0.015494\n",
      "best loss:  0.015494061004038351\n",
      "Epoch 123 of 500, Train Loss: 0.015486\n",
      "best loss:  0.01548596463990109\n",
      "Epoch 124 of 500, Train Loss: 0.015478\n",
      "best loss:  0.015477859215899335\n",
      "Epoch 125 of 500, Train Loss: 0.015471\n",
      "best loss:  0.015471021272512802\n",
      "Epoch 126 of 500, Train Loss: 0.015464\n",
      "best loss:  0.015463700055283703\n",
      "Epoch 127 of 500, Train Loss: 0.015455\n",
      "best loss:  0.015455350332601038\n",
      "Epoch 128 of 500, Train Loss: 0.015447\n",
      "best loss:  0.015447282317850752\n",
      "Epoch 129 of 500, Train Loss: 0.015441\n",
      "best loss:  0.015440888640375143\n",
      "Epoch 130 of 500, Train Loss: 0.015434\n",
      "best loss:  0.015433596359121851\n",
      "Epoch 131 of 500, Train Loss: 0.015425\n",
      "best loss:  0.015424608016727752\n",
      "Epoch 132 of 500, Train Loss: 0.015418\n",
      "best loss:  0.015417661472736287\n",
      "Epoch 133 of 500, Train Loss: 0.015409\n",
      "best loss:  0.015409062462660317\n",
      "Epoch 134 of 500, Train Loss: 0.015399\n",
      "best loss:  0.015398919478811453\n",
      "Epoch 135 of 500, Train Loss: 0.015391\n",
      "best loss:  0.015390724063961038\n",
      "Epoch 136 of 500, Train Loss: 0.015382\n",
      "best loss:  0.015382341811769144\n",
      "Epoch 137 of 500, Train Loss: 0.015373\n",
      "best loss:  0.015373024294448858\n",
      "Epoch 138 of 500, Train Loss: 0.015365\n",
      "best loss:  0.015364576016043768\n",
      "Epoch 139 of 500, Train Loss: 0.015358\n",
      "best loss:  0.01535787152003699\n",
      "Epoch 140 of 500, Train Loss: 0.015350\n",
      "best loss:  0.01535015440791441\n",
      "Epoch 141 of 500, Train Loss: 0.015343\n",
      "best loss:  0.015342999677455355\n",
      "Epoch 142 of 500, Train Loss: 0.015338\n",
      "best loss:  0.015337563784253357\n",
      "Epoch 143 of 500, Train Loss: 0.015333\n",
      "best loss:  0.015333134687980348\n",
      "Epoch 144 of 500, Train Loss: 0.015326\n",
      "best loss:  0.015326370423986367\n",
      "Epoch 145 of 500, Train Loss: 0.015320\n",
      "best loss:  0.015320334374379409\n",
      "Epoch 146 of 500, Train Loss: 0.015315\n",
      "best loss:  0.015314776667659062\n",
      "Epoch 147 of 500, Train Loss: 0.015309\n",
      "best loss:  0.015309467018525976\n",
      "Epoch 148 of 500, Train Loss: 0.015305\n",
      "best loss:  0.015304605524162148\n",
      "Epoch 149 of 500, Train Loss: 0.015301\n",
      "best loss:  0.015300684081717185\n",
      "Epoch 150 of 500, Train Loss: 0.015296\n",
      "best loss:  0.015296255927596083\n",
      "Epoch 151 of 500, Train Loss: 0.015291\n",
      "best loss:  0.01529128195956176\n",
      "Epoch 152 of 500, Train Loss: 0.015286\n",
      "best loss:  0.01528644674663367\n",
      "Epoch 153 of 500, Train Loss: 0.015283\n",
      "best loss:  0.015282672987628166\n",
      "Epoch 154 of 500, Train Loss: 0.015278\n",
      "best loss:  0.015278193576698185\n",
      "Epoch 155 of 500, Train Loss: 0.015271\n",
      "best loss:  0.01527116161725094\n",
      "Epoch 156 of 500, Train Loss: 0.015266\n",
      "best loss:  0.015266312651070875\n",
      "Epoch 157 of 500, Train Loss: 0.015260\n",
      "best loss:  0.015259746377502546\n",
      "Epoch 158 of 500, Train Loss: 0.015255\n",
      "best loss:  0.0152547065637439\n",
      "Epoch 159 of 500, Train Loss: 0.015249\n",
      "best loss:  0.015248668348496102\n",
      "Epoch 160 of 500, Train Loss: 0.015243\n",
      "best loss:  0.015243112158333494\n",
      "Epoch 161 of 500, Train Loss: 0.015237\n",
      "best loss:  0.015237033054460013\n",
      "Epoch 162 of 500, Train Loss: 0.015231\n",
      "best loss:  0.015230597345638558\n",
      "Epoch 163 of 500, Train Loss: 0.015224\n",
      "best loss:  0.015223808482139509\n",
      "Epoch 164 of 500, Train Loss: 0.015217\n",
      "best loss:  0.015217448802224483\n",
      "Epoch 165 of 500, Train Loss: 0.015211\n",
      "best loss:  0.015210998342273208\n",
      "Epoch 166 of 500, Train Loss: 0.015205\n",
      "best loss:  0.015205176845878767\n",
      "Epoch 167 of 500, Train Loss: 0.015200\n",
      "best loss:  0.01520039130596971\n",
      "Epoch 168 of 500, Train Loss: 0.015196\n",
      "best loss:  0.015195967804851704\n",
      "Epoch 169 of 500, Train Loss: 0.015192\n",
      "best loss:  0.015192479317080114\n",
      "Epoch 170 of 500, Train Loss: 0.015188\n",
      "best loss:  0.015188118224050272\n",
      "Epoch 171 of 500, Train Loss: 0.015186\n",
      "best loss:  0.0151856310892121\n",
      "Epoch 172 of 500, Train Loss: 0.015182\n",
      "best loss:  0.015182251185801443\n",
      "Epoch 173 of 500, Train Loss: 0.015180\n",
      "best loss:  0.015180000064076942\n",
      "Epoch 174 of 500, Train Loss: 0.015178\n",
      "best loss:  0.015177861800689842\n",
      "Epoch 175 of 500, Train Loss: 0.015177\n",
      "best loss:  0.015176616145014518\n",
      "Epoch 176 of 500, Train Loss: 0.015175\n",
      "best loss:  0.015175042202643863\n",
      "Epoch 177 of 500, Train Loss: 0.015174\n",
      "best loss:  0.015174117051886192\n",
      "Epoch 178 of 500, Train Loss: 0.015172\n",
      "best loss:  0.01517173721632819\n",
      "Epoch 179 of 500, Train Loss: 0.015170\n",
      "best loss:  0.01516966353749943\n",
      "Epoch 180 of 500, Train Loss: 0.015168\n",
      "best loss:  0.015167567753070576\n",
      "Epoch 181 of 500, Train Loss: 0.015165\n",
      "best loss:  0.015165325910955768\n",
      "Epoch 182 of 500, Train Loss: 0.015164\n",
      "best loss:  0.015163672372316148\n",
      "Epoch 183 of 500, Train Loss: 0.015162\n",
      "best loss:  0.015162300914544034\n",
      "Epoch 184 of 500, Train Loss: 0.015161\n",
      "best loss:  0.015160998356593557\n",
      "Epoch 185 of 500, Train Loss: 0.015160\n",
      "best loss:  0.015160036934115526\n",
      "Epoch 186 of 500, Train Loss: 0.015159\n",
      "best loss:  0.015158862598843208\n",
      "Epoch 187 of 500, Train Loss: 0.015157\n",
      "best loss:  0.015157495167326587\n",
      "Epoch 188 of 500, Train Loss: 0.015156\n",
      "best loss:  0.015155905713516743\n",
      "Epoch 189 of 500, Train Loss: 0.015155\n",
      "best loss:  0.015154780817530371\n",
      "Epoch 190 of 500, Train Loss: 0.015154\n",
      "best loss:  0.015154000243998688\n",
      "Epoch 191 of 500, Train Loss: 0.015153\n",
      "best loss:  0.015153275875378285\n",
      "Epoch 192 of 500, Train Loss: 0.015153\n",
      "best loss:  0.015152660609121875\n",
      "Epoch 193 of 500, Train Loss: 0.015152\n",
      "best loss:  0.015152440727426664\n",
      "Epoch 194 of 500, Train Loss: 0.015152\n",
      "best loss:  0.015151519401679393\n",
      "Epoch 195 of 500, Train Loss: 0.015151\n",
      "best loss:  0.015151024775989282\n",
      "Epoch 196 of 500, Train Loss: 0.015150\n",
      "best loss:  0.015150107015122826\n",
      "Epoch 197 of 500, Train Loss: 0.015150\n",
      "best loss:  0.01514956956303339\n",
      "Epoch 198 of 500, Train Loss: 0.015149\n",
      "best loss:  0.015149045779591773\n",
      "Epoch 199 of 500, Train Loss: 0.015148\n",
      "best loss:  0.015148010001113538\n",
      "Epoch 200 of 500, Train Loss: 0.015147\n",
      "best loss:  0.015146636473359685\n",
      "Epoch 201 of 500, Train Loss: 0.015147\n",
      "Epoch 202 of 500, Train Loss: 0.015146\n",
      "best loss:  0.015146108709236016\n",
      "Epoch 203 of 500, Train Loss: 0.015146\n",
      "best loss:  0.015145983969993067\n",
      "Epoch 204 of 500, Train Loss: 0.015145\n",
      "best loss:  0.015145456771927619\n",
      "Epoch 205 of 500, Train Loss: 0.015145\n",
      "best loss:  0.015144596051635914\n",
      "Epoch 206 of 500, Train Loss: 0.015144\n",
      "best loss:  0.01514444731662317\n",
      "Epoch 207 of 500, Train Loss: 0.015144\n",
      "best loss:  0.015143891679603144\n",
      "Epoch 208 of 500, Train Loss: 0.015142\n",
      "best loss:  0.015142279065461367\n",
      "Epoch 209 of 500, Train Loss: 0.015143\n",
      "Epoch 210 of 500, Train Loss: 0.015141\n",
      "best loss:  0.015141177627839177\n",
      "Epoch 211 of 500, Train Loss: 0.015141\n",
      "Epoch 212 of 500, Train Loss: 0.015142\n",
      "Epoch 213 of 500, Train Loss: 0.015141\n",
      "best loss:  0.015140950552308787\n",
      "Epoch 214 of 500, Train Loss: 0.015141\n",
      "best loss:  0.015140910347510316\n",
      "Epoch 215 of 500, Train Loss: 0.015141\n",
      "Epoch 216 of 500, Train Loss: 0.015140\n",
      "best loss:  0.01514014471056077\n",
      "Epoch 217 of 500, Train Loss: 0.015141\n",
      "Epoch 218 of 500, Train Loss: 0.015138\n",
      "best loss:  0.015138499873064011\n",
      "Epoch 219 of 500, Train Loss: 0.015140\n",
      "Epoch 220 of 500, Train Loss: 0.015138\n",
      "best loss:  0.015137903591529502\n",
      "Epoch 221 of 500, Train Loss: 0.015140\n",
      "Epoch 222 of 500, Train Loss: 0.015138\n",
      "Epoch 223 of 500, Train Loss: 0.015139\n",
      "Epoch 224 of 500, Train Loss: 0.015138\n",
      "Epoch 225 of 500, Train Loss: 0.015138\n",
      "best loss:  0.015137547723878564\n",
      "Epoch 226 of 500, Train Loss: 0.015137\n",
      "best loss:  0.015136834201511256\n",
      "Epoch 227 of 500, Train Loss: 0.015137\n",
      "Epoch 228 of 500, Train Loss: 0.015137\n",
      "Epoch 229 of 500, Train Loss: 0.015137\n",
      "Epoch 230 of 500, Train Loss: 0.015136\n",
      "best loss:  0.01513637286518852\n",
      "Epoch 231 of 500, Train Loss: 0.015138\n",
      "Epoch 232 of 500, Train Loss: 0.015136\n",
      "best loss:  0.015135823652669859\n",
      "Epoch 233 of 500, Train Loss: 0.015139\n",
      "Epoch 234 of 500, Train Loss: 0.015136\n",
      "Epoch 235 of 500, Train Loss: 0.015142\n",
      "Epoch 236 of 500, Train Loss: 0.015139\n",
      "Epoch 237 of 500, Train Loss: 0.015145\n",
      "Epoch 238 of 500, Train Loss: 0.015141\n",
      "Epoch 239 of 500, Train Loss: 0.015149\n",
      "Epoch 240 of 500, Train Loss: 0.015148\n",
      "Epoch 241 of 500, Train Loss: 0.015158\n",
      "Epoch 242 of 500, Train Loss: 0.015161\n",
      "Epoch 243 of 500, Train Loss: 0.015178\n",
      "Epoch 244 of 500, Train Loss: 0.015196\n",
      "Epoch 245 of 500, Train Loss: 0.015218\n",
      "Epoch 246 of 500, Train Loss: 0.015264\n",
      "Epoch 247 of 500, Train Loss: 0.015281\n",
      "Epoch 248 of 500, Train Loss: 0.015338\n",
      "Epoch 249 of 500, Train Loss: 0.015319\n",
      "Epoch 250 of 500, Train Loss: 0.015355\n",
      "Epoch 251 of 500, Train Loss: 0.015309\n",
      "Epoch 252 of 500, Train Loss: 0.015329\n",
      "Epoch 253 of 500, Train Loss: 0.015276\n",
      "Epoch 254 of 500, Train Loss: 0.015286\n",
      "Epoch 255 of 500, Train Loss: 0.015241\n",
      "Epoch 256 of 500, Train Loss: 0.015246\n",
      "Epoch 257 of 500, Train Loss: 0.015210\n",
      "Epoch 258 of 500, Train Loss: 0.015217\n",
      "Epoch 259 of 500, Train Loss: 0.015190\n",
      "Epoch 260 of 500, Train Loss: 0.015199\n",
      "Epoch 261 of 500, Train Loss: 0.015174\n",
      "Epoch 262 of 500, Train Loss: 0.015186\n",
      "Epoch 263 of 500, Train Loss: 0.015166\n",
      "Epoch 264 of 500, Train Loss: 0.015182\n",
      "Epoch 265 of 500, Train Loss: 0.015161\n",
      "Epoch 266 of 500, Train Loss: 0.015176\n",
      "Epoch 267 of 500, Train Loss: 0.015160\n",
      "Epoch 268 of 500, Train Loss: 0.015177\n",
      "Epoch 269 of 500, Train Loss: 0.015162\n",
      "Epoch 270 of 500, Train Loss: 0.015179\n",
      "Epoch 271 of 500, Train Loss: 0.015164\n",
      "Epoch 272 of 500, Train Loss: 0.015184\n",
      "Epoch 273 of 500, Train Loss: 0.015169\n",
      "Epoch 274 of 500, Train Loss: 0.015187\n",
      "Epoch 275 of 500, Train Loss: 0.015172\n",
      "Epoch 276 of 500, Train Loss: 0.015190\n",
      "Epoch 277 of 500, Train Loss: 0.015173\n",
      "Epoch 278 of 500, Train Loss: 0.015190\n",
      "Epoch 279 of 500, Train Loss: 0.015173\n",
      "Epoch 280 of 500, Train Loss: 0.015188\n",
      "Epoch 281 of 500, Train Loss: 0.015171\n",
      "Epoch 282 of 500, Train Loss: 0.015185\n",
      "Epoch 283 of 500, Train Loss: 0.015169\n",
      "Epoch 284 of 500, Train Loss: 0.015181\n",
      "Epoch 285 of 500, Train Loss: 0.015166\n",
      "Epoch 286 of 500, Train Loss: 0.015178\n",
      "Epoch 287 of 500, Train Loss: 0.015161\n",
      "Epoch 288 of 500, Train Loss: 0.015173\n",
      "Epoch 289 of 500, Train Loss: 0.015156\n",
      "Epoch 290 of 500, Train Loss: 0.015168\n",
      "Epoch 291 of 500, Train Loss: 0.015152\n",
      "Epoch 292 of 500, Train Loss: 0.015165\n",
      "Epoch 293 of 500, Train Loss: 0.015149\n",
      "Epoch 294 of 500, Train Loss: 0.015164\n",
      "Epoch 295 of 500, Train Loss: 0.015147\n",
      "Epoch 296 of 500, Train Loss: 0.015164\n",
      "Epoch 297 of 500, Train Loss: 0.015145\n",
      "Epoch 298 of 500, Train Loss: 0.015161\n",
      "Epoch 299 of 500, Train Loss: 0.015144\n",
      "Epoch 300 of 500, Train Loss: 0.015159\n",
      "Epoch 301 of 500, Train Loss: 0.015143\n",
      "Epoch 302 of 500, Train Loss: 0.015158\n",
      "Epoch 303 of 500, Train Loss: 0.015143\n",
      "Epoch 304 of 500, Train Loss: 0.015160\n",
      "Epoch 305 of 500, Train Loss: 0.015143\n",
      "Epoch 306 of 500, Train Loss: 0.015159\n",
      "Epoch 307 of 500, Train Loss: 0.015143\n",
      "Epoch 308 of 500, Train Loss: 0.015161\n",
      "Epoch 309 of 500, Train Loss: 0.015143\n",
      "Epoch 310 of 500, Train Loss: 0.015157\n",
      "Epoch 311 of 500, Train Loss: 0.015141\n",
      "Epoch 312 of 500, Train Loss: 0.015157\n",
      "Epoch 313 of 500, Train Loss: 0.015141\n",
      "Epoch 314 of 500, Train Loss: 0.015154\n",
      "Epoch 315 of 500, Train Loss: 0.015141\n",
      "Epoch 316 of 500, Train Loss: 0.015155\n",
      "Epoch 317 of 500, Train Loss: 0.015141\n",
      "Epoch 318 of 500, Train Loss: 0.015151\n",
      "Epoch 319 of 500, Train Loss: 0.015142\n",
      "Epoch 320 of 500, Train Loss: 0.015154\n",
      "Epoch 321 of 500, Train Loss: 0.015141\n",
      "Epoch 322 of 500, Train Loss: 0.015150\n",
      "Epoch 323 of 500, Train Loss: 0.015139\n",
      "Epoch 324 of 500, Train Loss: 0.015146\n",
      "Epoch 325 of 500, Train Loss: 0.015136\n",
      "Epoch 326 of 500, Train Loss: 0.015143\n",
      "Epoch 327 of 500, Train Loss: 0.015140\n",
      "Epoch 328 of 500, Train Loss: 0.015148\n",
      "Epoch 329 of 500, Train Loss: 0.015137\n",
      "Epoch 330 of 500, Train Loss: 0.015144\n",
      "Epoch 331 of 500, Train Loss: 0.015141\n",
      "Epoch 332 of 500, Train Loss: 0.015150\n",
      "Epoch 333 of 500, Train Loss: 0.015140\n",
      "Epoch 334 of 500, Train Loss: 0.015145\n",
      "Epoch 335 of 500, Train Loss: 0.015137\n",
      "Epoch 336 of 500, Train Loss: 0.015143\n",
      "Epoch 337 of 500, Train Loss: 0.015135\n",
      "best loss:  0.015135307605658087\n",
      "Epoch 338 of 500, Train Loss: 0.015140\n",
      "Epoch 339 of 500, Train Loss: 0.015136\n",
      "Epoch 340 of 500, Train Loss: 0.015143\n",
      "Epoch 341 of 500, Train Loss: 0.015137\n",
      "Epoch 342 of 500, Train Loss: 0.015141\n",
      "Epoch 343 of 500, Train Loss: 0.015137\n",
      "Epoch 344 of 500, Train Loss: 0.015142\n",
      "Epoch 345 of 500, Train Loss: 0.015142\n",
      "Epoch 346 of 500, Train Loss: 0.015142\n",
      "Epoch 347 of 500, Train Loss: 0.015140\n",
      "Epoch 348 of 500, Train Loss: 0.015139\n",
      "Epoch 349 of 500, Train Loss: 0.015138\n",
      "Epoch 350 of 500, Train Loss: 0.015137\n",
      "Epoch 351 of 500, Train Loss: 0.015135\n",
      "best loss:  0.015134686035151022\n",
      "Epoch 352 of 500, Train Loss: 0.015138\n",
      "Epoch 353 of 500, Train Loss: 0.015136\n",
      "Epoch 354 of 500, Train Loss: 0.015137\n",
      "Epoch 355 of 500, Train Loss: 0.015135\n",
      "Epoch 356 of 500, Train Loss: 0.015136\n",
      "Epoch 357 of 500, Train Loss: 0.015135\n",
      "Epoch 358 of 500, Train Loss: 0.015136\n",
      "Epoch 359 of 500, Train Loss: 0.015135\n",
      "best loss:  0.015134583151756692\n",
      "Epoch 360 of 500, Train Loss: 0.015140\n",
      "Epoch 361 of 500, Train Loss: 0.015139\n",
      "Epoch 362 of 500, Train Loss: 0.015140\n",
      "Epoch 363 of 500, Train Loss: 0.015135\n",
      "Epoch 364 of 500, Train Loss: 0.015136\n",
      "Epoch 365 of 500, Train Loss: 0.015135\n",
      "Epoch 366 of 500, Train Loss: 0.015135\n",
      "Epoch 367 of 500, Train Loss: 0.015135\n",
      "Epoch 368 of 500, Train Loss: 0.015137\n",
      "Epoch 369 of 500, Train Loss: 0.015135\n",
      "Epoch 370 of 500, Train Loss: 0.015134\n",
      "best loss:  0.015133997384826977\n",
      "Epoch 371 of 500, Train Loss: 0.015130\n",
      "best loss:  0.015130399748892343\n",
      "Epoch 372 of 500, Train Loss: 0.015130\n",
      "best loss:  0.015129708250905134\n",
      "Epoch 373 of 500, Train Loss: 0.015131\n",
      "Epoch 374 of 500, Train Loss: 0.015131\n",
      "Epoch 375 of 500, Train Loss: 0.015132\n",
      "Epoch 376 of 500, Train Loss: 0.015133\n",
      "Epoch 377 of 500, Train Loss: 0.015135\n",
      "Epoch 378 of 500, Train Loss: 0.015132\n",
      "Epoch 379 of 500, Train Loss: 0.015135\n",
      "Epoch 380 of 500, Train Loss: 0.015134\n",
      "Epoch 381 of 500, Train Loss: 0.015136\n",
      "Epoch 382 of 500, Train Loss: 0.015135\n",
      "Epoch 383 of 500, Train Loss: 0.015136\n",
      "Epoch 384 of 500, Train Loss: 0.015131\n",
      "Epoch 385 of 500, Train Loss: 0.015132\n",
      "Epoch 386 of 500, Train Loss: 0.015128\n",
      "best loss:  0.015127664255952938\n",
      "Epoch 387 of 500, Train Loss: 0.015131\n",
      "Epoch 388 of 500, Train Loss: 0.015130\n",
      "Epoch 389 of 500, Train Loss: 0.015132\n",
      "Epoch 390 of 500, Train Loss: 0.015129\n",
      "Epoch 391 of 500, Train Loss: 0.015131\n",
      "Epoch 392 of 500, Train Loss: 0.015128\n",
      "best loss:  0.015127596069739178\n",
      "Epoch 393 of 500, Train Loss: 0.015130\n",
      "Epoch 394 of 500, Train Loss: 0.015129\n",
      "Epoch 395 of 500, Train Loss: 0.015131\n",
      "Epoch 396 of 500, Train Loss: 0.015129\n",
      "Epoch 397 of 500, Train Loss: 0.015133\n",
      "Epoch 398 of 500, Train Loss: 0.015128\n",
      "Epoch 399 of 500, Train Loss: 0.015132\n",
      "Epoch 400 of 500, Train Loss: 0.015126\n",
      "best loss:  0.015126051704756656\n",
      "Epoch 401 of 500, Train Loss: 0.015129\n",
      "Epoch 402 of 500, Train Loss: 0.015124\n",
      "best loss:  0.015124363900304865\n",
      "Epoch 403 of 500, Train Loss: 0.015132\n",
      "Epoch 404 of 500, Train Loss: 0.015124\n",
      "Epoch 405 of 500, Train Loss: 0.015131\n",
      "Epoch 406 of 500, Train Loss: 0.015124\n",
      "Epoch 407 of 500, Train Loss: 0.015129\n",
      "Epoch 408 of 500, Train Loss: 0.015124\n",
      "best loss:  0.01512426304222128\n",
      "Epoch 409 of 500, Train Loss: 0.015130\n",
      "Epoch 410 of 500, Train Loss: 0.015122\n",
      "best loss:  0.015121548504942248\n",
      "Epoch 411 of 500, Train Loss: 0.015127\n",
      "Epoch 412 of 500, Train Loss: 0.015125\n",
      "Epoch 413 of 500, Train Loss: 0.015131\n",
      "Epoch 414 of 500, Train Loss: 0.015128\n",
      "Epoch 415 of 500, Train Loss: 0.015134\n",
      "Epoch 416 of 500, Train Loss: 0.015129\n",
      "Epoch 417 of 500, Train Loss: 0.015134\n",
      "Epoch 418 of 500, Train Loss: 0.015128\n",
      "Epoch 419 of 500, Train Loss: 0.015131\n",
      "Epoch 420 of 500, Train Loss: 0.015125\n",
      "Epoch 421 of 500, Train Loss: 0.015131\n",
      "Epoch 422 of 500, Train Loss: 0.015125\n",
      "Epoch 423 of 500, Train Loss: 0.015132\n",
      "Epoch 424 of 500, Train Loss: 0.015127\n",
      "Epoch 425 of 500, Train Loss: 0.015131\n",
      "Epoch 426 of 500, Train Loss: 0.015125\n",
      "Epoch 427 of 500, Train Loss: 0.015129\n",
      "Epoch 428 of 500, Train Loss: 0.015126\n",
      "Epoch 429 of 500, Train Loss: 0.015132\n",
      "Epoch 430 of 500, Train Loss: 0.015130\n",
      "Epoch 431 of 500, Train Loss: 0.015133\n",
      "Epoch 432 of 500, Train Loss: 0.015128\n",
      "Epoch 433 of 500, Train Loss: 0.015134\n",
      "Epoch 434 of 500, Train Loss: 0.015132\n",
      "Epoch 435 of 500, Train Loss: 0.015132\n",
      "Epoch 436 of 500, Train Loss: 0.015128\n",
      "Epoch 437 of 500, Train Loss: 0.015129\n",
      "Epoch 438 of 500, Train Loss: 0.015130\n",
      "Epoch 439 of 500, Train Loss: 0.015131\n",
      "Epoch 440 of 500, Train Loss: 0.015131\n",
      "Epoch 441 of 500, Train Loss: 0.015130\n",
      "Epoch 442 of 500, Train Loss: 0.015130\n",
      "Epoch 443 of 500, Train Loss: 0.015129\n",
      "Epoch 444 of 500, Train Loss: 0.015134\n",
      "Epoch 445 of 500, Train Loss: 0.015132\n",
      "Epoch 446 of 500, Train Loss: 0.015136\n",
      "Epoch 447 of 500, Train Loss: 0.015135\n",
      "Epoch 448 of 500, Train Loss: 0.015139\n",
      "Epoch 449 of 500, Train Loss: 0.015137\n",
      "Epoch 450 of 500, Train Loss: 0.015143\n",
      "Epoch 451 of 500, Train Loss: 0.015138\n",
      "Epoch 452 of 500, Train Loss: 0.015146\n",
      "Epoch 453 of 500, Train Loss: 0.015139\n",
      "Epoch 454 of 500, Train Loss: 0.015146\n",
      "Epoch 455 of 500, Train Loss: 0.015141\n",
      "Epoch 456 of 500, Train Loss: 0.015155\n",
      "Epoch 457 of 500, Train Loss: 0.015151\n",
      "Epoch 458 of 500, Train Loss: 0.015161\n",
      "Epoch 459 of 500, Train Loss: 0.015154\n",
      "Epoch 460 of 500, Train Loss: 0.015170\n",
      "Epoch 461 of 500, Train Loss: 0.015162\n",
      "Epoch 462 of 500, Train Loss: 0.015180\n",
      "Epoch 463 of 500, Train Loss: 0.015173\n",
      "Epoch 464 of 500, Train Loss: 0.015196\n",
      "Epoch 465 of 500, Train Loss: 0.015184\n",
      "Epoch 466 of 500, Train Loss: 0.015213\n",
      "Epoch 467 of 500, Train Loss: 0.015202\n",
      "Epoch 468 of 500, Train Loss: 0.015231\n",
      "Epoch 469 of 500, Train Loss: 0.015208\n",
      "Epoch 470 of 500, Train Loss: 0.015237\n",
      "Epoch 471 of 500, Train Loss: 0.015212\n",
      "Epoch 472 of 500, Train Loss: 0.015240\n",
      "Epoch 473 of 500, Train Loss: 0.015210\n",
      "Epoch 474 of 500, Train Loss: 0.015239\n",
      "Epoch 475 of 500, Train Loss: 0.015202\n",
      "Epoch 476 of 500, Train Loss: 0.015230\n",
      "Epoch 477 of 500, Train Loss: 0.015190\n",
      "Epoch 478 of 500, Train Loss: 0.015220\n",
      "Epoch 479 of 500, Train Loss: 0.015182\n",
      "Epoch 480 of 500, Train Loss: 0.015212\n",
      "Epoch 481 of 500, Train Loss: 0.015172\n",
      "Epoch 482 of 500, Train Loss: 0.015199\n",
      "Epoch 483 of 500, Train Loss: 0.015159\n",
      "Epoch 484 of 500, Train Loss: 0.015187\n",
      "Epoch 485 of 500, Train Loss: 0.015151\n",
      "Epoch 486 of 500, Train Loss: 0.015182\n",
      "Epoch 487 of 500, Train Loss: 0.015142\n",
      "Epoch 488 of 500, Train Loss: 0.015172\n",
      "Epoch 489 of 500, Train Loss: 0.015134\n",
      "Epoch 490 of 500, Train Loss: 0.015162\n",
      "Epoch 491 of 500, Train Loss: 0.015125\n",
      "Epoch 492 of 500, Train Loss: 0.015156\n",
      "Epoch 493 of 500, Train Loss: 0.015126\n",
      "Epoch 494 of 500, Train Loss: 0.015154\n",
      "Epoch 495 of 500, Train Loss: 0.015123\n",
      "Epoch 496 of 500, Train Loss: 0.015153\n",
      "Epoch 497 of 500, Train Loss: 0.015130\n",
      "Epoch 498 of 500, Train Loss: 0.015162\n",
      "Epoch 499 of 500, Train Loss: 0.015138\n",
      "Epoch 500 of 500, Train Loss: 0.015174\n",
      "latent train shape:  (16395, 20)\n",
      "M: 20, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 32\n",
      "Training the subspace: 0 / 20\n",
      "Training the subspace: 1 / 20\n",
      "Training the subspace: 2 / 20\n",
      "Training the subspace: 3 / 20\n",
      "Training the subspace: 4 / 20\n",
      "Training the subspace: 5 / 20\n",
      "Training the subspace: 6 / 20\n",
      "Training the subspace: 7 / 20\n",
      "Training the subspace: 8 / 20\n",
      "Training the subspace: 9 / 20\n",
      "Training the subspace: 10 / 20\n",
      "Training the subspace: 11 / 20\n",
      "Training the subspace: 12 / 20\n",
      "Training the subspace: 13 / 20\n",
      "Training the subspace: 14 / 20\n",
      "Training the subspace: 15 / 20\n",
      "Training the subspace: 16 / 20\n",
      "Training the subspace: 17 / 20\n",
      "Training the subspace: 18 / 20\n",
      "Training the subspace: 19 / 20\n",
      "Encoding the subspace: 0 / 20\n",
      "Encoding the subspace: 1 / 20\n",
      "Encoding the subspace: 2 / 20\n",
      "Encoding the subspace: 3 / 20\n",
      "Encoding the subspace: 4 / 20\n",
      "Encoding the subspace: 5 / 20\n",
      "Encoding the subspace: 6 / 20\n",
      "Encoding the subspace: 7 / 20\n",
      "Encoding the subspace: 8 / 20\n",
      "Encoding the subspace: 9 / 20\n",
      "Encoding the subspace: 10 / 20\n",
      "Encoding the subspace: 11 / 20\n",
      "Encoding the subspace: 12 / 20\n",
      "Encoding the subspace: 13 / 20\n",
      "Encoding the subspace: 14 / 20\n",
      "Encoding the subspace: 15 / 20\n",
      "Encoding the subspace: 16 / 20\n",
      "Encoding the subspace: 17 / 20\n",
      "Encoding the subspace: 18 / 20\n",
      "Encoding the subspace: 19 / 20\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=30, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.515962\n",
      "best loss:  0.5159624591469765\n",
      "Epoch 2 of 500, Train Loss: 0.062539\n",
      "best loss:  0.06253924277925127\n",
      "Epoch 3 of 500, Train Loss: 0.055453\n",
      "best loss:  0.055453478041928\n",
      "Epoch 4 of 500, Train Loss: 0.053815\n",
      "best loss:  0.053815410300988215\n",
      "Epoch 5 of 500, Train Loss: 0.051112\n",
      "best loss:  0.05111223612027604\n",
      "Epoch 6 of 500, Train Loss: 0.047339\n",
      "best loss:  0.04733866841673071\n",
      "Epoch 7 of 500, Train Loss: 0.043109\n",
      "best loss:  0.04310927237113288\n",
      "Epoch 8 of 500, Train Loss: 0.039256\n",
      "best loss:  0.03925606512776829\n",
      "Epoch 9 of 500, Train Loss: 0.036111\n",
      "best loss:  0.036110998768460495\n",
      "Epoch 10 of 500, Train Loss: 0.033577\n",
      "best loss:  0.033576732753620796\n",
      "Epoch 11 of 500, Train Loss: 0.031453\n",
      "best loss:  0.03145280663851764\n",
      "Epoch 12 of 500, Train Loss: 0.029594\n",
      "best loss:  0.029593706201372104\n",
      "Epoch 13 of 500, Train Loss: 0.027957\n",
      "best loss:  0.027956601963270197\n",
      "Epoch 14 of 500, Train Loss: 0.026549\n",
      "best loss:  0.026549320931495854\n",
      "Epoch 15 of 500, Train Loss: 0.025362\n",
      "best loss:  0.025362309993446856\n",
      "Epoch 16 of 500, Train Loss: 0.024356\n",
      "best loss:  0.024356343463521682\n",
      "Epoch 17 of 500, Train Loss: 0.023483\n",
      "best loss:  0.023483092335771134\n",
      "Epoch 18 of 500, Train Loss: 0.022714\n",
      "best loss:  0.022713927046283937\n",
      "Epoch 19 of 500, Train Loss: 0.022030\n",
      "best loss:  0.02202982252081062\n",
      "Epoch 20 of 500, Train Loss: 0.021420\n",
      "best loss:  0.021420215749698004\n",
      "Epoch 21 of 500, Train Loss: 0.020876\n",
      "best loss:  0.020875747508031874\n",
      "Epoch 22 of 500, Train Loss: 0.020390\n",
      "best loss:  0.020390160005925903\n",
      "Epoch 23 of 500, Train Loss: 0.019953\n",
      "best loss:  0.019952879187808005\n",
      "Epoch 24 of 500, Train Loss: 0.019555\n",
      "best loss:  0.019554577056497206\n",
      "Epoch 25 of 500, Train Loss: 0.019188\n",
      "best loss:  0.01918785598051424\n",
      "Epoch 26 of 500, Train Loss: 0.018848\n",
      "best loss:  0.018848060452254747\n",
      "Epoch 27 of 500, Train Loss: 0.018533\n",
      "best loss:  0.018533095257634715\n",
      "Epoch 28 of 500, Train Loss: 0.018238\n",
      "best loss:  0.01823824229866786\n",
      "Epoch 29 of 500, Train Loss: 0.017964\n",
      "best loss:  0.017963765131972023\n",
      "Epoch 30 of 500, Train Loss: 0.017712\n",
      "best loss:  0.017712218503967692\n",
      "Epoch 31 of 500, Train Loss: 0.017479\n",
      "best loss:  0.01747902190468866\n",
      "Epoch 32 of 500, Train Loss: 0.017264\n",
      "best loss:  0.01726439623784837\n",
      "Epoch 33 of 500, Train Loss: 0.017067\n",
      "best loss:  0.017066860288587334\n",
      "Epoch 34 of 500, Train Loss: 0.016882\n",
      "best loss:  0.01688151127548686\n",
      "Epoch 35 of 500, Train Loss: 0.016709\n",
      "best loss:  0.01670933709919359\n",
      "Epoch 36 of 500, Train Loss: 0.016554\n",
      "best loss:  0.016553555941028707\n",
      "Epoch 37 of 500, Train Loss: 0.016401\n",
      "best loss:  0.016401119757391325\n",
      "Epoch 38 of 500, Train Loss: 0.016263\n",
      "best loss:  0.016262673126044503\n",
      "Epoch 39 of 500, Train Loss: 0.016127\n",
      "best loss:  0.016126982703924094\n",
      "Epoch 40 of 500, Train Loss: 0.016003\n",
      "best loss:  0.0160029411125051\n",
      "Epoch 41 of 500, Train Loss: 0.015878\n",
      "best loss:  0.015877979599767864\n",
      "Epoch 42 of 500, Train Loss: 0.015765\n",
      "best loss:  0.01576514960701628\n",
      "Epoch 43 of 500, Train Loss: 0.015653\n",
      "best loss:  0.01565318594415051\n",
      "Epoch 44 of 500, Train Loss: 0.015556\n",
      "best loss:  0.01555613652724491\n",
      "Epoch 45 of 500, Train Loss: 0.015460\n",
      "best loss:  0.01545972145223889\n",
      "Epoch 46 of 500, Train Loss: 0.015370\n",
      "best loss:  0.015369874515025623\n",
      "Epoch 47 of 500, Train Loss: 0.015291\n",
      "best loss:  0.015290538592682791\n",
      "Epoch 48 of 500, Train Loss: 0.015214\n",
      "best loss:  0.015213977716738618\n",
      "Epoch 49 of 500, Train Loss: 0.015143\n",
      "best loss:  0.015143149945246013\n",
      "Epoch 50 of 500, Train Loss: 0.015078\n",
      "best loss:  0.015077893971239917\n",
      "Epoch 51 of 500, Train Loss: 0.015016\n",
      "best loss:  0.015016240593228244\n",
      "Epoch 52 of 500, Train Loss: 0.014961\n",
      "best loss:  0.014960748256525285\n",
      "Epoch 53 of 500, Train Loss: 0.014908\n",
      "best loss:  0.014908422895999583\n",
      "Epoch 54 of 500, Train Loss: 0.014862\n",
      "best loss:  0.014861531216784763\n",
      "Epoch 55 of 500, Train Loss: 0.014812\n",
      "best loss:  0.014811805550499942\n",
      "Epoch 56 of 500, Train Loss: 0.014768\n",
      "best loss:  0.014768437722466828\n",
      "Epoch 57 of 500, Train Loss: 0.014729\n",
      "best loss:  0.014728867275822663\n",
      "Epoch 58 of 500, Train Loss: 0.014688\n",
      "best loss:  0.014688373153240513\n",
      "Epoch 59 of 500, Train Loss: 0.014646\n",
      "best loss:  0.014646232667353813\n",
      "Epoch 60 of 500, Train Loss: 0.014613\n",
      "best loss:  0.01461279433348772\n",
      "Epoch 61 of 500, Train Loss: 0.014577\n",
      "best loss:  0.01457700638850436\n",
      "Epoch 62 of 500, Train Loss: 0.014547\n",
      "best loss:  0.014546765456952573\n",
      "Epoch 63 of 500, Train Loss: 0.014516\n",
      "best loss:  0.01451618025145248\n",
      "Epoch 64 of 500, Train Loss: 0.014485\n",
      "best loss:  0.01448519251087334\n",
      "Epoch 65 of 500, Train Loss: 0.014457\n",
      "best loss:  0.01445691449822165\n",
      "Epoch 66 of 500, Train Loss: 0.014428\n",
      "best loss:  0.01442768145352602\n",
      "Epoch 67 of 500, Train Loss: 0.014402\n",
      "best loss:  0.014402356001687445\n",
      "Epoch 68 of 500, Train Loss: 0.014377\n",
      "best loss:  0.014376638969767463\n",
      "Epoch 69 of 500, Train Loss: 0.014352\n",
      "best loss:  0.01435154573062377\n",
      "Epoch 70 of 500, Train Loss: 0.014332\n",
      "best loss:  0.014331840666808737\n",
      "Epoch 71 of 500, Train Loss: 0.014308\n",
      "best loss:  0.014308350392871774\n",
      "Epoch 72 of 500, Train Loss: 0.014286\n",
      "best loss:  0.014286286388920951\n",
      "Epoch 73 of 500, Train Loss: 0.014268\n",
      "best loss:  0.014268244798755447\n",
      "Epoch 74 of 500, Train Loss: 0.014249\n",
      "best loss:  0.014248521677028864\n",
      "Epoch 75 of 500, Train Loss: 0.014228\n",
      "best loss:  0.014228026647041299\n",
      "Epoch 76 of 500, Train Loss: 0.014209\n",
      "best loss:  0.014208906113355992\n",
      "Epoch 77 of 500, Train Loss: 0.014191\n",
      "best loss:  0.014191396343124699\n",
      "Epoch 78 of 500, Train Loss: 0.014173\n",
      "best loss:  0.014172839246962811\n",
      "Epoch 79 of 500, Train Loss: 0.014156\n",
      "best loss:  0.01415646380055407\n",
      "Epoch 80 of 500, Train Loss: 0.014140\n",
      "best loss:  0.014139611176539993\n",
      "Epoch 81 of 500, Train Loss: 0.014120\n",
      "best loss:  0.014120198727374203\n",
      "Epoch 82 of 500, Train Loss: 0.014106\n",
      "best loss:  0.014105806214050859\n",
      "Epoch 83 of 500, Train Loss: 0.014087\n",
      "best loss:  0.014086608887539974\n",
      "Epoch 84 of 500, Train Loss: 0.014066\n",
      "best loss:  0.01406642165334621\n",
      "Epoch 85 of 500, Train Loss: 0.014050\n",
      "best loss:  0.014050141464225775\n",
      "Epoch 86 of 500, Train Loss: 0.014033\n",
      "best loss:  0.01403273804322636\n",
      "Epoch 87 of 500, Train Loss: 0.014012\n",
      "best loss:  0.01401226097526563\n",
      "Epoch 88 of 500, Train Loss: 0.013998\n",
      "best loss:  0.013997655491391557\n",
      "Epoch 89 of 500, Train Loss: 0.013978\n",
      "best loss:  0.013978433394813621\n",
      "Epoch 90 of 500, Train Loss: 0.013964\n",
      "best loss:  0.013964297078448788\n",
      "Epoch 91 of 500, Train Loss: 0.013949\n",
      "best loss:  0.013949410098942372\n",
      "Epoch 92 of 500, Train Loss: 0.013936\n",
      "best loss:  0.01393618997589815\n",
      "Epoch 93 of 500, Train Loss: 0.013923\n",
      "best loss:  0.013922673373035509\n",
      "Epoch 94 of 500, Train Loss: 0.013911\n",
      "best loss:  0.013910660678800556\n",
      "Epoch 95 of 500, Train Loss: 0.013901\n",
      "best loss:  0.01390129462937651\n",
      "Epoch 96 of 500, Train Loss: 0.013893\n",
      "best loss:  0.013892585625321524\n",
      "Epoch 97 of 500, Train Loss: 0.013887\n",
      "best loss:  0.013886744962362644\n",
      "Epoch 98 of 500, Train Loss: 0.013881\n",
      "best loss:  0.013880654461068181\n",
      "Epoch 99 of 500, Train Loss: 0.013870\n",
      "best loss:  0.013870085072487073\n",
      "Epoch 100 of 500, Train Loss: 0.013856\n",
      "best loss:  0.013856307725925648\n",
      "Epoch 101 of 500, Train Loss: 0.013846\n",
      "best loss:  0.013845870887468996\n",
      "Epoch 102 of 500, Train Loss: 0.013835\n",
      "best loss:  0.013834875956441225\n",
      "Epoch 103 of 500, Train Loss: 0.013822\n",
      "best loss:  0.013822376990217947\n",
      "Epoch 104 of 500, Train Loss: 0.013808\n",
      "best loss:  0.013808410439654722\n",
      "Epoch 105 of 500, Train Loss: 0.013799\n",
      "best loss:  0.013799350712107234\n",
      "Epoch 106 of 500, Train Loss: 0.013788\n",
      "best loss:  0.013788494649442462\n",
      "Epoch 107 of 500, Train Loss: 0.013781\n",
      "best loss:  0.013781222042378558\n",
      "Epoch 108 of 500, Train Loss: 0.013771\n",
      "best loss:  0.01377145957148475\n",
      "Epoch 109 of 500, Train Loss: 0.013766\n",
      "best loss:  0.013766071686968785\n",
      "Epoch 110 of 500, Train Loss: 0.013758\n",
      "best loss:  0.013757798809002247\n",
      "Epoch 111 of 500, Train Loss: 0.013750\n",
      "best loss:  0.013750437884898352\n",
      "Epoch 112 of 500, Train Loss: 0.013741\n",
      "best loss:  0.01374130002439542\n",
      "Epoch 113 of 500, Train Loss: 0.013732\n",
      "best loss:  0.013732103647369992\n",
      "Epoch 114 of 500, Train Loss: 0.013724\n",
      "best loss:  0.013724340753117216\n",
      "Epoch 115 of 500, Train Loss: 0.013716\n",
      "best loss:  0.013715884648489066\n",
      "Epoch 116 of 500, Train Loss: 0.013707\n",
      "best loss:  0.013707188138324954\n",
      "Epoch 117 of 500, Train Loss: 0.013700\n",
      "best loss:  0.013700013354424272\n",
      "Epoch 118 of 500, Train Loss: 0.013691\n",
      "best loss:  0.01369113322413281\n",
      "Epoch 119 of 500, Train Loss: 0.013684\n",
      "best loss:  0.01368383245403669\n",
      "Epoch 120 of 500, Train Loss: 0.013675\n",
      "best loss:  0.013675214831497116\n",
      "Epoch 121 of 500, Train Loss: 0.013665\n",
      "best loss:  0.013665153679723107\n",
      "Epoch 122 of 500, Train Loss: 0.013654\n",
      "best loss:  0.013653848907060185\n",
      "Epoch 123 of 500, Train Loss: 0.013644\n",
      "best loss:  0.013644335439740933\n",
      "Epoch 124 of 500, Train Loss: 0.013634\n",
      "best loss:  0.013633809393080485\n",
      "Epoch 125 of 500, Train Loss: 0.013624\n",
      "best loss:  0.013623905170288355\n",
      "Epoch 126 of 500, Train Loss: 0.013615\n",
      "best loss:  0.013614768815062609\n",
      "Epoch 127 of 500, Train Loss: 0.013605\n",
      "best loss:  0.013605010241825971\n",
      "Epoch 128 of 500, Train Loss: 0.013597\n",
      "best loss:  0.013597470390756592\n",
      "Epoch 129 of 500, Train Loss: 0.013591\n",
      "best loss:  0.013590598225100833\n",
      "Epoch 130 of 500, Train Loss: 0.013587\n",
      "best loss:  0.013586636303063093\n",
      "Epoch 131 of 500, Train Loss: 0.013583\n",
      "best loss:  0.013582952309746382\n",
      "Epoch 132 of 500, Train Loss: 0.013580\n",
      "best loss:  0.01357993659655412\n",
      "Epoch 133 of 500, Train Loss: 0.013577\n",
      "best loss:  0.013576811236598305\n",
      "Epoch 134 of 500, Train Loss: 0.013574\n",
      "best loss:  0.013574308841651695\n",
      "Epoch 135 of 500, Train Loss: 0.013573\n",
      "best loss:  0.013572987960471027\n",
      "Epoch 136 of 500, Train Loss: 0.013572\n",
      "best loss:  0.013571556772388763\n",
      "Epoch 137 of 500, Train Loss: 0.013570\n",
      "best loss:  0.013570106851366573\n",
      "Epoch 138 of 500, Train Loss: 0.013568\n",
      "best loss:  0.013567951292407792\n",
      "Epoch 139 of 500, Train Loss: 0.013566\n",
      "best loss:  0.013566439386092618\n",
      "Epoch 140 of 500, Train Loss: 0.013563\n",
      "best loss:  0.013563411324979853\n",
      "Epoch 141 of 500, Train Loss: 0.013561\n",
      "best loss:  0.0135607328787067\n",
      "Epoch 142 of 500, Train Loss: 0.013556\n",
      "best loss:  0.0135561723872124\n",
      "Epoch 143 of 500, Train Loss: 0.013551\n",
      "best loss:  0.013550620422971095\n",
      "Epoch 144 of 500, Train Loss: 0.013546\n",
      "best loss:  0.01354616270364486\n",
      "Epoch 145 of 500, Train Loss: 0.013540\n",
      "best loss:  0.013539900406500209\n",
      "Epoch 146 of 500, Train Loss: 0.013532\n",
      "best loss:  0.01353166698272405\n",
      "Epoch 147 of 500, Train Loss: 0.013522\n",
      "best loss:  0.013522473472538729\n",
      "Epoch 148 of 500, Train Loss: 0.013512\n",
      "best loss:  0.01351213647420843\n",
      "Epoch 149 of 500, Train Loss: 0.013501\n",
      "best loss:  0.013500569840846223\n",
      "Epoch 150 of 500, Train Loss: 0.013490\n",
      "best loss:  0.013490336522994232\n",
      "Epoch 151 of 500, Train Loss: 0.013480\n",
      "best loss:  0.013480136873085023\n",
      "Epoch 152 of 500, Train Loss: 0.013471\n",
      "best loss:  0.01347122220918103\n",
      "Epoch 153 of 500, Train Loss: 0.013464\n",
      "best loss:  0.013463652558496597\n",
      "Epoch 154 of 500, Train Loss: 0.013457\n",
      "best loss:  0.01345686220979975\n",
      "Epoch 155 of 500, Train Loss: 0.013452\n",
      "best loss:  0.013451670801182198\n",
      "Epoch 156 of 500, Train Loss: 0.013447\n",
      "best loss:  0.013446771191404194\n",
      "Epoch 157 of 500, Train Loss: 0.013442\n",
      "best loss:  0.013441921273596556\n",
      "Epoch 158 of 500, Train Loss: 0.013438\n",
      "best loss:  0.013438368527725873\n",
      "Epoch 159 of 500, Train Loss: 0.013434\n",
      "best loss:  0.013434184604602353\n",
      "Epoch 160 of 500, Train Loss: 0.013431\n",
      "best loss:  0.013431173334594874\n",
      "Epoch 161 of 500, Train Loss: 0.013429\n",
      "best loss:  0.013429065098464418\n",
      "Epoch 162 of 500, Train Loss: 0.013427\n",
      "best loss:  0.013426782989733701\n",
      "Epoch 163 of 500, Train Loss: 0.013426\n",
      "best loss:  0.013425714523818473\n",
      "Epoch 164 of 500, Train Loss: 0.013424\n",
      "best loss:  0.013424347421468431\n",
      "Epoch 165 of 500, Train Loss: 0.013422\n",
      "best loss:  0.013422187987413589\n",
      "Epoch 166 of 500, Train Loss: 0.013419\n",
      "best loss:  0.013419478693110653\n",
      "Epoch 167 of 500, Train Loss: 0.013417\n",
      "best loss:  0.013417375864652383\n",
      "Epoch 168 of 500, Train Loss: 0.013415\n",
      "best loss:  0.013414556323460397\n",
      "Epoch 169 of 500, Train Loss: 0.013413\n",
      "best loss:  0.013412763061842975\n",
      "Epoch 170 of 500, Train Loss: 0.013412\n",
      "best loss:  0.013411539920349425\n",
      "Epoch 171 of 500, Train Loss: 0.013410\n",
      "best loss:  0.013409662154970633\n",
      "Epoch 172 of 500, Train Loss: 0.013408\n",
      "best loss:  0.013407967664985448\n",
      "Epoch 173 of 500, Train Loss: 0.013406\n",
      "best loss:  0.013406316568811475\n",
      "Epoch 174 of 500, Train Loss: 0.013405\n",
      "best loss:  0.013404935499330145\n",
      "Epoch 175 of 500, Train Loss: 0.013405\n",
      "best loss:  0.013404536757450335\n",
      "Epoch 176 of 500, Train Loss: 0.013403\n",
      "best loss:  0.013403350418991258\n",
      "Epoch 177 of 500, Train Loss: 0.013403\n",
      "Epoch 178 of 500, Train Loss: 0.013401\n",
      "best loss:  0.013400644913826556\n",
      "Epoch 179 of 500, Train Loss: 0.013398\n",
      "best loss:  0.013398257301228712\n",
      "Epoch 180 of 500, Train Loss: 0.013396\n",
      "best loss:  0.013396183961042196\n",
      "Epoch 181 of 500, Train Loss: 0.013395\n",
      "best loss:  0.013394557585526445\n",
      "Epoch 182 of 500, Train Loss: 0.013393\n",
      "best loss:  0.01339290877231108\n",
      "Epoch 183 of 500, Train Loss: 0.013392\n",
      "best loss:  0.013392238569754545\n",
      "Epoch 184 of 500, Train Loss: 0.013392\n",
      "best loss:  0.013391562531541586\n",
      "Epoch 185 of 500, Train Loss: 0.013390\n",
      "best loss:  0.013390114274626785\n",
      "Epoch 186 of 500, Train Loss: 0.013388\n",
      "best loss:  0.013388234163795113\n",
      "Epoch 187 of 500, Train Loss: 0.013386\n",
      "best loss:  0.01338636474850833\n",
      "Epoch 188 of 500, Train Loss: 0.013385\n",
      "best loss:  0.013385145056608328\n",
      "Epoch 189 of 500, Train Loss: 0.013384\n",
      "best loss:  0.013384012068854645\n",
      "Epoch 190 of 500, Train Loss: 0.013383\n",
      "best loss:  0.013382771492587374\n",
      "Epoch 191 of 500, Train Loss: 0.013382\n",
      "best loss:  0.013381839831144902\n",
      "Epoch 192 of 500, Train Loss: 0.013381\n",
      "best loss:  0.013381268606203528\n",
      "Epoch 193 of 500, Train Loss: 0.013381\n",
      "best loss:  0.013380724009913357\n",
      "Epoch 194 of 500, Train Loss: 0.013380\n",
      "best loss:  0.01337990744554712\n",
      "Epoch 195 of 500, Train Loss: 0.013381\n",
      "Epoch 196 of 500, Train Loss: 0.013380\n",
      "best loss:  0.013379589439272274\n",
      "Epoch 197 of 500, Train Loss: 0.013378\n",
      "best loss:  0.013378442477393362\n",
      "Epoch 198 of 500, Train Loss: 0.013378\n",
      "best loss:  0.013378102220676103\n",
      "Epoch 199 of 500, Train Loss: 0.013376\n",
      "best loss:  0.013376001612456558\n",
      "Epoch 200 of 500, Train Loss: 0.013376\n",
      "Epoch 201 of 500, Train Loss: 0.013375\n",
      "best loss:  0.013374530467748022\n",
      "Epoch 202 of 500, Train Loss: 0.013373\n",
      "best loss:  0.01337344764349188\n",
      "Epoch 203 of 500, Train Loss: 0.013371\n",
      "best loss:  0.013370567943779896\n",
      "Epoch 204 of 500, Train Loss: 0.013370\n",
      "best loss:  0.013369936455183756\n",
      "Epoch 205 of 500, Train Loss: 0.013369\n",
      "best loss:  0.01336902880395019\n",
      "Epoch 206 of 500, Train Loss: 0.013368\n",
      "best loss:  0.013367617856566759\n",
      "Epoch 207 of 500, Train Loss: 0.013369\n",
      "Epoch 208 of 500, Train Loss: 0.013367\n",
      "best loss:  0.013367440875271306\n",
      "Epoch 209 of 500, Train Loss: 0.013366\n",
      "best loss:  0.013366310912736714\n",
      "Epoch 210 of 500, Train Loss: 0.013370\n",
      "Epoch 211 of 500, Train Loss: 0.013371\n",
      "Epoch 212 of 500, Train Loss: 0.013372\n",
      "Epoch 213 of 500, Train Loss: 0.013371\n",
      "Epoch 214 of 500, Train Loss: 0.013374\n",
      "Epoch 215 of 500, Train Loss: 0.013375\n",
      "Epoch 216 of 500, Train Loss: 0.013377\n",
      "Epoch 217 of 500, Train Loss: 0.013378\n",
      "Epoch 218 of 500, Train Loss: 0.013386\n",
      "Epoch 219 of 500, Train Loss: 0.013394\n",
      "Epoch 220 of 500, Train Loss: 0.013411\n",
      "Epoch 221 of 500, Train Loss: 0.013429\n",
      "Epoch 222 of 500, Train Loss: 0.013454\n",
      "Epoch 223 of 500, Train Loss: 0.013481\n",
      "Epoch 224 of 500, Train Loss: 0.013495\n",
      "Epoch 225 of 500, Train Loss: 0.013512\n",
      "Epoch 226 of 500, Train Loss: 0.013504\n",
      "Epoch 227 of 500, Train Loss: 0.013514\n",
      "Epoch 228 of 500, Train Loss: 0.013486\n",
      "Epoch 229 of 500, Train Loss: 0.013486\n",
      "Epoch 230 of 500, Train Loss: 0.013461\n",
      "Epoch 231 of 500, Train Loss: 0.013496\n",
      "Epoch 232 of 500, Train Loss: 0.013490\n",
      "Epoch 233 of 500, Train Loss: 0.013597\n",
      "Epoch 234 of 500, Train Loss: 0.013580\n",
      "Epoch 235 of 500, Train Loss: 0.013699\n",
      "Epoch 236 of 500, Train Loss: 0.013625\n",
      "Epoch 237 of 500, Train Loss: 0.013657\n",
      "Epoch 238 of 500, Train Loss: 0.013581\n",
      "Epoch 239 of 500, Train Loss: 0.013585\n",
      "Epoch 240 of 500, Train Loss: 0.013520\n",
      "Epoch 241 of 500, Train Loss: 0.013523\n",
      "Epoch 242 of 500, Train Loss: 0.013471\n",
      "Epoch 243 of 500, Train Loss: 0.013475\n",
      "Epoch 244 of 500, Train Loss: 0.013436\n",
      "Epoch 245 of 500, Train Loss: 0.013444\n",
      "Epoch 246 of 500, Train Loss: 0.013418\n",
      "Epoch 247 of 500, Train Loss: 0.013432\n",
      "Epoch 248 of 500, Train Loss: 0.013411\n",
      "Epoch 249 of 500, Train Loss: 0.013426\n",
      "Epoch 250 of 500, Train Loss: 0.013406\n",
      "Epoch 251 of 500, Train Loss: 0.013422\n",
      "Epoch 252 of 500, Train Loss: 0.013402\n",
      "Epoch 253 of 500, Train Loss: 0.013421\n",
      "Epoch 254 of 500, Train Loss: 0.013406\n",
      "Epoch 255 of 500, Train Loss: 0.013435\n",
      "Epoch 256 of 500, Train Loss: 0.013422\n",
      "Epoch 257 of 500, Train Loss: 0.013458\n",
      "Epoch 258 of 500, Train Loss: 0.013441\n",
      "Epoch 259 of 500, Train Loss: 0.013478\n",
      "Epoch 260 of 500, Train Loss: 0.013455\n",
      "Epoch 261 of 500, Train Loss: 0.013485\n",
      "Epoch 262 of 500, Train Loss: 0.013457\n",
      "Epoch 263 of 500, Train Loss: 0.013480\n",
      "Epoch 264 of 500, Train Loss: 0.013448\n",
      "Epoch 265 of 500, Train Loss: 0.013465\n",
      "Epoch 266 of 500, Train Loss: 0.013434\n",
      "Epoch 267 of 500, Train Loss: 0.013450\n",
      "Epoch 268 of 500, Train Loss: 0.013422\n",
      "Epoch 269 of 500, Train Loss: 0.013438\n",
      "Epoch 270 of 500, Train Loss: 0.013414\n",
      "Epoch 271 of 500, Train Loss: 0.013430\n",
      "Epoch 272 of 500, Train Loss: 0.013408\n",
      "Epoch 273 of 500, Train Loss: 0.013426\n",
      "Epoch 274 of 500, Train Loss: 0.013407\n",
      "Epoch 275 of 500, Train Loss: 0.013426\n",
      "Epoch 276 of 500, Train Loss: 0.013408\n",
      "Epoch 277 of 500, Train Loss: 0.013425\n",
      "Epoch 278 of 500, Train Loss: 0.013405\n",
      "Epoch 279 of 500, Train Loss: 0.013423\n",
      "Epoch 280 of 500, Train Loss: 0.013405\n",
      "Epoch 281 of 500, Train Loss: 0.013425\n",
      "Epoch 282 of 500, Train Loss: 0.013406\n",
      "Epoch 283 of 500, Train Loss: 0.013428\n",
      "Epoch 284 of 500, Train Loss: 0.013410\n",
      "Epoch 285 of 500, Train Loss: 0.013432\n",
      "Epoch 286 of 500, Train Loss: 0.013413\n",
      "Epoch 287 of 500, Train Loss: 0.013433\n",
      "Epoch 288 of 500, Train Loss: 0.013413\n",
      "Epoch 289 of 500, Train Loss: 0.013431\n",
      "Epoch 290 of 500, Train Loss: 0.013410\n",
      "Epoch 291 of 500, Train Loss: 0.013427\n",
      "Epoch 292 of 500, Train Loss: 0.013407\n",
      "Epoch 293 of 500, Train Loss: 0.013423\n",
      "Epoch 294 of 500, Train Loss: 0.013403\n",
      "Epoch 295 of 500, Train Loss: 0.013417\n",
      "Epoch 296 of 500, Train Loss: 0.013397\n",
      "Epoch 297 of 500, Train Loss: 0.013412\n",
      "Epoch 298 of 500, Train Loss: 0.013392\n",
      "Epoch 299 of 500, Train Loss: 0.013406\n",
      "Epoch 300 of 500, Train Loss: 0.013389\n",
      "Epoch 301 of 500, Train Loss: 0.013403\n",
      "Epoch 302 of 500, Train Loss: 0.013387\n",
      "Epoch 303 of 500, Train Loss: 0.013401\n",
      "Epoch 304 of 500, Train Loss: 0.013385\n",
      "Epoch 305 of 500, Train Loss: 0.013399\n",
      "Epoch 306 of 500, Train Loss: 0.013385\n",
      "Epoch 307 of 500, Train Loss: 0.013401\n",
      "Epoch 308 of 500, Train Loss: 0.013389\n",
      "Epoch 309 of 500, Train Loss: 0.013405\n",
      "Epoch 310 of 500, Train Loss: 0.013391\n",
      "Epoch 311 of 500, Train Loss: 0.013406\n",
      "Epoch 312 of 500, Train Loss: 0.013390\n",
      "Epoch 313 of 500, Train Loss: 0.013405\n",
      "Epoch 314 of 500, Train Loss: 0.013391\n",
      "Epoch 315 of 500, Train Loss: 0.013408\n",
      "Epoch 316 of 500, Train Loss: 0.013393\n",
      "Epoch 317 of 500, Train Loss: 0.013409\n",
      "Epoch 318 of 500, Train Loss: 0.013392\n",
      "Epoch 319 of 500, Train Loss: 0.013406\n",
      "Epoch 320 of 500, Train Loss: 0.013389\n",
      "Epoch 321 of 500, Train Loss: 0.013402\n",
      "Epoch 322 of 500, Train Loss: 0.013386\n",
      "Epoch 323 of 500, Train Loss: 0.013400\n",
      "Epoch 324 of 500, Train Loss: 0.013385\n",
      "Epoch 325 of 500, Train Loss: 0.013398\n",
      "Epoch 326 of 500, Train Loss: 0.013382\n",
      "Epoch 327 of 500, Train Loss: 0.013393\n",
      "Epoch 328 of 500, Train Loss: 0.013379\n",
      "Epoch 329 of 500, Train Loss: 0.013391\n",
      "Epoch 330 of 500, Train Loss: 0.013378\n",
      "Epoch 331 of 500, Train Loss: 0.013390\n",
      "Epoch 332 of 500, Train Loss: 0.013379\n",
      "Epoch 333 of 500, Train Loss: 0.013390\n",
      "Epoch 334 of 500, Train Loss: 0.013379\n",
      "Epoch 335 of 500, Train Loss: 0.013390\n",
      "Epoch 336 of 500, Train Loss: 0.013379\n",
      "Epoch 337 of 500, Train Loss: 0.013390\n",
      "Epoch 338 of 500, Train Loss: 0.013379\n",
      "Epoch 339 of 500, Train Loss: 0.013390\n",
      "Epoch 340 of 500, Train Loss: 0.013380\n",
      "Epoch 341 of 500, Train Loss: 0.013392\n",
      "Epoch 342 of 500, Train Loss: 0.013382\n",
      "Epoch 343 of 500, Train Loss: 0.013394\n",
      "Epoch 344 of 500, Train Loss: 0.013382\n",
      "Epoch 345 of 500, Train Loss: 0.013395\n",
      "Epoch 346 of 500, Train Loss: 0.013383\n",
      "Epoch 347 of 500, Train Loss: 0.013397\n",
      "Epoch 348 of 500, Train Loss: 0.013386\n",
      "Epoch 349 of 500, Train Loss: 0.013398\n",
      "Epoch 350 of 500, Train Loss: 0.013384\n",
      "Epoch 351 of 500, Train Loss: 0.013394\n",
      "Epoch 352 of 500, Train Loss: 0.013382\n",
      "Epoch 353 of 500, Train Loss: 0.013393\n",
      "Epoch 354 of 500, Train Loss: 0.013382\n",
      "Epoch 355 of 500, Train Loss: 0.013392\n",
      "Epoch 356 of 500, Train Loss: 0.013379\n",
      "Epoch 357 of 500, Train Loss: 0.013388\n",
      "Epoch 358 of 500, Train Loss: 0.013375\n",
      "Epoch 359 of 500, Train Loss: 0.013384\n",
      "Epoch 360 of 500, Train Loss: 0.013375\n",
      "Epoch 361 of 500, Train Loss: 0.013386\n",
      "Epoch 362 of 500, Train Loss: 0.013377\n",
      "Epoch 363 of 500, Train Loss: 0.013388\n",
      "Epoch 364 of 500, Train Loss: 0.013377\n",
      "Epoch 365 of 500, Train Loss: 0.013389\n",
      "Epoch 366 of 500, Train Loss: 0.013377\n",
      "Epoch 367 of 500, Train Loss: 0.013389\n",
      "Epoch 368 of 500, Train Loss: 0.013379\n",
      "Epoch 369 of 500, Train Loss: 0.013393\n",
      "Epoch 370 of 500, Train Loss: 0.013381\n",
      "Epoch 371 of 500, Train Loss: 0.013394\n",
      "Epoch 372 of 500, Train Loss: 0.013381\n",
      "Epoch 373 of 500, Train Loss: 0.013395\n",
      "Epoch 374 of 500, Train Loss: 0.013383\n",
      "Epoch 375 of 500, Train Loss: 0.013398\n",
      "Epoch 376 of 500, Train Loss: 0.013387\n",
      "Epoch 377 of 500, Train Loss: 0.013402\n",
      "Epoch 378 of 500, Train Loss: 0.013389\n",
      "Epoch 379 of 500, Train Loss: 0.013403\n",
      "Epoch 380 of 500, Train Loss: 0.013388\n",
      "Epoch 381 of 500, Train Loss: 0.013402\n",
      "Epoch 382 of 500, Train Loss: 0.013387\n",
      "Epoch 383 of 500, Train Loss: 0.013399\n",
      "Epoch 384 of 500, Train Loss: 0.013384\n",
      "Epoch 385 of 500, Train Loss: 0.013398\n",
      "Epoch 386 of 500, Train Loss: 0.013383\n",
      "Epoch 387 of 500, Train Loss: 0.013395\n",
      "Epoch 388 of 500, Train Loss: 0.013380\n",
      "Epoch 389 of 500, Train Loss: 0.013390\n",
      "Epoch 390 of 500, Train Loss: 0.013376\n",
      "Epoch 391 of 500, Train Loss: 0.013387\n",
      "Epoch 392 of 500, Train Loss: 0.013373\n",
      "Epoch 393 of 500, Train Loss: 0.013385\n",
      "Epoch 394 of 500, Train Loss: 0.013372\n",
      "Epoch 395 of 500, Train Loss: 0.013386\n",
      "Epoch 396 of 500, Train Loss: 0.013374\n",
      "Epoch 397 of 500, Train Loss: 0.013390\n",
      "Epoch 398 of 500, Train Loss: 0.013379\n",
      "Epoch 399 of 500, Train Loss: 0.013396\n",
      "Epoch 400 of 500, Train Loss: 0.013385\n",
      "Epoch 401 of 500, Train Loss: 0.013400\n",
      "Epoch 402 of 500, Train Loss: 0.013387\n",
      "Epoch 403 of 500, Train Loss: 0.013404\n",
      "Epoch 404 of 500, Train Loss: 0.013390\n",
      "Epoch 405 of 500, Train Loss: 0.013404\n",
      "Epoch 406 of 500, Train Loss: 0.013388\n",
      "Epoch 407 of 500, Train Loss: 0.013400\n",
      "Epoch 408 of 500, Train Loss: 0.013384\n",
      "Epoch 409 of 500, Train Loss: 0.013396\n",
      "Epoch 410 of 500, Train Loss: 0.013381\n",
      "Epoch 411 of 500, Train Loss: 0.013394\n",
      "Epoch 412 of 500, Train Loss: 0.013380\n",
      "Epoch 413 of 500, Train Loss: 0.013395\n",
      "Epoch 414 of 500, Train Loss: 0.013379\n",
      "Epoch 415 of 500, Train Loss: 0.013395\n",
      "Epoch 416 of 500, Train Loss: 0.013380\n",
      "Epoch 417 of 500, Train Loss: 0.013395\n",
      "Epoch 418 of 500, Train Loss: 0.013379\n",
      "Epoch 419 of 500, Train Loss: 0.013393\n",
      "Epoch 420 of 500, Train Loss: 0.013377\n",
      "Epoch 421 of 500, Train Loss: 0.013392\n",
      "Epoch 422 of 500, Train Loss: 0.013379\n",
      "Epoch 423 of 500, Train Loss: 0.013392\n",
      "Epoch 424 of 500, Train Loss: 0.013379\n",
      "Epoch 425 of 500, Train Loss: 0.013390\n",
      "Epoch 426 of 500, Train Loss: 0.013377\n",
      "Epoch 427 of 500, Train Loss: 0.013389\n",
      "Epoch 428 of 500, Train Loss: 0.013376\n",
      "Epoch 429 of 500, Train Loss: 0.013390\n",
      "Epoch 430 of 500, Train Loss: 0.013378\n",
      "Epoch 431 of 500, Train Loss: 0.013394\n",
      "Epoch 432 of 500, Train Loss: 0.013384\n",
      "Epoch 433 of 500, Train Loss: 0.013400\n",
      "Epoch 434 of 500, Train Loss: 0.013386\n",
      "Epoch 435 of 500, Train Loss: 0.013400\n",
      "Epoch 436 of 500, Train Loss: 0.013386\n",
      "Epoch 437 of 500, Train Loss: 0.013400\n",
      "Epoch 438 of 500, Train Loss: 0.013387\n",
      "Epoch 439 of 500, Train Loss: 0.013403\n",
      "Epoch 440 of 500, Train Loss: 0.013389\n",
      "Epoch 441 of 500, Train Loss: 0.013403\n",
      "Epoch 442 of 500, Train Loss: 0.013385\n",
      "Epoch 443 of 500, Train Loss: 0.013396\n",
      "Epoch 444 of 500, Train Loss: 0.013380\n",
      "Epoch 445 of 500, Train Loss: 0.013392\n",
      "Epoch 446 of 500, Train Loss: 0.013377\n",
      "Epoch 447 of 500, Train Loss: 0.013389\n",
      "Epoch 448 of 500, Train Loss: 0.013376\n",
      "Epoch 449 of 500, Train Loss: 0.013388\n",
      "Epoch 450 of 500, Train Loss: 0.013377\n",
      "Epoch 451 of 500, Train Loss: 0.013391\n",
      "Epoch 452 of 500, Train Loss: 0.013380\n",
      "Epoch 453 of 500, Train Loss: 0.013394\n",
      "Epoch 454 of 500, Train Loss: 0.013381\n",
      "Epoch 455 of 500, Train Loss: 0.013394\n",
      "Epoch 456 of 500, Train Loss: 0.013381\n",
      "Epoch 457 of 500, Train Loss: 0.013396\n",
      "Epoch 458 of 500, Train Loss: 0.013385\n",
      "Epoch 459 of 500, Train Loss: 0.013399\n",
      "Epoch 460 of 500, Train Loss: 0.013388\n",
      "Epoch 461 of 500, Train Loss: 0.013399\n",
      "Epoch 462 of 500, Train Loss: 0.013387\n",
      "Epoch 463 of 500, Train Loss: 0.013399\n",
      "Epoch 464 of 500, Train Loss: 0.013385\n",
      "Epoch 465 of 500, Train Loss: 0.013397\n",
      "Epoch 466 of 500, Train Loss: 0.013385\n",
      "Epoch 467 of 500, Train Loss: 0.013396\n",
      "Epoch 468 of 500, Train Loss: 0.013383\n",
      "Epoch 469 of 500, Train Loss: 0.013394\n",
      "Epoch 470 of 500, Train Loss: 0.013382\n",
      "Epoch 471 of 500, Train Loss: 0.013394\n",
      "Epoch 472 of 500, Train Loss: 0.013382\n",
      "Epoch 473 of 500, Train Loss: 0.013392\n",
      "Epoch 474 of 500, Train Loss: 0.013381\n",
      "Epoch 475 of 500, Train Loss: 0.013392\n",
      "Epoch 476 of 500, Train Loss: 0.013380\n",
      "Epoch 477 of 500, Train Loss: 0.013391\n",
      "Epoch 478 of 500, Train Loss: 0.013379\n",
      "Epoch 479 of 500, Train Loss: 0.013390\n",
      "Epoch 480 of 500, Train Loss: 0.013377\n",
      "Epoch 481 of 500, Train Loss: 0.013389\n",
      "Epoch 482 of 500, Train Loss: 0.013378\n",
      "Epoch 483 of 500, Train Loss: 0.013389\n",
      "Epoch 484 of 500, Train Loss: 0.013378\n",
      "Epoch 485 of 500, Train Loss: 0.013391\n",
      "Epoch 486 of 500, Train Loss: 0.013379\n",
      "Epoch 487 of 500, Train Loss: 0.013393\n",
      "Epoch 488 of 500, Train Loss: 0.013381\n",
      "Epoch 489 of 500, Train Loss: 0.013394\n",
      "Epoch 490 of 500, Train Loss: 0.013383\n",
      "Epoch 491 of 500, Train Loss: 0.013395\n",
      "Epoch 492 of 500, Train Loss: 0.013384\n",
      "Epoch 493 of 500, Train Loss: 0.013396\n",
      "Epoch 494 of 500, Train Loss: 0.013383\n",
      "Epoch 495 of 500, Train Loss: 0.013397\n",
      "Epoch 496 of 500, Train Loss: 0.013382\n",
      "Epoch 497 of 500, Train Loss: 0.013396\n",
      "Epoch 498 of 500, Train Loss: 0.013381\n",
      "Epoch 499 of 500, Train Loss: 0.013393\n",
      "Epoch 500 of 500, Train Loss: 0.013380\n",
      "latent train shape:  (16395, 30)\n",
      "M: 30, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 87\n",
      "Training the subspace: 0 / 30\n",
      "Training the subspace: 1 / 30\n",
      "Training the subspace: 2 / 30\n",
      "Training the subspace: 3 / 30\n",
      "Training the subspace: 4 / 30\n",
      "Training the subspace: 5 / 30\n",
      "Training the subspace: 6 / 30\n",
      "Training the subspace: 7 / 30\n",
      "Training the subspace: 8 / 30\n",
      "Training the subspace: 9 / 30\n",
      "Training the subspace: 10 / 30\n",
      "Training the subspace: 11 / 30\n",
      "Training the subspace: 12 / 30\n",
      "Training the subspace: 13 / 30\n",
      "Training the subspace: 14 / 30\n",
      "Training the subspace: 15 / 30\n",
      "Training the subspace: 16 / 30\n",
      "Training the subspace: 17 / 30\n",
      "Training the subspace: 18 / 30\n",
      "Training the subspace: 19 / 30\n",
      "Training the subspace: 20 / 30\n",
      "Training the subspace: 21 / 30\n",
      "Training the subspace: 22 / 30\n",
      "Training the subspace: 23 / 30\n",
      "Training the subspace: 24 / 30\n",
      "Training the subspace: 25 / 30\n",
      "Training the subspace: 26 / 30\n",
      "Training the subspace: 27 / 30\n",
      "Training the subspace: 28 / 30\n",
      "Training the subspace: 29 / 30\n",
      "Encoding the subspace: 0 / 30\n",
      "Encoding the subspace: 1 / 30\n",
      "Encoding the subspace: 2 / 30\n",
      "Encoding the subspace: 3 / 30\n",
      "Encoding the subspace: 4 / 30\n",
      "Encoding the subspace: 5 / 30\n",
      "Encoding the subspace: 6 / 30\n",
      "Encoding the subspace: 7 / 30\n",
      "Encoding the subspace: 8 / 30\n",
      "Encoding the subspace: 9 / 30\n",
      "Encoding the subspace: 10 / 30\n",
      "Encoding the subspace: 11 / 30\n",
      "Encoding the subspace: 12 / 30\n",
      "Encoding the subspace: 13 / 30\n",
      "Encoding the subspace: 14 / 30\n",
      "Encoding the subspace: 15 / 30\n",
      "Encoding the subspace: 16 / 30\n",
      "Encoding the subspace: 17 / 30\n",
      "Encoding the subspace: 18 / 30\n",
      "Encoding the subspace: 19 / 30\n",
      "Encoding the subspace: 20 / 30\n",
      "Encoding the subspace: 21 / 30\n",
      "Encoding the subspace: 22 / 30\n",
      "Encoding the subspace: 23 / 30\n",
      "Encoding the subspace: 24 / 30\n",
      "Encoding the subspace: 25 / 30\n",
      "Encoding the subspace: 26 / 30\n",
      "Encoding the subspace: 27 / 30\n",
      "Encoding the subspace: 28 / 30\n",
      "Encoding the subspace: 29 / 30\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=45, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.432652\n",
      "best loss:  0.432652405253792\n",
      "Epoch 2 of 500, Train Loss: 0.056529\n",
      "best loss:  0.05652899948280853\n",
      "Epoch 3 of 500, Train Loss: 0.053486\n",
      "best loss:  0.053486240946944055\n",
      "Epoch 4 of 500, Train Loss: 0.049720\n",
      "best loss:  0.04972047247705519\n",
      "Epoch 5 of 500, Train Loss: 0.044447\n",
      "best loss:  0.04444708710025577\n",
      "Epoch 6 of 500, Train Loss: 0.038992\n",
      "best loss:  0.03899156845801026\n",
      "Epoch 7 of 500, Train Loss: 0.034521\n",
      "best loss:  0.03452063182796633\n",
      "Epoch 8 of 500, Train Loss: 0.031199\n",
      "best loss:  0.031198605802993096\n",
      "Epoch 9 of 500, Train Loss: 0.028827\n",
      "best loss:  0.028826578258529537\n",
      "Epoch 10 of 500, Train Loss: 0.027103\n",
      "best loss:  0.02710343775963916\n",
      "Epoch 11 of 500, Train Loss: 0.025742\n",
      "best loss:  0.025741645549658628\n",
      "Epoch 12 of 500, Train Loss: 0.024569\n",
      "best loss:  0.02456856039334666\n",
      "Epoch 13 of 500, Train Loss: 0.023512\n",
      "best loss:  0.023511880502021736\n",
      "Epoch 14 of 500, Train Loss: 0.022548\n",
      "best loss:  0.022548434271995165\n",
      "Epoch 15 of 500, Train Loss: 0.021679\n",
      "best loss:  0.021678805819625946\n",
      "Epoch 16 of 500, Train Loss: 0.020909\n",
      "best loss:  0.020909346412242815\n",
      "Epoch 17 of 500, Train Loss: 0.020231\n",
      "best loss:  0.02023076438281844\n",
      "Epoch 18 of 500, Train Loss: 0.019628\n",
      "best loss:  0.019628471086187968\n",
      "Epoch 19 of 500, Train Loss: 0.019087\n",
      "best loss:  0.01908669432750614\n",
      "Epoch 20 of 500, Train Loss: 0.018595\n",
      "best loss:  0.01859541247190922\n",
      "Epoch 21 of 500, Train Loss: 0.018141\n",
      "best loss:  0.018140522826529764\n",
      "Epoch 22 of 500, Train Loss: 0.017719\n",
      "best loss:  0.017718621025849205\n",
      "Epoch 23 of 500, Train Loss: 0.017327\n",
      "best loss:  0.017327472607942672\n",
      "Epoch 24 of 500, Train Loss: 0.016959\n",
      "best loss:  0.016958638166786756\n",
      "Epoch 25 of 500, Train Loss: 0.016609\n",
      "best loss:  0.016609051913454364\n",
      "Epoch 26 of 500, Train Loss: 0.016286\n",
      "best loss:  0.016285991298653865\n",
      "Epoch 27 of 500, Train Loss: 0.015984\n",
      "best loss:  0.015983635817777165\n",
      "Epoch 28 of 500, Train Loss: 0.015700\n",
      "best loss:  0.015699729640370724\n",
      "Epoch 29 of 500, Train Loss: 0.015445\n",
      "best loss:  0.01544523808081786\n",
      "Epoch 30 of 500, Train Loss: 0.015202\n",
      "best loss:  0.015202103354589104\n",
      "Epoch 31 of 500, Train Loss: 0.014987\n",
      "best loss:  0.014986881290475816\n",
      "Epoch 32 of 500, Train Loss: 0.014778\n",
      "best loss:  0.014777786436837253\n",
      "Epoch 33 of 500, Train Loss: 0.014588\n",
      "best loss:  0.014588175015930129\n",
      "Epoch 34 of 500, Train Loss: 0.014412\n",
      "best loss:  0.014412133909794393\n",
      "Epoch 35 of 500, Train Loss: 0.014256\n",
      "best loss:  0.014256341027749284\n",
      "Epoch 36 of 500, Train Loss: 0.014106\n",
      "best loss:  0.01410586884278667\n",
      "Epoch 37 of 500, Train Loss: 0.013972\n",
      "best loss:  0.013972343085739804\n",
      "Epoch 38 of 500, Train Loss: 0.013849\n",
      "best loss:  0.013849241927296804\n",
      "Epoch 39 of 500, Train Loss: 0.013742\n",
      "best loss:  0.013742162504002394\n",
      "Epoch 40 of 500, Train Loss: 0.013643\n",
      "best loss:  0.01364312039114275\n",
      "Epoch 41 of 500, Train Loss: 0.013548\n",
      "best loss:  0.013547802460379899\n",
      "Epoch 42 of 500, Train Loss: 0.013462\n",
      "best loss:  0.013461558437723911\n",
      "Epoch 43 of 500, Train Loss: 0.013379\n",
      "best loss:  0.013379349490598769\n",
      "Epoch 44 of 500, Train Loss: 0.013309\n",
      "best loss:  0.013309411149762584\n",
      "Epoch 45 of 500, Train Loss: 0.013239\n",
      "best loss:  0.013238966841992467\n",
      "Epoch 46 of 500, Train Loss: 0.013170\n",
      "best loss:  0.013170272252222797\n",
      "Epoch 47 of 500, Train Loss: 0.013107\n",
      "best loss:  0.013107311562635005\n",
      "Epoch 48 of 500, Train Loss: 0.013047\n",
      "best loss:  0.013046868535096201\n",
      "Epoch 49 of 500, Train Loss: 0.012999\n",
      "best loss:  0.012998681142971338\n",
      "Epoch 50 of 500, Train Loss: 0.012942\n",
      "best loss:  0.012941971045599297\n",
      "Epoch 51 of 500, Train Loss: 0.012888\n",
      "best loss:  0.012887807692448038\n",
      "Epoch 52 of 500, Train Loss: 0.012842\n",
      "best loss:  0.012842120282133579\n",
      "Epoch 53 of 500, Train Loss: 0.012799\n",
      "best loss:  0.012799016201585287\n",
      "Epoch 54 of 500, Train Loss: 0.012751\n",
      "best loss:  0.012750718573175097\n",
      "Epoch 55 of 500, Train Loss: 0.012704\n",
      "best loss:  0.012703796400816736\n",
      "Epoch 56 of 500, Train Loss: 0.012668\n",
      "best loss:  0.012667509772979054\n",
      "Epoch 57 of 500, Train Loss: 0.012626\n",
      "best loss:  0.01262623375162569\n",
      "Epoch 58 of 500, Train Loss: 0.012585\n",
      "best loss:  0.01258502549933118\n",
      "Epoch 59 of 500, Train Loss: 0.012550\n",
      "best loss:  0.012550126310201477\n",
      "Epoch 60 of 500, Train Loss: 0.012516\n",
      "best loss:  0.01251556607195205\n",
      "Epoch 61 of 500, Train Loss: 0.012487\n",
      "best loss:  0.01248664932046627\n",
      "Epoch 62 of 500, Train Loss: 0.012452\n",
      "best loss:  0.01245203299033672\n",
      "Epoch 63 of 500, Train Loss: 0.012427\n",
      "best loss:  0.012427450949145668\n",
      "Epoch 64 of 500, Train Loss: 0.012399\n",
      "best loss:  0.012399439935260199\n",
      "Epoch 65 of 500, Train Loss: 0.012366\n",
      "best loss:  0.012365500330780597\n",
      "Epoch 66 of 500, Train Loss: 0.012339\n",
      "best loss:  0.012339381797531577\n",
      "Epoch 67 of 500, Train Loss: 0.012310\n",
      "best loss:  0.012310044095461626\n",
      "Epoch 68 of 500, Train Loss: 0.012284\n",
      "best loss:  0.012283566741989348\n",
      "Epoch 69 of 500, Train Loss: 0.012251\n",
      "best loss:  0.012250647885551634\n",
      "Epoch 70 of 500, Train Loss: 0.012223\n",
      "best loss:  0.012223320664583467\n",
      "Epoch 71 of 500, Train Loss: 0.012196\n",
      "best loss:  0.01219597470526071\n",
      "Epoch 72 of 500, Train Loss: 0.012169\n",
      "best loss:  0.012169260155118988\n",
      "Epoch 73 of 500, Train Loss: 0.012144\n",
      "best loss:  0.012143870666505798\n",
      "Epoch 74 of 500, Train Loss: 0.012118\n",
      "best loss:  0.012118040892212514\n",
      "Epoch 75 of 500, Train Loss: 0.012090\n",
      "best loss:  0.012089940030741425\n",
      "Epoch 76 of 500, Train Loss: 0.012068\n",
      "best loss:  0.012068414868644345\n",
      "Epoch 77 of 500, Train Loss: 0.012048\n",
      "best loss:  0.012048126141723\n",
      "Epoch 78 of 500, Train Loss: 0.012027\n",
      "best loss:  0.012026988373042351\n",
      "Epoch 79 of 500, Train Loss: 0.012009\n",
      "best loss:  0.012008961221648113\n",
      "Epoch 80 of 500, Train Loss: 0.011994\n",
      "best loss:  0.01199358841652491\n",
      "Epoch 81 of 500, Train Loss: 0.011979\n",
      "best loss:  0.011978534422431308\n",
      "Epoch 82 of 500, Train Loss: 0.011966\n",
      "best loss:  0.011965592991237622\n",
      "Epoch 83 of 500, Train Loss: 0.011956\n",
      "best loss:  0.011955678190895205\n",
      "Epoch 84 of 500, Train Loss: 0.011944\n",
      "best loss:  0.011944412636771878\n",
      "Epoch 85 of 500, Train Loss: 0.011935\n",
      "best loss:  0.011935265423899675\n",
      "Epoch 86 of 500, Train Loss: 0.011930\n",
      "best loss:  0.0119301396769078\n",
      "Epoch 87 of 500, Train Loss: 0.011921\n",
      "best loss:  0.011921454646625527\n",
      "Epoch 88 of 500, Train Loss: 0.011915\n",
      "best loss:  0.01191529844106489\n",
      "Epoch 89 of 500, Train Loss: 0.011907\n",
      "best loss:  0.011906943257353015\n",
      "Epoch 90 of 500, Train Loss: 0.011896\n",
      "best loss:  0.011896034229890617\n",
      "Epoch 91 of 500, Train Loss: 0.011882\n",
      "best loss:  0.011881722216972677\n",
      "Epoch 92 of 500, Train Loss: 0.011874\n",
      "best loss:  0.01187365366853936\n",
      "Epoch 93 of 500, Train Loss: 0.011864\n",
      "best loss:  0.011864219556987654\n",
      "Epoch 94 of 500, Train Loss: 0.011855\n",
      "best loss:  0.011855087262761506\n",
      "Epoch 95 of 500, Train Loss: 0.011848\n",
      "best loss:  0.011848275633749928\n",
      "Epoch 96 of 500, Train Loss: 0.011843\n",
      "best loss:  0.011843037185107553\n",
      "Epoch 97 of 500, Train Loss: 0.011840\n",
      "best loss:  0.011840209301945558\n",
      "Epoch 98 of 500, Train Loss: 0.011838\n",
      "best loss:  0.011837960294535377\n",
      "Epoch 99 of 500, Train Loss: 0.011836\n",
      "best loss:  0.011836313066689131\n",
      "Epoch 100 of 500, Train Loss: 0.011832\n",
      "best loss:  0.011831658631301478\n",
      "Epoch 101 of 500, Train Loss: 0.011826\n",
      "best loss:  0.011826183750021611\n",
      "Epoch 102 of 500, Train Loss: 0.011821\n",
      "best loss:  0.011821411397419112\n",
      "Epoch 103 of 500, Train Loss: 0.011817\n",
      "best loss:  0.011817029772539327\n",
      "Epoch 104 of 500, Train Loss: 0.011809\n",
      "best loss:  0.011808888997874483\n",
      "Epoch 105 of 500, Train Loss: 0.011803\n",
      "best loss:  0.011802635324713703\n",
      "Epoch 106 of 500, Train Loss: 0.011796\n",
      "best loss:  0.011796243060917175\n",
      "Epoch 107 of 500, Train Loss: 0.011793\n",
      "best loss:  0.011792603248568765\n",
      "Epoch 108 of 500, Train Loss: 0.011785\n",
      "best loss:  0.011785060011302554\n",
      "Epoch 109 of 500, Train Loss: 0.011782\n",
      "best loss:  0.011781960739208161\n",
      "Epoch 110 of 500, Train Loss: 0.011779\n",
      "best loss:  0.011779176234515824\n",
      "Epoch 111 of 500, Train Loss: 0.011780\n",
      "Epoch 112 of 500, Train Loss: 0.011779\n",
      "best loss:  0.0117790830643557\n",
      "Epoch 113 of 500, Train Loss: 0.011780\n",
      "Epoch 114 of 500, Train Loss: 0.011778\n",
      "best loss:  0.01177840276874869\n",
      "Epoch 115 of 500, Train Loss: 0.011779\n",
      "Epoch 116 of 500, Train Loss: 0.011775\n",
      "best loss:  0.011775330306245592\n",
      "Epoch 117 of 500, Train Loss: 0.011774\n",
      "best loss:  0.011773883442479266\n",
      "Epoch 118 of 500, Train Loss: 0.011772\n",
      "best loss:  0.011771967339684985\n",
      "Epoch 119 of 500, Train Loss: 0.011772\n",
      "best loss:  0.011771793522472249\n",
      "Epoch 120 of 500, Train Loss: 0.011773\n",
      "Epoch 121 of 500, Train Loss: 0.011778\n",
      "Epoch 122 of 500, Train Loss: 0.011782\n",
      "Epoch 123 of 500, Train Loss: 0.011789\n",
      "Epoch 124 of 500, Train Loss: 0.011796\n",
      "Epoch 125 of 500, Train Loss: 0.011800\n",
      "Epoch 126 of 500, Train Loss: 0.011801\n",
      "Epoch 127 of 500, Train Loss: 0.011797\n",
      "Epoch 128 of 500, Train Loss: 0.011791\n",
      "Epoch 129 of 500, Train Loss: 0.011784\n",
      "Epoch 130 of 500, Train Loss: 0.011778\n",
      "Epoch 131 of 500, Train Loss: 0.011773\n",
      "Epoch 132 of 500, Train Loss: 0.011774\n",
      "Epoch 133 of 500, Train Loss: 0.011775\n",
      "Epoch 134 of 500, Train Loss: 0.011781\n",
      "Epoch 135 of 500, Train Loss: 0.011787\n",
      "Epoch 136 of 500, Train Loss: 0.011795\n",
      "Epoch 137 of 500, Train Loss: 0.011801\n",
      "Epoch 138 of 500, Train Loss: 0.011806\n",
      "Epoch 139 of 500, Train Loss: 0.011805\n",
      "Epoch 140 of 500, Train Loss: 0.011806\n",
      "Epoch 141 of 500, Train Loss: 0.011800\n",
      "Epoch 142 of 500, Train Loss: 0.011792\n",
      "Epoch 143 of 500, Train Loss: 0.011780\n",
      "Epoch 144 of 500, Train Loss: 0.011774\n",
      "Epoch 145 of 500, Train Loss: 0.011767\n",
      "best loss:  0.01176676142404881\n",
      "Epoch 146 of 500, Train Loss: 0.011762\n",
      "best loss:  0.01176186228331626\n",
      "Epoch 147 of 500, Train Loss: 0.011761\n",
      "best loss:  0.011760909611277116\n",
      "Epoch 148 of 500, Train Loss: 0.011759\n",
      "best loss:  0.011759187280206739\n",
      "Epoch 149 of 500, Train Loss: 0.011759\n",
      "Epoch 150 of 500, Train Loss: 0.011756\n",
      "best loss:  0.011755676446897664\n",
      "Epoch 151 of 500, Train Loss: 0.011754\n",
      "best loss:  0.011753620034536751\n",
      "Epoch 152 of 500, Train Loss: 0.011747\n",
      "best loss:  0.011747357166934463\n",
      "Epoch 153 of 500, Train Loss: 0.011745\n",
      "best loss:  0.011744629851945805\n",
      "Epoch 154 of 500, Train Loss: 0.011736\n",
      "best loss:  0.011736071703422186\n",
      "Epoch 155 of 500, Train Loss: 0.011732\n",
      "best loss:  0.011731958624211053\n",
      "Epoch 156 of 500, Train Loss: 0.011727\n",
      "best loss:  0.011727253072344618\n",
      "Epoch 157 of 500, Train Loss: 0.011723\n",
      "best loss:  0.011723193172487257\n",
      "Epoch 158 of 500, Train Loss: 0.011718\n",
      "best loss:  0.01171786000926405\n",
      "Epoch 159 of 500, Train Loss: 0.011713\n",
      "best loss:  0.011713132091442325\n",
      "Epoch 160 of 500, Train Loss: 0.011710\n",
      "best loss:  0.011710377340227575\n",
      "Epoch 161 of 500, Train Loss: 0.011707\n",
      "best loss:  0.011707260331266213\n",
      "Epoch 162 of 500, Train Loss: 0.011702\n",
      "best loss:  0.011702059241338424\n",
      "Epoch 163 of 500, Train Loss: 0.011699\n",
      "best loss:  0.011699338102453324\n",
      "Epoch 164 of 500, Train Loss: 0.011697\n",
      "best loss:  0.011697065550946746\n",
      "Epoch 165 of 500, Train Loss: 0.011695\n",
      "best loss:  0.011694543023820237\n",
      "Epoch 166 of 500, Train Loss: 0.011690\n",
      "best loss:  0.011690135735115072\n",
      "Epoch 167 of 500, Train Loss: 0.011688\n",
      "best loss:  0.011687889280945583\n",
      "Epoch 168 of 500, Train Loss: 0.011685\n",
      "best loss:  0.011685152251030636\n",
      "Epoch 169 of 500, Train Loss: 0.011683\n",
      "best loss:  0.011683141948274417\n",
      "Epoch 170 of 500, Train Loss: 0.011680\n",
      "best loss:  0.011679653044137562\n",
      "Epoch 171 of 500, Train Loss: 0.011679\n",
      "best loss:  0.011678700543901263\n",
      "Epoch 172 of 500, Train Loss: 0.011677\n",
      "best loss:  0.011676927907762384\n",
      "Epoch 173 of 500, Train Loss: 0.011677\n",
      "best loss:  0.011676735945439413\n",
      "Epoch 174 of 500, Train Loss: 0.011673\n",
      "best loss:  0.011672912257233483\n",
      "Epoch 175 of 500, Train Loss: 0.011673\n",
      "best loss:  0.011672827876204545\n",
      "Epoch 176 of 500, Train Loss: 0.011670\n",
      "best loss:  0.011669816183401885\n",
      "Epoch 177 of 500, Train Loss: 0.011672\n",
      "Epoch 178 of 500, Train Loss: 0.011669\n",
      "best loss:  0.011668513701369539\n",
      "Epoch 179 of 500, Train Loss: 0.011670\n",
      "Epoch 180 of 500, Train Loss: 0.011667\n",
      "best loss:  0.011666910560862992\n",
      "Epoch 181 of 500, Train Loss: 0.011669\n",
      "Epoch 182 of 500, Train Loss: 0.011665\n",
      "best loss:  0.011665379373034527\n",
      "Epoch 183 of 500, Train Loss: 0.011667\n",
      "Epoch 184 of 500, Train Loss: 0.011663\n",
      "best loss:  0.011662725352771516\n",
      "Epoch 185 of 500, Train Loss: 0.011667\n",
      "Epoch 186 of 500, Train Loss: 0.011663\n",
      "Epoch 187 of 500, Train Loss: 0.011668\n",
      "Epoch 188 of 500, Train Loss: 0.011664\n",
      "Epoch 189 of 500, Train Loss: 0.011670\n",
      "Epoch 190 of 500, Train Loss: 0.011665\n",
      "Epoch 191 of 500, Train Loss: 0.011674\n",
      "Epoch 192 of 500, Train Loss: 0.011671\n",
      "Epoch 193 of 500, Train Loss: 0.011683\n",
      "Epoch 194 of 500, Train Loss: 0.011681\n",
      "Epoch 195 of 500, Train Loss: 0.011697\n",
      "Epoch 196 of 500, Train Loss: 0.011700\n",
      "Epoch 197 of 500, Train Loss: 0.011718\n",
      "Epoch 198 of 500, Train Loss: 0.011721\n",
      "Epoch 199 of 500, Train Loss: 0.011731\n",
      "Epoch 200 of 500, Train Loss: 0.011727\n",
      "Epoch 201 of 500, Train Loss: 0.011725\n",
      "Epoch 202 of 500, Train Loss: 0.011721\n",
      "Epoch 203 of 500, Train Loss: 0.011712\n",
      "Epoch 204 of 500, Train Loss: 0.011714\n",
      "Epoch 205 of 500, Train Loss: 0.011706\n",
      "Epoch 206 of 500, Train Loss: 0.011710\n",
      "Epoch 207 of 500, Train Loss: 0.011702\n",
      "Epoch 208 of 500, Train Loss: 0.011706\n",
      "Epoch 209 of 500, Train Loss: 0.011700\n",
      "Epoch 210 of 500, Train Loss: 0.011701\n",
      "Epoch 211 of 500, Train Loss: 0.011693\n",
      "Epoch 212 of 500, Train Loss: 0.011691\n",
      "Epoch 213 of 500, Train Loss: 0.011684\n",
      "Epoch 214 of 500, Train Loss: 0.011681\n",
      "Epoch 215 of 500, Train Loss: 0.011678\n",
      "Epoch 216 of 500, Train Loss: 0.011676\n",
      "Epoch 217 of 500, Train Loss: 0.011676\n",
      "Epoch 218 of 500, Train Loss: 0.011672\n",
      "Epoch 219 of 500, Train Loss: 0.011672\n",
      "Epoch 220 of 500, Train Loss: 0.011667\n",
      "Epoch 221 of 500, Train Loss: 0.011669\n",
      "Epoch 222 of 500, Train Loss: 0.011661\n",
      "best loss:  0.011661313399388239\n",
      "Epoch 223 of 500, Train Loss: 0.011664\n",
      "Epoch 224 of 500, Train Loss: 0.011658\n",
      "best loss:  0.011657540262505312\n",
      "Epoch 225 of 500, Train Loss: 0.011664\n",
      "Epoch 226 of 500, Train Loss: 0.011663\n",
      "Epoch 227 of 500, Train Loss: 0.011680\n",
      "Epoch 228 of 500, Train Loss: 0.011686\n",
      "Epoch 229 of 500, Train Loss: 0.011733\n",
      "Epoch 230 of 500, Train Loss: 0.011764\n",
      "Epoch 231 of 500, Train Loss: 0.011934\n",
      "Epoch 232 of 500, Train Loss: 0.011933\n",
      "Epoch 233 of 500, Train Loss: 0.012140\n",
      "Epoch 234 of 500, Train Loss: 0.012016\n",
      "Epoch 235 of 500, Train Loss: 0.012029\n",
      "Epoch 236 of 500, Train Loss: 0.011955\n",
      "Epoch 237 of 500, Train Loss: 0.011953\n",
      "Epoch 238 of 500, Train Loss: 0.011896\n",
      "Epoch 239 of 500, Train Loss: 0.011876\n",
      "Epoch 240 of 500, Train Loss: 0.011833\n",
      "Epoch 241 of 500, Train Loss: 0.011810\n",
      "Epoch 242 of 500, Train Loss: 0.011784\n",
      "Epoch 243 of 500, Train Loss: 0.011772\n",
      "Epoch 244 of 500, Train Loss: 0.011779\n",
      "Epoch 245 of 500, Train Loss: 0.011783\n",
      "Epoch 246 of 500, Train Loss: 0.011817\n",
      "Epoch 247 of 500, Train Loss: 0.011810\n",
      "Epoch 248 of 500, Train Loss: 0.011851\n",
      "Epoch 249 of 500, Train Loss: 0.011810\n",
      "Epoch 250 of 500, Train Loss: 0.011834\n",
      "Epoch 251 of 500, Train Loss: 0.011777\n",
      "Epoch 252 of 500, Train Loss: 0.011819\n",
      "Epoch 253 of 500, Train Loss: 0.011770\n",
      "Epoch 254 of 500, Train Loss: 0.011819\n",
      "Epoch 255 of 500, Train Loss: 0.011770\n",
      "Epoch 256 of 500, Train Loss: 0.011803\n",
      "Epoch 257 of 500, Train Loss: 0.011753\n",
      "Epoch 258 of 500, Train Loss: 0.011778\n",
      "Epoch 259 of 500, Train Loss: 0.011731\n",
      "Epoch 260 of 500, Train Loss: 0.011750\n",
      "Epoch 261 of 500, Train Loss: 0.011713\n",
      "Epoch 262 of 500, Train Loss: 0.011729\n",
      "Epoch 263 of 500, Train Loss: 0.011701\n",
      "Epoch 264 of 500, Train Loss: 0.011714\n",
      "Epoch 265 of 500, Train Loss: 0.011690\n",
      "Epoch 266 of 500, Train Loss: 0.011705\n",
      "Epoch 267 of 500, Train Loss: 0.011687\n",
      "Epoch 268 of 500, Train Loss: 0.011702\n",
      "Epoch 269 of 500, Train Loss: 0.011692\n",
      "Epoch 270 of 500, Train Loss: 0.011706\n",
      "Epoch 271 of 500, Train Loss: 0.011693\n",
      "Epoch 272 of 500, Train Loss: 0.011698\n",
      "Epoch 273 of 500, Train Loss: 0.011681\n",
      "Epoch 274 of 500, Train Loss: 0.011685\n",
      "Epoch 275 of 500, Train Loss: 0.011671\n",
      "Epoch 276 of 500, Train Loss: 0.011674\n",
      "Epoch 277 of 500, Train Loss: 0.011668\n",
      "Epoch 278 of 500, Train Loss: 0.011673\n",
      "Epoch 279 of 500, Train Loss: 0.011671\n",
      "Epoch 280 of 500, Train Loss: 0.011678\n",
      "Epoch 281 of 500, Train Loss: 0.011677\n",
      "Epoch 282 of 500, Train Loss: 0.011687\n",
      "Epoch 283 of 500, Train Loss: 0.011684\n",
      "Epoch 284 of 500, Train Loss: 0.011688\n",
      "Epoch 285 of 500, Train Loss: 0.011686\n",
      "Epoch 286 of 500, Train Loss: 0.011687\n",
      "Epoch 287 of 500, Train Loss: 0.011682\n",
      "Epoch 288 of 500, Train Loss: 0.011681\n",
      "Epoch 289 of 500, Train Loss: 0.011677\n",
      "Epoch 290 of 500, Train Loss: 0.011676\n",
      "Epoch 291 of 500, Train Loss: 0.011673\n",
      "Epoch 292 of 500, Train Loss: 0.011673\n",
      "Epoch 293 of 500, Train Loss: 0.011672\n",
      "Epoch 294 of 500, Train Loss: 0.011673\n",
      "Epoch 295 of 500, Train Loss: 0.011672\n",
      "Epoch 296 of 500, Train Loss: 0.011673\n",
      "Epoch 297 of 500, Train Loss: 0.011672\n",
      "Epoch 298 of 500, Train Loss: 0.011674\n",
      "Epoch 299 of 500, Train Loss: 0.011675\n",
      "Epoch 300 of 500, Train Loss: 0.011677\n",
      "Epoch 301 of 500, Train Loss: 0.011679\n",
      "Epoch 302 of 500, Train Loss: 0.011680\n",
      "Epoch 303 of 500, Train Loss: 0.011682\n",
      "Epoch 304 of 500, Train Loss: 0.011683\n",
      "Epoch 305 of 500, Train Loss: 0.011684\n",
      "Epoch 306 of 500, Train Loss: 0.011686\n",
      "Epoch 307 of 500, Train Loss: 0.011688\n",
      "Epoch 308 of 500, Train Loss: 0.011689\n",
      "Epoch 309 of 500, Train Loss: 0.011692\n",
      "Epoch 310 of 500, Train Loss: 0.011694\n",
      "Epoch 311 of 500, Train Loss: 0.011698\n",
      "Epoch 312 of 500, Train Loss: 0.011698\n",
      "Epoch 313 of 500, Train Loss: 0.011702\n",
      "Epoch 314 of 500, Train Loss: 0.011701\n",
      "Epoch 315 of 500, Train Loss: 0.011704\n",
      "Epoch 316 of 500, Train Loss: 0.011699\n",
      "Epoch 317 of 500, Train Loss: 0.011703\n",
      "Epoch 318 of 500, Train Loss: 0.011696\n",
      "Epoch 319 of 500, Train Loss: 0.011701\n",
      "Epoch 320 of 500, Train Loss: 0.011692\n",
      "Epoch 321 of 500, Train Loss: 0.011696\n",
      "Epoch 322 of 500, Train Loss: 0.011682\n",
      "Epoch 323 of 500, Train Loss: 0.011689\n",
      "Epoch 324 of 500, Train Loss: 0.011673\n",
      "Epoch 325 of 500, Train Loss: 0.011684\n",
      "Epoch 326 of 500, Train Loss: 0.011667\n",
      "Epoch 327 of 500, Train Loss: 0.011681\n",
      "Epoch 328 of 500, Train Loss: 0.011664\n",
      "Epoch 329 of 500, Train Loss: 0.011677\n",
      "Epoch 330 of 500, Train Loss: 0.011657\n",
      "best loss:  0.011657293251415086\n",
      "Epoch 331 of 500, Train Loss: 0.011673\n",
      "Epoch 332 of 500, Train Loss: 0.011654\n",
      "best loss:  0.011654096459717194\n",
      "Epoch 333 of 500, Train Loss: 0.011672\n",
      "Epoch 334 of 500, Train Loss: 0.011654\n",
      "Epoch 335 of 500, Train Loss: 0.011674\n",
      "Epoch 336 of 500, Train Loss: 0.011659\n",
      "Epoch 337 of 500, Train Loss: 0.011679\n",
      "Epoch 338 of 500, Train Loss: 0.011670\n",
      "Epoch 339 of 500, Train Loss: 0.011689\n",
      "Epoch 340 of 500, Train Loss: 0.011687\n",
      "Epoch 341 of 500, Train Loss: 0.011703\n",
      "Epoch 342 of 500, Train Loss: 0.011697\n",
      "Epoch 343 of 500, Train Loss: 0.011709\n",
      "Epoch 344 of 500, Train Loss: 0.011694\n",
      "Epoch 345 of 500, Train Loss: 0.011702\n",
      "Epoch 346 of 500, Train Loss: 0.011679\n",
      "Epoch 347 of 500, Train Loss: 0.011687\n",
      "Epoch 348 of 500, Train Loss: 0.011666\n",
      "Epoch 349 of 500, Train Loss: 0.011674\n",
      "Epoch 350 of 500, Train Loss: 0.011659\n",
      "Epoch 351 of 500, Train Loss: 0.011665\n",
      "Epoch 352 of 500, Train Loss: 0.011653\n",
      "best loss:  0.011652697414672362\n",
      "Epoch 353 of 500, Train Loss: 0.011658\n",
      "Epoch 354 of 500, Train Loss: 0.011649\n",
      "best loss:  0.011649300860035298\n",
      "Epoch 355 of 500, Train Loss: 0.011653\n",
      "Epoch 356 of 500, Train Loss: 0.011647\n",
      "best loss:  0.011647131579635707\n",
      "Epoch 357 of 500, Train Loss: 0.011651\n",
      "Epoch 358 of 500, Train Loss: 0.011648\n",
      "Epoch 359 of 500, Train Loss: 0.011651\n",
      "Epoch 360 of 500, Train Loss: 0.011649\n",
      "Epoch 361 of 500, Train Loss: 0.011651\n",
      "Epoch 362 of 500, Train Loss: 0.011652\n",
      "Epoch 363 of 500, Train Loss: 0.011652\n",
      "Epoch 364 of 500, Train Loss: 0.011653\n",
      "Epoch 365 of 500, Train Loss: 0.011651\n",
      "Epoch 366 of 500, Train Loss: 0.011653\n",
      "Epoch 367 of 500, Train Loss: 0.011650\n",
      "Epoch 368 of 500, Train Loss: 0.011652\n",
      "Epoch 369 of 500, Train Loss: 0.011649\n",
      "Epoch 370 of 500, Train Loss: 0.011650\n",
      "Epoch 371 of 500, Train Loss: 0.011647\n",
      "best loss:  0.011646720444600371\n",
      "Epoch 372 of 500, Train Loss: 0.011649\n",
      "Epoch 373 of 500, Train Loss: 0.011646\n",
      "best loss:  0.011645539273199257\n",
      "Epoch 374 of 500, Train Loss: 0.011648\n",
      "Epoch 375 of 500, Train Loss: 0.011644\n",
      "best loss:  0.011644443268066474\n",
      "Epoch 376 of 500, Train Loss: 0.011645\n",
      "Epoch 377 of 500, Train Loss: 0.011642\n",
      "best loss:  0.011642027786069247\n",
      "Epoch 378 of 500, Train Loss: 0.011643\n",
      "Epoch 379 of 500, Train Loss: 0.011642\n",
      "best loss:  0.01164181300984017\n",
      "Epoch 380 of 500, Train Loss: 0.011644\n",
      "Epoch 381 of 500, Train Loss: 0.011642\n",
      "Epoch 382 of 500, Train Loss: 0.011641\n",
      "best loss:  0.011641290120495296\n",
      "Epoch 383 of 500, Train Loss: 0.011642\n",
      "Epoch 384 of 500, Train Loss: 0.011641\n",
      "best loss:  0.011640977434017118\n",
      "Epoch 385 of 500, Train Loss: 0.011643\n",
      "Epoch 386 of 500, Train Loss: 0.011641\n",
      "Epoch 387 of 500, Train Loss: 0.011644\n",
      "Epoch 388 of 500, Train Loss: 0.011641\n",
      "Epoch 389 of 500, Train Loss: 0.011645\n",
      "Epoch 390 of 500, Train Loss: 0.011641\n",
      "Epoch 391 of 500, Train Loss: 0.011646\n",
      "Epoch 392 of 500, Train Loss: 0.011641\n",
      "best loss:  0.01164064384015934\n",
      "Epoch 393 of 500, Train Loss: 0.011646\n",
      "Epoch 394 of 500, Train Loss: 0.011640\n",
      "best loss:  0.011639895177511283\n",
      "Epoch 395 of 500, Train Loss: 0.011646\n",
      "Epoch 396 of 500, Train Loss: 0.011638\n",
      "best loss:  0.011637929428771296\n",
      "Epoch 397 of 500, Train Loss: 0.011643\n",
      "Epoch 398 of 500, Train Loss: 0.011635\n",
      "best loss:  0.011634739126325152\n",
      "Epoch 399 of 500, Train Loss: 0.011641\n",
      "Epoch 400 of 500, Train Loss: 0.011632\n",
      "best loss:  0.011631613759488199\n",
      "Epoch 401 of 500, Train Loss: 0.011638\n",
      "Epoch 402 of 500, Train Loss: 0.011629\n",
      "best loss:  0.011629473298874706\n",
      "Epoch 403 of 500, Train Loss: 0.011637\n",
      "Epoch 404 of 500, Train Loss: 0.011629\n",
      "best loss:  0.011629329454546339\n",
      "Epoch 405 of 500, Train Loss: 0.011638\n",
      "Epoch 406 of 500, Train Loss: 0.011631\n",
      "Epoch 407 of 500, Train Loss: 0.011639\n",
      "Epoch 408 of 500, Train Loss: 0.011635\n",
      "Epoch 409 of 500, Train Loss: 0.011643\n",
      "Epoch 410 of 500, Train Loss: 0.011640\n",
      "Epoch 411 of 500, Train Loss: 0.011648\n",
      "Epoch 412 of 500, Train Loss: 0.011647\n",
      "Epoch 413 of 500, Train Loss: 0.011654\n",
      "Epoch 414 of 500, Train Loss: 0.011655\n",
      "Epoch 415 of 500, Train Loss: 0.011661\n",
      "Epoch 416 of 500, Train Loss: 0.011662\n",
      "Epoch 417 of 500, Train Loss: 0.011667\n",
      "Epoch 418 of 500, Train Loss: 0.011667\n",
      "Epoch 419 of 500, Train Loss: 0.011672\n",
      "Epoch 420 of 500, Train Loss: 0.011671\n",
      "Epoch 421 of 500, Train Loss: 0.011677\n",
      "Epoch 422 of 500, Train Loss: 0.011674\n",
      "Epoch 423 of 500, Train Loss: 0.011680\n",
      "Epoch 424 of 500, Train Loss: 0.011675\n",
      "Epoch 425 of 500, Train Loss: 0.011682\n",
      "Epoch 426 of 500, Train Loss: 0.011675\n",
      "Epoch 427 of 500, Train Loss: 0.011683\n",
      "Epoch 428 of 500, Train Loss: 0.011675\n",
      "Epoch 429 of 500, Train Loss: 0.011683\n",
      "Epoch 430 of 500, Train Loss: 0.011674\n",
      "Epoch 431 of 500, Train Loss: 0.011682\n",
      "Epoch 432 of 500, Train Loss: 0.011673\n",
      "Epoch 433 of 500, Train Loss: 0.011680\n",
      "Epoch 434 of 500, Train Loss: 0.011672\n",
      "Epoch 435 of 500, Train Loss: 0.011680\n",
      "Epoch 436 of 500, Train Loss: 0.011673\n",
      "Epoch 437 of 500, Train Loss: 0.011682\n",
      "Epoch 438 of 500, Train Loss: 0.011677\n",
      "Epoch 439 of 500, Train Loss: 0.011688\n",
      "Epoch 440 of 500, Train Loss: 0.011684\n",
      "Epoch 441 of 500, Train Loss: 0.011696\n",
      "Epoch 442 of 500, Train Loss: 0.011693\n",
      "Epoch 443 of 500, Train Loss: 0.011705\n",
      "Epoch 444 of 500, Train Loss: 0.011703\n",
      "Epoch 445 of 500, Train Loss: 0.011714\n",
      "Epoch 446 of 500, Train Loss: 0.011710\n",
      "Epoch 447 of 500, Train Loss: 0.011716\n",
      "Epoch 448 of 500, Train Loss: 0.011711\n",
      "Epoch 449 of 500, Train Loss: 0.011712\n",
      "Epoch 450 of 500, Train Loss: 0.011704\n",
      "Epoch 451 of 500, Train Loss: 0.011702\n",
      "Epoch 452 of 500, Train Loss: 0.011693\n",
      "Epoch 453 of 500, Train Loss: 0.011690\n",
      "Epoch 454 of 500, Train Loss: 0.011680\n",
      "Epoch 455 of 500, Train Loss: 0.011677\n",
      "Epoch 456 of 500, Train Loss: 0.011666\n",
      "Epoch 457 of 500, Train Loss: 0.011665\n",
      "Epoch 458 of 500, Train Loss: 0.011653\n",
      "Epoch 459 of 500, Train Loss: 0.011654\n",
      "Epoch 460 of 500, Train Loss: 0.011641\n",
      "Epoch 461 of 500, Train Loss: 0.011644\n",
      "Epoch 462 of 500, Train Loss: 0.011630\n",
      "Epoch 463 of 500, Train Loss: 0.011635\n",
      "Epoch 464 of 500, Train Loss: 0.011622\n",
      "best loss:  0.011621706169224958\n",
      "Epoch 465 of 500, Train Loss: 0.011628\n",
      "Epoch 466 of 500, Train Loss: 0.011614\n",
      "best loss:  0.011614423780566688\n",
      "Epoch 467 of 500, Train Loss: 0.011622\n",
      "Epoch 468 of 500, Train Loss: 0.011609\n",
      "best loss:  0.011609407137292473\n",
      "Epoch 469 of 500, Train Loss: 0.011619\n",
      "Epoch 470 of 500, Train Loss: 0.011608\n",
      "best loss:  0.011607862748056759\n",
      "Epoch 471 of 500, Train Loss: 0.011620\n",
      "Epoch 472 of 500, Train Loss: 0.011612\n",
      "Epoch 473 of 500, Train Loss: 0.011629\n",
      "Epoch 474 of 500, Train Loss: 0.011628\n",
      "Epoch 475 of 500, Train Loss: 0.011652\n",
      "Epoch 476 of 500, Train Loss: 0.011667\n",
      "Epoch 477 of 500, Train Loss: 0.011694\n",
      "Epoch 478 of 500, Train Loss: 0.011726\n",
      "Epoch 479 of 500, Train Loss: 0.011738\n",
      "Epoch 480 of 500, Train Loss: 0.011771\n",
      "Epoch 481 of 500, Train Loss: 0.011754\n",
      "Epoch 482 of 500, Train Loss: 0.011775\n",
      "Epoch 483 of 500, Train Loss: 0.011754\n",
      "Epoch 484 of 500, Train Loss: 0.011765\n",
      "Epoch 485 of 500, Train Loss: 0.011742\n",
      "Epoch 486 of 500, Train Loss: 0.011743\n",
      "Epoch 487 of 500, Train Loss: 0.011728\n",
      "Epoch 488 of 500, Train Loss: 0.011725\n",
      "Epoch 489 of 500, Train Loss: 0.011714\n",
      "Epoch 490 of 500, Train Loss: 0.011714\n",
      "Epoch 491 of 500, Train Loss: 0.011713\n",
      "Epoch 492 of 500, Train Loss: 0.011710\n",
      "Epoch 493 of 500, Train Loss: 0.011718\n",
      "Epoch 494 of 500, Train Loss: 0.011715\n",
      "Epoch 495 of 500, Train Loss: 0.011729\n",
      "Epoch 496 of 500, Train Loss: 0.011721\n",
      "Epoch 497 of 500, Train Loss: 0.011735\n",
      "Epoch 498 of 500, Train Loss: 0.011723\n",
      "Epoch 499 of 500, Train Loss: 0.011732\n",
      "Epoch 500 of 500, Train Loss: 0.011708\n",
      "latent train shape:  (16395, 45)\n",
      "M: 45, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 95\n",
      "Training the subspace: 0 / 45\n",
      "Training the subspace: 1 / 45\n",
      "Training the subspace: 2 / 45\n",
      "Training the subspace: 3 / 45\n",
      "Training the subspace: 4 / 45\n",
      "Training the subspace: 5 / 45\n",
      "Training the subspace: 6 / 45\n",
      "Training the subspace: 7 / 45\n",
      "Training the subspace: 8 / 45\n",
      "Training the subspace: 9 / 45\n",
      "Training the subspace: 10 / 45\n",
      "Training the subspace: 11 / 45\n",
      "Training the subspace: 12 / 45\n",
      "Training the subspace: 13 / 45\n",
      "Training the subspace: 14 / 45\n",
      "Training the subspace: 15 / 45\n",
      "Training the subspace: 16 / 45\n",
      "Training the subspace: 17 / 45\n",
      "Training the subspace: 18 / 45\n",
      "Training the subspace: 19 / 45\n",
      "Training the subspace: 20 / 45\n",
      "Training the subspace: 21 / 45\n",
      "Training the subspace: 22 / 45\n",
      "Training the subspace: 23 / 45\n",
      "Training the subspace: 24 / 45\n",
      "Training the subspace: 25 / 45\n",
      "Training the subspace: 26 / 45\n",
      "Training the subspace: 27 / 45\n",
      "Training the subspace: 28 / 45\n",
      "Training the subspace: 29 / 45\n",
      "Training the subspace: 30 / 45\n",
      "Training the subspace: 31 / 45\n",
      "Training the subspace: 32 / 45\n",
      "Training the subspace: 33 / 45\n",
      "Training the subspace: 34 / 45\n",
      "Training the subspace: 35 / 45\n",
      "Training the subspace: 36 / 45\n",
      "Training the subspace: 37 / 45\n",
      "Training the subspace: 38 / 45\n",
      "Training the subspace: 39 / 45\n",
      "Training the subspace: 40 / 45\n",
      "Training the subspace: 41 / 45\n",
      "Training the subspace: 42 / 45\n",
      "Training the subspace: 43 / 45\n",
      "Training the subspace: 44 / 45\n",
      "Encoding the subspace: 0 / 45\n",
      "Encoding the subspace: 1 / 45\n",
      "Encoding the subspace: 2 / 45\n",
      "Encoding the subspace: 3 / 45\n",
      "Encoding the subspace: 4 / 45\n",
      "Encoding the subspace: 5 / 45\n",
      "Encoding the subspace: 6 / 45\n",
      "Encoding the subspace: 7 / 45\n",
      "Encoding the subspace: 8 / 45\n",
      "Encoding the subspace: 9 / 45\n",
      "Encoding the subspace: 10 / 45\n",
      "Encoding the subspace: 11 / 45\n",
      "Encoding the subspace: 12 / 45\n",
      "Encoding the subspace: 13 / 45\n",
      "Encoding the subspace: 14 / 45\n",
      "Encoding the subspace: 15 / 45\n",
      "Encoding the subspace: 16 / 45\n",
      "Encoding the subspace: 17 / 45\n",
      "Encoding the subspace: 18 / 45\n",
      "Encoding the subspace: 19 / 45\n",
      "Encoding the subspace: 20 / 45\n",
      "Encoding the subspace: 21 / 45\n",
      "Encoding the subspace: 22 / 45\n",
      "Encoding the subspace: 23 / 45\n",
      "Encoding the subspace: 24 / 45\n",
      "Encoding the subspace: 25 / 45\n",
      "Encoding the subspace: 26 / 45\n",
      "Encoding the subspace: 27 / 45\n",
      "Encoding the subspace: 28 / 45\n",
      "Encoding the subspace: 29 / 45\n",
      "Encoding the subspace: 30 / 45\n",
      "Encoding the subspace: 31 / 45\n",
      "Encoding the subspace: 32 / 45\n",
      "Encoding the subspace: 33 / 45\n",
      "Encoding the subspace: 34 / 45\n",
      "Encoding the subspace: 35 / 45\n",
      "Encoding the subspace: 36 / 45\n",
      "Encoding the subspace: 37 / 45\n",
      "Encoding the subspace: 38 / 45\n",
      "Encoding the subspace: 39 / 45\n",
      "Encoding the subspace: 40 / 45\n",
      "Encoding the subspace: 41 / 45\n",
      "Encoding the subspace: 42 / 45\n",
      "Encoding the subspace: 43 / 45\n",
      "Encoding the subspace: 44 / 45\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=60, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.381830\n",
      "best loss:  0.3818302894112214\n",
      "Epoch 2 of 500, Train Loss: 0.054939\n",
      "best loss:  0.05493926559620024\n",
      "Epoch 3 of 500, Train Loss: 0.051201\n",
      "best loss:  0.05120149724927276\n",
      "Epoch 4 of 500, Train Loss: 0.045270\n",
      "best loss:  0.045270348813334235\n",
      "Epoch 5 of 500, Train Loss: 0.038637\n",
      "best loss:  0.03863706004529715\n",
      "Epoch 6 of 500, Train Loss: 0.033281\n",
      "best loss:  0.03328066245542637\n",
      "Epoch 7 of 500, Train Loss: 0.029490\n",
      "best loss:  0.029490035138717428\n",
      "Epoch 8 of 500, Train Loss: 0.026908\n",
      "best loss:  0.026907533402699083\n",
      "Epoch 9 of 500, Train Loss: 0.025085\n",
      "best loss:  0.025085486751861955\n",
      "Epoch 10 of 500, Train Loss: 0.023673\n",
      "best loss:  0.02367252870165028\n",
      "Epoch 11 of 500, Train Loss: 0.022471\n",
      "best loss:  0.022471329505472308\n",
      "Epoch 12 of 500, Train Loss: 0.021417\n",
      "best loss:  0.021417419547296323\n",
      "Epoch 13 of 500, Train Loss: 0.020488\n",
      "best loss:  0.0204876394481705\n",
      "Epoch 14 of 500, Train Loss: 0.019670\n",
      "best loss:  0.019669657653872675\n",
      "Epoch 15 of 500, Train Loss: 0.018947\n",
      "best loss:  0.018947181675868296\n",
      "Epoch 16 of 500, Train Loss: 0.018301\n",
      "best loss:  0.018300505216249676\n",
      "Epoch 17 of 500, Train Loss: 0.017714\n",
      "best loss:  0.017713868847503075\n",
      "Epoch 18 of 500, Train Loss: 0.017176\n",
      "best loss:  0.017176269842835157\n",
      "Epoch 19 of 500, Train Loss: 0.016674\n",
      "best loss:  0.01667382735276212\n",
      "Epoch 20 of 500, Train Loss: 0.016206\n",
      "best loss:  0.016206290671473743\n",
      "Epoch 21 of 500, Train Loss: 0.015767\n",
      "best loss:  0.015766705830568852\n",
      "Epoch 22 of 500, Train Loss: 0.015360\n",
      "best loss:  0.01536043576897093\n",
      "Epoch 23 of 500, Train Loss: 0.014988\n",
      "best loss:  0.014988441352124232\n",
      "Epoch 24 of 500, Train Loss: 0.014649\n",
      "best loss:  0.014649231969595475\n",
      "Epoch 25 of 500, Train Loss: 0.014341\n",
      "best loss:  0.014340856650622215\n",
      "Epoch 26 of 500, Train Loss: 0.014057\n",
      "best loss:  0.01405690797914159\n",
      "Epoch 27 of 500, Train Loss: 0.013802\n",
      "best loss:  0.013801946886363908\n",
      "Epoch 28 of 500, Train Loss: 0.013565\n",
      "best loss:  0.013564801504185726\n",
      "Epoch 29 of 500, Train Loss: 0.013357\n",
      "best loss:  0.013356882625560876\n",
      "Epoch 30 of 500, Train Loss: 0.013166\n",
      "best loss:  0.013165693108782952\n",
      "Epoch 31 of 500, Train Loss: 0.012992\n",
      "best loss:  0.01299171460284346\n",
      "Epoch 32 of 500, Train Loss: 0.012832\n",
      "best loss:  0.01283163807621991\n",
      "Epoch 33 of 500, Train Loss: 0.012691\n",
      "best loss:  0.012691057306559136\n",
      "Epoch 34 of 500, Train Loss: 0.012557\n",
      "best loss:  0.012556662215614807\n",
      "Epoch 35 of 500, Train Loss: 0.012452\n",
      "best loss:  0.01245248784801143\n",
      "Epoch 36 of 500, Train Loss: 0.012327\n",
      "best loss:  0.012326682377209556\n",
      "Epoch 37 of 500, Train Loss: 0.012227\n",
      "best loss:  0.012226557072798917\n",
      "Epoch 38 of 500, Train Loss: 0.012132\n",
      "best loss:  0.01213186051174926\n",
      "Epoch 39 of 500, Train Loss: 0.012035\n",
      "best loss:  0.012035471681629979\n",
      "Epoch 40 of 500, Train Loss: 0.011949\n",
      "best loss:  0.011949200442380294\n",
      "Epoch 41 of 500, Train Loss: 0.011880\n",
      "best loss:  0.01188008213738817\n",
      "Epoch 42 of 500, Train Loss: 0.011799\n",
      "best loss:  0.011798775288079641\n",
      "Epoch 43 of 500, Train Loss: 0.011742\n",
      "best loss:  0.011741673446832543\n",
      "Epoch 44 of 500, Train Loss: 0.011662\n",
      "best loss:  0.011662229999824058\n",
      "Epoch 45 of 500, Train Loss: 0.011609\n",
      "best loss:  0.011609004216418464\n",
      "Epoch 46 of 500, Train Loss: 0.011553\n",
      "best loss:  0.01155282322329996\n",
      "Epoch 47 of 500, Train Loss: 0.011493\n",
      "best loss:  0.011493101562174802\n",
      "Epoch 48 of 500, Train Loss: 0.011427\n",
      "best loss:  0.011427446911716111\n",
      "Epoch 49 of 500, Train Loss: 0.011382\n",
      "best loss:  0.01138231396817696\n",
      "Epoch 50 of 500, Train Loss: 0.011330\n",
      "best loss:  0.011330465640188664\n",
      "Epoch 51 of 500, Train Loss: 0.011277\n",
      "best loss:  0.011277039037419974\n",
      "Epoch 52 of 500, Train Loss: 0.011232\n",
      "best loss:  0.011232011802151597\n",
      "Epoch 53 of 500, Train Loss: 0.011197\n",
      "best loss:  0.011197092585074318\n",
      "Epoch 54 of 500, Train Loss: 0.011153\n",
      "best loss:  0.011152848307708416\n",
      "Epoch 55 of 500, Train Loss: 0.011111\n",
      "best loss:  0.011110991384728465\n",
      "Epoch 56 of 500, Train Loss: 0.011077\n",
      "best loss:  0.01107741984646576\n",
      "Epoch 57 of 500, Train Loss: 0.011046\n",
      "best loss:  0.011046465833503537\n",
      "Epoch 58 of 500, Train Loss: 0.011004\n",
      "best loss:  0.011003735848741295\n",
      "Epoch 59 of 500, Train Loss: 0.010977\n",
      "best loss:  0.010976724983364444\n",
      "Epoch 60 of 500, Train Loss: 0.010945\n",
      "best loss:  0.010945297575730817\n",
      "Epoch 61 of 500, Train Loss: 0.010912\n",
      "best loss:  0.010912244584137943\n",
      "Epoch 62 of 500, Train Loss: 0.010890\n",
      "best loss:  0.010889896346764212\n",
      "Epoch 63 of 500, Train Loss: 0.010863\n",
      "best loss:  0.010862973020407516\n",
      "Epoch 64 of 500, Train Loss: 0.010842\n",
      "best loss:  0.010842400002870853\n",
      "Epoch 65 of 500, Train Loss: 0.010815\n",
      "best loss:  0.010814643057303272\n",
      "Epoch 66 of 500, Train Loss: 0.010796\n",
      "best loss:  0.010795908074355829\n",
      "Epoch 67 of 500, Train Loss: 0.010772\n",
      "best loss:  0.010772233536284662\n",
      "Epoch 68 of 500, Train Loss: 0.010758\n",
      "best loss:  0.010758483126784797\n",
      "Epoch 69 of 500, Train Loss: 0.010732\n",
      "best loss:  0.010732086436947548\n",
      "Epoch 70 of 500, Train Loss: 0.010714\n",
      "best loss:  0.010714148777575852\n",
      "Epoch 71 of 500, Train Loss: 0.010704\n",
      "best loss:  0.010703971294181732\n",
      "Epoch 72 of 500, Train Loss: 0.010689\n",
      "best loss:  0.010688911239159033\n",
      "Epoch 73 of 500, Train Loss: 0.010678\n",
      "best loss:  0.010678321597019261\n",
      "Epoch 74 of 500, Train Loss: 0.010670\n",
      "best loss:  0.010669711755693334\n",
      "Epoch 75 of 500, Train Loss: 0.010660\n",
      "best loss:  0.010659988357143137\n",
      "Epoch 76 of 500, Train Loss: 0.010651\n",
      "best loss:  0.010650847574186967\n",
      "Epoch 77 of 500, Train Loss: 0.010640\n",
      "best loss:  0.010639782131233526\n",
      "Epoch 78 of 500, Train Loss: 0.010630\n",
      "best loss:  0.010629563969257648\n",
      "Epoch 79 of 500, Train Loss: 0.010619\n",
      "best loss:  0.010619193160511164\n",
      "Epoch 80 of 500, Train Loss: 0.010607\n",
      "best loss:  0.010607209053795691\n",
      "Epoch 81 of 500, Train Loss: 0.010599\n",
      "best loss:  0.010599175197466695\n",
      "Epoch 82 of 500, Train Loss: 0.010592\n",
      "best loss:  0.010591687083080982\n",
      "Epoch 83 of 500, Train Loss: 0.010583\n",
      "best loss:  0.010582722463016038\n",
      "Epoch 84 of 500, Train Loss: 0.010579\n",
      "best loss:  0.0105786845249317\n",
      "Epoch 85 of 500, Train Loss: 0.010572\n",
      "best loss:  0.010571686137091304\n",
      "Epoch 86 of 500, Train Loss: 0.010566\n",
      "best loss:  0.010566313492829998\n",
      "Epoch 87 of 500, Train Loss: 0.010566\n",
      "best loss:  0.010566008124273097\n",
      "Epoch 88 of 500, Train Loss: 0.010562\n",
      "best loss:  0.010561825327400132\n",
      "Epoch 89 of 500, Train Loss: 0.010559\n",
      "best loss:  0.010559363988933854\n",
      "Epoch 90 of 500, Train Loss: 0.010558\n",
      "best loss:  0.010558183423185707\n",
      "Epoch 91 of 500, Train Loss: 0.010557\n",
      "best loss:  0.010556563347972416\n",
      "Epoch 92 of 500, Train Loss: 0.010554\n",
      "best loss:  0.010553776460939787\n",
      "Epoch 93 of 500, Train Loss: 0.010551\n",
      "best loss:  0.010551052468525635\n",
      "Epoch 94 of 500, Train Loss: 0.010547\n",
      "best loss:  0.010546656047393813\n",
      "Epoch 95 of 500, Train Loss: 0.010544\n",
      "best loss:  0.010543803396533684\n",
      "Epoch 96 of 500, Train Loss: 0.010538\n",
      "best loss:  0.010538092799946781\n",
      "Epoch 97 of 500, Train Loss: 0.010535\n",
      "best loss:  0.010534553106016004\n",
      "Epoch 98 of 500, Train Loss: 0.010528\n",
      "best loss:  0.010528085475932814\n",
      "Epoch 99 of 500, Train Loss: 0.010528\n",
      "Epoch 100 of 500, Train Loss: 0.010525\n",
      "best loss:  0.010524816231264248\n",
      "Epoch 101 of 500, Train Loss: 0.010527\n",
      "Epoch 102 of 500, Train Loss: 0.010526\n",
      "Epoch 103 of 500, Train Loss: 0.010535\n",
      "Epoch 104 of 500, Train Loss: 0.010535\n",
      "Epoch 105 of 500, Train Loss: 0.010552\n",
      "Epoch 106 of 500, Train Loss: 0.010553\n",
      "Epoch 107 of 500, Train Loss: 0.010582\n",
      "Epoch 108 of 500, Train Loss: 0.010586\n",
      "Epoch 109 of 500, Train Loss: 0.010631\n",
      "Epoch 110 of 500, Train Loss: 0.010637\n",
      "Epoch 111 of 500, Train Loss: 0.010715\n",
      "Epoch 112 of 500, Train Loss: 0.010758\n",
      "Epoch 113 of 500, Train Loss: 0.010854\n",
      "Epoch 114 of 500, Train Loss: 0.010857\n",
      "Epoch 115 of 500, Train Loss: 0.010853\n",
      "Epoch 116 of 500, Train Loss: 0.010786\n",
      "Epoch 117 of 500, Train Loss: 0.010725\n",
      "Epoch 118 of 500, Train Loss: 0.010649\n",
      "Epoch 119 of 500, Train Loss: 0.010640\n",
      "Epoch 120 of 500, Train Loss: 0.010694\n",
      "Epoch 121 of 500, Train Loss: 0.010761\n",
      "Epoch 122 of 500, Train Loss: 0.010983\n",
      "Epoch 123 of 500, Train Loss: 0.010871\n",
      "Epoch 124 of 500, Train Loss: 0.010891\n",
      "Epoch 125 of 500, Train Loss: 0.010862\n",
      "Epoch 126 of 500, Train Loss: 0.010877\n",
      "Epoch 127 of 500, Train Loss: 0.010820\n",
      "Epoch 128 of 500, Train Loss: 0.010754\n",
      "Epoch 129 of 500, Train Loss: 0.010734\n",
      "Epoch 130 of 500, Train Loss: 0.010680\n",
      "Epoch 131 of 500, Train Loss: 0.010675\n",
      "Epoch 132 of 500, Train Loss: 0.010637\n",
      "Epoch 133 of 500, Train Loss: 0.010640\n",
      "Epoch 134 of 500, Train Loss: 0.010615\n",
      "Epoch 135 of 500, Train Loss: 0.010622\n",
      "Epoch 136 of 500, Train Loss: 0.010621\n",
      "Epoch 137 of 500, Train Loss: 0.010623\n",
      "Epoch 138 of 500, Train Loss: 0.010630\n",
      "Epoch 139 of 500, Train Loss: 0.010624\n",
      "Epoch 140 of 500, Train Loss: 0.010645\n",
      "Epoch 141 of 500, Train Loss: 0.010632\n",
      "Epoch 142 of 500, Train Loss: 0.010651\n",
      "Epoch 143 of 500, Train Loss: 0.010641\n",
      "Epoch 144 of 500, Train Loss: 0.010657\n",
      "Epoch 145 of 500, Train Loss: 0.010635\n",
      "Epoch 146 of 500, Train Loss: 0.010651\n",
      "Epoch 147 of 500, Train Loss: 0.010618\n",
      "Epoch 148 of 500, Train Loss: 0.010635\n",
      "Epoch 149 of 500, Train Loss: 0.010613\n",
      "Epoch 150 of 500, Train Loss: 0.010626\n",
      "Epoch 151 of 500, Train Loss: 0.010613\n",
      "Epoch 152 of 500, Train Loss: 0.010627\n",
      "Epoch 153 of 500, Train Loss: 0.010619\n",
      "Epoch 154 of 500, Train Loss: 0.010635\n",
      "Epoch 155 of 500, Train Loss: 0.010632\n",
      "Epoch 156 of 500, Train Loss: 0.010644\n",
      "Epoch 157 of 500, Train Loss: 0.010635\n",
      "Epoch 158 of 500, Train Loss: 0.010639\n",
      "Epoch 159 of 500, Train Loss: 0.010637\n",
      "Epoch 160 of 500, Train Loss: 0.010635\n",
      "Epoch 161 of 500, Train Loss: 0.010632\n",
      "Epoch 162 of 500, Train Loss: 0.010629\n",
      "Epoch 163 of 500, Train Loss: 0.010624\n",
      "Epoch 164 of 500, Train Loss: 0.010618\n",
      "Epoch 165 of 500, Train Loss: 0.010617\n",
      "Epoch 166 of 500, Train Loss: 0.010611\n",
      "Epoch 167 of 500, Train Loss: 0.010613\n",
      "Epoch 168 of 500, Train Loss: 0.010606\n",
      "Epoch 169 of 500, Train Loss: 0.010615\n",
      "Epoch 170 of 500, Train Loss: 0.010603\n",
      "Epoch 171 of 500, Train Loss: 0.010622\n",
      "Epoch 172 of 500, Train Loss: 0.010612\n",
      "Epoch 173 of 500, Train Loss: 0.010650\n",
      "Epoch 174 of 500, Train Loss: 0.010626\n",
      "Epoch 175 of 500, Train Loss: 0.010665\n",
      "Epoch 176 of 500, Train Loss: 0.010617\n",
      "Epoch 177 of 500, Train Loss: 0.010646\n",
      "Epoch 178 of 500, Train Loss: 0.010588\n",
      "Epoch 179 of 500, Train Loss: 0.010610\n",
      "Epoch 180 of 500, Train Loss: 0.010558\n",
      "Epoch 181 of 500, Train Loss: 0.010577\n",
      "Epoch 182 of 500, Train Loss: 0.010541\n",
      "Epoch 183 of 500, Train Loss: 0.010557\n",
      "Epoch 184 of 500, Train Loss: 0.010532\n",
      "Epoch 185 of 500, Train Loss: 0.010547\n",
      "Epoch 186 of 500, Train Loss: 0.010536\n",
      "Epoch 187 of 500, Train Loss: 0.010553\n",
      "Epoch 188 of 500, Train Loss: 0.010555\n",
      "Epoch 189 of 500, Train Loss: 0.010571\n",
      "Epoch 190 of 500, Train Loss: 0.010583\n",
      "Epoch 191 of 500, Train Loss: 0.010598\n",
      "Epoch 192 of 500, Train Loss: 0.010613\n",
      "Epoch 193 of 500, Train Loss: 0.010615\n",
      "Epoch 194 of 500, Train Loss: 0.010626\n",
      "Epoch 195 of 500, Train Loss: 0.010616\n",
      "Epoch 196 of 500, Train Loss: 0.010618\n",
      "Epoch 197 of 500, Train Loss: 0.010605\n",
      "Epoch 198 of 500, Train Loss: 0.010598\n",
      "Epoch 199 of 500, Train Loss: 0.010586\n",
      "Epoch 200 of 500, Train Loss: 0.010574\n",
      "Epoch 201 of 500, Train Loss: 0.010565\n",
      "Epoch 202 of 500, Train Loss: 0.010554\n",
      "Epoch 203 of 500, Train Loss: 0.010548\n",
      "Epoch 204 of 500, Train Loss: 0.010542\n",
      "Epoch 205 of 500, Train Loss: 0.010536\n",
      "Epoch 206 of 500, Train Loss: 0.010536\n",
      "Epoch 207 of 500, Train Loss: 0.010530\n",
      "Epoch 208 of 500, Train Loss: 0.010540\n",
      "Epoch 209 of 500, Train Loss: 0.010535\n",
      "Epoch 210 of 500, Train Loss: 0.010560\n",
      "Epoch 211 of 500, Train Loss: 0.010559\n",
      "Epoch 212 of 500, Train Loss: 0.010583\n",
      "Epoch 213 of 500, Train Loss: 0.010567\n",
      "Epoch 214 of 500, Train Loss: 0.010558\n",
      "Epoch 215 of 500, Train Loss: 0.010540\n",
      "Epoch 216 of 500, Train Loss: 0.010539\n",
      "Epoch 217 of 500, Train Loss: 0.010534\n",
      "Epoch 218 of 500, Train Loss: 0.010547\n",
      "Epoch 219 of 500, Train Loss: 0.010548\n",
      "Epoch 220 of 500, Train Loss: 0.010579\n",
      "Epoch 221 of 500, Train Loss: 0.010585\n",
      "Epoch 222 of 500, Train Loss: 0.010640\n",
      "Epoch 223 of 500, Train Loss: 0.010641\n",
      "Epoch 224 of 500, Train Loss: 0.010687\n",
      "Epoch 225 of 500, Train Loss: 0.010633\n",
      "Epoch 226 of 500, Train Loss: 0.010588\n",
      "Epoch 227 of 500, Train Loss: 0.010528\n",
      "Epoch 228 of 500, Train Loss: 0.010492\n",
      "best loss:  0.010492258836391613\n",
      "Epoch 229 of 500, Train Loss: 0.010461\n",
      "best loss:  0.010461224477003553\n",
      "Epoch 230 of 500, Train Loss: 0.010459\n",
      "best loss:  0.010458548338280615\n",
      "Epoch 231 of 500, Train Loss: 0.010441\n",
      "best loss:  0.010441122963968593\n",
      "Epoch 232 of 500, Train Loss: 0.010457\n",
      "Epoch 233 of 500, Train Loss: 0.010451\n",
      "Epoch 234 of 500, Train Loss: 0.010477\n",
      "Epoch 235 of 500, Train Loss: 0.010475\n",
      "Epoch 236 of 500, Train Loss: 0.010492\n",
      "Epoch 237 of 500, Train Loss: 0.010493\n",
      "Epoch 238 of 500, Train Loss: 0.010506\n",
      "Epoch 239 of 500, Train Loss: 0.010527\n",
      "Epoch 240 of 500, Train Loss: 0.010562\n",
      "Epoch 241 of 500, Train Loss: 0.010610\n",
      "Epoch 242 of 500, Train Loss: 0.010637\n",
      "Epoch 243 of 500, Train Loss: 0.010656\n",
      "Epoch 244 of 500, Train Loss: 0.010606\n",
      "Epoch 245 of 500, Train Loss: 0.010590\n",
      "Epoch 246 of 500, Train Loss: 0.010556\n",
      "Epoch 247 of 500, Train Loss: 0.010550\n",
      "Epoch 248 of 500, Train Loss: 0.010520\n",
      "Epoch 249 of 500, Train Loss: 0.010529\n",
      "Epoch 250 of 500, Train Loss: 0.010501\n",
      "Epoch 251 of 500, Train Loss: 0.010518\n",
      "Epoch 252 of 500, Train Loss: 0.010500\n",
      "Epoch 253 of 500, Train Loss: 0.010506\n",
      "Epoch 254 of 500, Train Loss: 0.010496\n",
      "Epoch 255 of 500, Train Loss: 0.010482\n",
      "Epoch 256 of 500, Train Loss: 0.010471\n",
      "Epoch 257 of 500, Train Loss: 0.010453\n",
      "Epoch 258 of 500, Train Loss: 0.010441\n",
      "best loss:  0.010440643519648646\n",
      "Epoch 259 of 500, Train Loss: 0.010434\n",
      "best loss:  0.01043400888941738\n",
      "Epoch 260 of 500, Train Loss: 0.010424\n",
      "best loss:  0.010423960316589629\n",
      "Epoch 261 of 500, Train Loss: 0.010430\n",
      "Epoch 262 of 500, Train Loss: 0.010429\n",
      "Epoch 263 of 500, Train Loss: 0.010444\n",
      "Epoch 264 of 500, Train Loss: 0.010457\n",
      "Epoch 265 of 500, Train Loss: 0.010475\n",
      "Epoch 266 of 500, Train Loss: 0.010485\n",
      "Epoch 267 of 500, Train Loss: 0.010475\n",
      "Epoch 268 of 500, Train Loss: 0.010446\n",
      "Epoch 269 of 500, Train Loss: 0.010412\n",
      "best loss:  0.010412286155403216\n",
      "Epoch 270 of 500, Train Loss: 0.010395\n",
      "best loss:  0.010394590612940397\n",
      "Epoch 271 of 500, Train Loss: 0.010375\n",
      "best loss:  0.010375163837696079\n",
      "Epoch 272 of 500, Train Loss: 0.010369\n",
      "best loss:  0.010368819076427107\n",
      "Epoch 273 of 500, Train Loss: 0.010369\n",
      "Epoch 274 of 500, Train Loss: 0.010375\n",
      "Epoch 275 of 500, Train Loss: 0.010385\n",
      "Epoch 276 of 500, Train Loss: 0.010397\n",
      "Epoch 277 of 500, Train Loss: 0.010409\n",
      "Epoch 278 of 500, Train Loss: 0.010421\n",
      "Epoch 279 of 500, Train Loss: 0.010431\n",
      "Epoch 280 of 500, Train Loss: 0.010443\n",
      "Epoch 281 of 500, Train Loss: 0.010458\n",
      "Epoch 282 of 500, Train Loss: 0.010479\n",
      "Epoch 283 of 500, Train Loss: 0.010472\n",
      "Epoch 284 of 500, Train Loss: 0.010491\n",
      "Epoch 285 of 500, Train Loss: 0.010456\n",
      "Epoch 286 of 500, Train Loss: 0.010469\n",
      "Epoch 287 of 500, Train Loss: 0.010443\n",
      "Epoch 288 of 500, Train Loss: 0.010450\n",
      "Epoch 289 of 500, Train Loss: 0.010438\n",
      "Epoch 290 of 500, Train Loss: 0.010446\n",
      "Epoch 291 of 500, Train Loss: 0.010452\n",
      "Epoch 292 of 500, Train Loss: 0.010463\n",
      "Epoch 293 of 500, Train Loss: 0.010482\n",
      "Epoch 294 of 500, Train Loss: 0.010499\n",
      "Epoch 295 of 500, Train Loss: 0.010523\n",
      "Epoch 296 of 500, Train Loss: 0.010535\n",
      "Epoch 297 of 500, Train Loss: 0.010559\n",
      "Epoch 298 of 500, Train Loss: 0.010547\n",
      "Epoch 299 of 500, Train Loss: 0.010565\n",
      "Epoch 300 of 500, Train Loss: 0.010529\n",
      "Epoch 301 of 500, Train Loss: 0.010523\n",
      "Epoch 302 of 500, Train Loss: 0.010470\n",
      "Epoch 303 of 500, Train Loss: 0.010456\n",
      "Epoch 304 of 500, Train Loss: 0.010412\n",
      "Epoch 305 of 500, Train Loss: 0.010410\n",
      "Epoch 306 of 500, Train Loss: 0.010380\n",
      "Epoch 307 of 500, Train Loss: 0.010388\n",
      "Epoch 308 of 500, Train Loss: 0.010377\n",
      "Epoch 309 of 500, Train Loss: 0.010391\n",
      "Epoch 310 of 500, Train Loss: 0.010391\n",
      "Epoch 311 of 500, Train Loss: 0.010409\n",
      "Epoch 312 of 500, Train Loss: 0.010420\n",
      "Epoch 313 of 500, Train Loss: 0.010442\n",
      "Epoch 314 of 500, Train Loss: 0.010456\n",
      "Epoch 315 of 500, Train Loss: 0.010449\n",
      "Epoch 316 of 500, Train Loss: 0.010441\n",
      "Epoch 317 of 500, Train Loss: 0.010420\n",
      "Epoch 318 of 500, Train Loss: 0.010399\n",
      "Epoch 319 of 500, Train Loss: 0.010391\n",
      "Epoch 320 of 500, Train Loss: 0.010380\n",
      "Epoch 321 of 500, Train Loss: 0.010381\n",
      "Epoch 322 of 500, Train Loss: 0.010376\n",
      "Epoch 323 of 500, Train Loss: 0.010379\n",
      "Epoch 324 of 500, Train Loss: 0.010383\n",
      "Epoch 325 of 500, Train Loss: 0.010396\n",
      "Epoch 326 of 500, Train Loss: 0.010413\n",
      "Epoch 327 of 500, Train Loss: 0.010427\n",
      "Epoch 328 of 500, Train Loss: 0.010434\n",
      "Epoch 329 of 500, Train Loss: 0.010435\n",
      "Epoch 330 of 500, Train Loss: 0.010416\n",
      "Epoch 331 of 500, Train Loss: 0.010406\n",
      "Epoch 332 of 500, Train Loss: 0.010390\n",
      "Epoch 333 of 500, Train Loss: 0.010392\n",
      "Epoch 334 of 500, Train Loss: 0.010392\n",
      "Epoch 335 of 500, Train Loss: 0.010397\n",
      "Epoch 336 of 500, Train Loss: 0.010409\n",
      "Epoch 337 of 500, Train Loss: 0.010421\n",
      "Epoch 338 of 500, Train Loss: 0.010445\n",
      "Epoch 339 of 500, Train Loss: 0.010474\n",
      "Epoch 340 of 500, Train Loss: 0.010507\n",
      "Epoch 341 of 500, Train Loss: 0.010542\n",
      "Epoch 342 of 500, Train Loss: 0.010536\n",
      "Epoch 343 of 500, Train Loss: 0.010547\n",
      "Epoch 344 of 500, Train Loss: 0.010513\n",
      "Epoch 345 of 500, Train Loss: 0.010508\n",
      "Epoch 346 of 500, Train Loss: 0.010476\n",
      "Epoch 347 of 500, Train Loss: 0.010469\n",
      "Epoch 348 of 500, Train Loss: 0.010440\n",
      "Epoch 349 of 500, Train Loss: 0.010433\n",
      "Epoch 350 of 500, Train Loss: 0.010408\n",
      "Epoch 351 of 500, Train Loss: 0.010407\n",
      "Epoch 352 of 500, Train Loss: 0.010387\n",
      "Epoch 353 of 500, Train Loss: 0.010391\n",
      "Epoch 354 of 500, Train Loss: 0.010378\n",
      "Epoch 355 of 500, Train Loss: 0.010387\n",
      "Epoch 356 of 500, Train Loss: 0.010380\n",
      "Epoch 357 of 500, Train Loss: 0.010394\n",
      "Epoch 358 of 500, Train Loss: 0.010389\n",
      "Epoch 359 of 500, Train Loss: 0.010414\n",
      "Epoch 360 of 500, Train Loss: 0.010405\n",
      "Epoch 361 of 500, Train Loss: 0.010436\n",
      "Epoch 362 of 500, Train Loss: 0.010411\n",
      "Epoch 363 of 500, Train Loss: 0.010436\n",
      "Epoch 364 of 500, Train Loss: 0.010401\n",
      "Epoch 365 of 500, Train Loss: 0.010416\n",
      "Epoch 366 of 500, Train Loss: 0.010389\n",
      "Epoch 367 of 500, Train Loss: 0.010409\n",
      "Epoch 368 of 500, Train Loss: 0.010394\n",
      "Epoch 369 of 500, Train Loss: 0.010419\n",
      "Epoch 370 of 500, Train Loss: 0.010414\n",
      "Epoch 371 of 500, Train Loss: 0.010436\n",
      "Epoch 372 of 500, Train Loss: 0.010434\n",
      "Epoch 373 of 500, Train Loss: 0.010450\n",
      "Epoch 374 of 500, Train Loss: 0.010447\n",
      "Epoch 375 of 500, Train Loss: 0.010455\n",
      "Epoch 376 of 500, Train Loss: 0.010448\n",
      "Epoch 377 of 500, Train Loss: 0.010444\n",
      "Epoch 378 of 500, Train Loss: 0.010432\n",
      "Epoch 379 of 500, Train Loss: 0.010419\n",
      "Epoch 380 of 500, Train Loss: 0.010411\n",
      "Epoch 381 of 500, Train Loss: 0.010398\n",
      "Epoch 382 of 500, Train Loss: 0.010398\n",
      "Epoch 383 of 500, Train Loss: 0.010384\n",
      "Epoch 384 of 500, Train Loss: 0.010391\n",
      "Epoch 385 of 500, Train Loss: 0.010375\n",
      "Epoch 386 of 500, Train Loss: 0.010388\n",
      "Epoch 387 of 500, Train Loss: 0.010371\n",
      "Epoch 388 of 500, Train Loss: 0.010388\n",
      "Epoch 389 of 500, Train Loss: 0.010376\n",
      "Epoch 390 of 500, Train Loss: 0.010395\n",
      "Epoch 391 of 500, Train Loss: 0.010389\n",
      "Epoch 392 of 500, Train Loss: 0.010410\n",
      "Epoch 393 of 500, Train Loss: 0.010415\n",
      "Epoch 394 of 500, Train Loss: 0.010437\n",
      "Epoch 395 of 500, Train Loss: 0.010457\n",
      "Epoch 396 of 500, Train Loss: 0.010474\n",
      "Epoch 397 of 500, Train Loss: 0.010498\n",
      "Epoch 398 of 500, Train Loss: 0.010506\n",
      "Epoch 399 of 500, Train Loss: 0.010524\n",
      "Epoch 400 of 500, Train Loss: 0.010524\n",
      "Epoch 401 of 500, Train Loss: 0.010547\n",
      "Epoch 402 of 500, Train Loss: 0.010539\n",
      "Epoch 403 of 500, Train Loss: 0.010567\n",
      "Epoch 404 of 500, Train Loss: 0.010555\n",
      "Epoch 405 of 500, Train Loss: 0.010584\n",
      "Epoch 406 of 500, Train Loss: 0.010563\n",
      "Epoch 407 of 500, Train Loss: 0.010585\n",
      "Epoch 408 of 500, Train Loss: 0.010555\n",
      "Epoch 409 of 500, Train Loss: 0.010566\n",
      "Epoch 410 of 500, Train Loss: 0.010535\n",
      "Epoch 411 of 500, Train Loss: 0.010537\n",
      "Epoch 412 of 500, Train Loss: 0.010510\n",
      "Epoch 413 of 500, Train Loss: 0.010508\n",
      "Epoch 414 of 500, Train Loss: 0.010486\n",
      "Epoch 415 of 500, Train Loss: 0.010481\n",
      "Epoch 416 of 500, Train Loss: 0.010462\n",
      "Epoch 417 of 500, Train Loss: 0.010461\n",
      "Epoch 418 of 500, Train Loss: 0.010445\n",
      "Epoch 419 of 500, Train Loss: 0.010450\n",
      "Epoch 420 of 500, Train Loss: 0.010438\n",
      "Epoch 421 of 500, Train Loss: 0.010454\n",
      "Epoch 422 of 500, Train Loss: 0.010448\n",
      "Epoch 423 of 500, Train Loss: 0.010480\n",
      "Epoch 424 of 500, Train Loss: 0.010480\n",
      "Epoch 425 of 500, Train Loss: 0.010537\n",
      "Epoch 426 of 500, Train Loss: 0.010542\n",
      "Epoch 427 of 500, Train Loss: 0.010613\n",
      "Epoch 428 of 500, Train Loss: 0.010600\n",
      "Epoch 429 of 500, Train Loss: 0.010643\n",
      "Epoch 430 of 500, Train Loss: 0.010596\n",
      "Epoch 431 of 500, Train Loss: 0.010611\n",
      "Epoch 432 of 500, Train Loss: 0.010544\n",
      "Epoch 433 of 500, Train Loss: 0.010561\n",
      "Epoch 434 of 500, Train Loss: 0.010501\n",
      "Epoch 435 of 500, Train Loss: 0.010524\n",
      "Epoch 436 of 500, Train Loss: 0.010475\n",
      "Epoch 437 of 500, Train Loss: 0.010507\n",
      "Epoch 438 of 500, Train Loss: 0.010475\n",
      "Epoch 439 of 500, Train Loss: 0.010527\n",
      "Epoch 440 of 500, Train Loss: 0.010516\n",
      "Epoch 441 of 500, Train Loss: 0.010590\n",
      "Epoch 442 of 500, Train Loss: 0.010571\n",
      "Epoch 443 of 500, Train Loss: 0.010621\n",
      "Epoch 444 of 500, Train Loss: 0.010582\n",
      "Epoch 445 of 500, Train Loss: 0.010566\n",
      "Epoch 446 of 500, Train Loss: 0.010541\n",
      "Epoch 447 of 500, Train Loss: 0.010530\n",
      "Epoch 448 of 500, Train Loss: 0.010519\n",
      "Epoch 449 of 500, Train Loss: 0.010522\n",
      "Epoch 450 of 500, Train Loss: 0.010532\n",
      "Epoch 451 of 500, Train Loss: 0.010536\n",
      "Epoch 452 of 500, Train Loss: 0.010558\n",
      "Epoch 453 of 500, Train Loss: 0.010559\n",
      "Epoch 454 of 500, Train Loss: 0.010581\n",
      "Epoch 455 of 500, Train Loss: 0.010571\n",
      "Epoch 456 of 500, Train Loss: 0.010581\n",
      "Epoch 457 of 500, Train Loss: 0.010558\n",
      "Epoch 458 of 500, Train Loss: 0.010553\n",
      "Epoch 459 of 500, Train Loss: 0.010531\n",
      "Epoch 460 of 500, Train Loss: 0.010524\n",
      "Epoch 461 of 500, Train Loss: 0.010511\n",
      "Epoch 462 of 500, Train Loss: 0.010513\n",
      "Epoch 463 of 500, Train Loss: 0.010507\n",
      "Epoch 464 of 500, Train Loss: 0.010520\n",
      "Epoch 465 of 500, Train Loss: 0.010519\n",
      "Epoch 466 of 500, Train Loss: 0.010539\n",
      "Epoch 467 of 500, Train Loss: 0.010542\n",
      "Epoch 468 of 500, Train Loss: 0.010555\n",
      "Epoch 469 of 500, Train Loss: 0.010561\n",
      "Epoch 470 of 500, Train Loss: 0.010558\n",
      "Epoch 471 of 500, Train Loss: 0.010548\n",
      "Epoch 472 of 500, Train Loss: 0.010548\n",
      "Epoch 473 of 500, Train Loss: 0.010521\n",
      "Epoch 474 of 500, Train Loss: 0.010533\n",
      "Epoch 475 of 500, Train Loss: 0.010507\n",
      "Epoch 476 of 500, Train Loss: 0.010535\n",
      "Epoch 477 of 500, Train Loss: 0.010513\n",
      "Epoch 478 of 500, Train Loss: 0.010563\n",
      "Epoch 479 of 500, Train Loss: 0.010544\n",
      "Epoch 480 of 500, Train Loss: 0.010604\n",
      "Epoch 481 of 500, Train Loss: 0.010583\n",
      "Epoch 482 of 500, Train Loss: 0.010618\n",
      "Epoch 483 of 500, Train Loss: 0.010584\n",
      "Epoch 484 of 500, Train Loss: 0.010599\n",
      "Epoch 485 of 500, Train Loss: 0.010555\n",
      "Epoch 486 of 500, Train Loss: 0.010579\n",
      "Epoch 487 of 500, Train Loss: 0.010542\n",
      "Epoch 488 of 500, Train Loss: 0.010567\n",
      "Epoch 489 of 500, Train Loss: 0.010539\n",
      "Epoch 490 of 500, Train Loss: 0.010554\n",
      "Epoch 491 of 500, Train Loss: 0.010525\n",
      "Epoch 492 of 500, Train Loss: 0.010537\n",
      "Epoch 493 of 500, Train Loss: 0.010506\n",
      "Epoch 494 of 500, Train Loss: 0.010523\n",
      "Epoch 495 of 500, Train Loss: 0.010493\n",
      "Epoch 496 of 500, Train Loss: 0.010512\n",
      "Epoch 497 of 500, Train Loss: 0.010484\n",
      "Epoch 498 of 500, Train Loss: 0.010503\n",
      "Epoch 499 of 500, Train Loss: 0.010482\n",
      "Epoch 500 of 500, Train Loss: 0.010502\n",
      "latent train shape:  (16395, 60)\n",
      "M: 60, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 55\n",
      "Training the subspace: 0 / 60\n",
      "Training the subspace: 1 / 60\n",
      "Training the subspace: 2 / 60\n",
      "Training the subspace: 3 / 60\n",
      "Training the subspace: 4 / 60\n",
      "Training the subspace: 5 / 60\n",
      "Training the subspace: 6 / 60\n",
      "Training the subspace: 7 / 60\n",
      "Training the subspace: 8 / 60\n",
      "Training the subspace: 9 / 60\n",
      "Training the subspace: 10 / 60\n",
      "Training the subspace: 11 / 60\n",
      "Training the subspace: 12 / 60\n",
      "Training the subspace: 13 / 60\n",
      "Training the subspace: 14 / 60\n",
      "Training the subspace: 15 / 60\n",
      "Training the subspace: 16 / 60\n",
      "Training the subspace: 17 / 60\n",
      "Training the subspace: 18 / 60\n",
      "Training the subspace: 19 / 60\n",
      "Training the subspace: 20 / 60\n",
      "Training the subspace: 21 / 60\n",
      "Training the subspace: 22 / 60\n",
      "Training the subspace: 23 / 60\n",
      "Training the subspace: 24 / 60\n",
      "Training the subspace: 25 / 60\n",
      "Training the subspace: 26 / 60\n",
      "Training the subspace: 27 / 60\n",
      "Training the subspace: 28 / 60\n",
      "Training the subspace: 29 / 60\n",
      "Training the subspace: 30 / 60\n",
      "Training the subspace: 31 / 60\n",
      "Training the subspace: 32 / 60\n",
      "Training the subspace: 33 / 60\n",
      "Training the subspace: 34 / 60\n",
      "Training the subspace: 35 / 60\n",
      "Training the subspace: 36 / 60\n",
      "Training the subspace: 37 / 60\n",
      "Training the subspace: 38 / 60\n",
      "Training the subspace: 39 / 60\n",
      "Training the subspace: 40 / 60\n",
      "Training the subspace: 41 / 60\n",
      "Training the subspace: 42 / 60\n",
      "Training the subspace: 43 / 60\n",
      "Training the subspace: 44 / 60\n",
      "Training the subspace: 45 / 60\n",
      "Training the subspace: 46 / 60\n",
      "Training the subspace: 47 / 60\n",
      "Training the subspace: 48 / 60\n",
      "Training the subspace: 49 / 60\n",
      "Training the subspace: 50 / 60\n",
      "Training the subspace: 51 / 60\n",
      "Training the subspace: 52 / 60\n",
      "Training the subspace: 53 / 60\n",
      "Training the subspace: 54 / 60\n",
      "Training the subspace: 55 / 60\n",
      "Training the subspace: 56 / 60\n",
      "Training the subspace: 57 / 60\n",
      "Training the subspace: 58 / 60\n",
      "Training the subspace: 59 / 60\n",
      "Encoding the subspace: 0 / 60\n",
      "Encoding the subspace: 1 / 60\n",
      "Encoding the subspace: 2 / 60\n",
      "Encoding the subspace: 3 / 60\n",
      "Encoding the subspace: 4 / 60\n",
      "Encoding the subspace: 5 / 60\n",
      "Encoding the subspace: 6 / 60\n",
      "Encoding the subspace: 7 / 60\n",
      "Encoding the subspace: 8 / 60\n",
      "Encoding the subspace: 9 / 60\n",
      "Encoding the subspace: 10 / 60\n",
      "Encoding the subspace: 11 / 60\n",
      "Encoding the subspace: 12 / 60\n",
      "Encoding the subspace: 13 / 60\n",
      "Encoding the subspace: 14 / 60\n",
      "Encoding the subspace: 15 / 60\n",
      "Encoding the subspace: 16 / 60\n",
      "Encoding the subspace: 17 / 60\n",
      "Encoding the subspace: 18 / 60\n",
      "Encoding the subspace: 19 / 60\n",
      "Encoding the subspace: 20 / 60\n",
      "Encoding the subspace: 21 / 60\n",
      "Encoding the subspace: 22 / 60\n",
      "Encoding the subspace: 23 / 60\n",
      "Encoding the subspace: 24 / 60\n",
      "Encoding the subspace: 25 / 60\n",
      "Encoding the subspace: 26 / 60\n",
      "Encoding the subspace: 27 / 60\n",
      "Encoding the subspace: 28 / 60\n",
      "Encoding the subspace: 29 / 60\n",
      "Encoding the subspace: 30 / 60\n",
      "Encoding the subspace: 31 / 60\n",
      "Encoding the subspace: 32 / 60\n",
      "Encoding the subspace: 33 / 60\n",
      "Encoding the subspace: 34 / 60\n",
      "Encoding the subspace: 35 / 60\n",
      "Encoding the subspace: 36 / 60\n",
      "Encoding the subspace: 37 / 60\n",
      "Encoding the subspace: 38 / 60\n",
      "Encoding the subspace: 39 / 60\n",
      "Encoding the subspace: 40 / 60\n",
      "Encoding the subspace: 41 / 60\n",
      "Encoding the subspace: 42 / 60\n",
      "Encoding the subspace: 43 / 60\n",
      "Encoding the subspace: 44 / 60\n",
      "Encoding the subspace: 45 / 60\n",
      "Encoding the subspace: 46 / 60\n",
      "Encoding the subspace: 47 / 60\n",
      "Encoding the subspace: 48 / 60\n",
      "Encoding the subspace: 49 / 60\n",
      "Encoding the subspace: 50 / 60\n",
      "Encoding the subspace: 51 / 60\n",
      "Encoding the subspace: 52 / 60\n",
      "Encoding the subspace: 53 / 60\n",
      "Encoding the subspace: 54 / 60\n",
      "Encoding the subspace: 55 / 60\n",
      "Encoding the subspace: 56 / 60\n",
      "Encoding the subspace: 57 / 60\n",
      "Encoding the subspace: 58 / 60\n",
      "Encoding the subspace: 59 / 60\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=90, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.321632\n",
      "best loss:  0.32163157997032005\n",
      "Epoch 2 of 500, Train Loss: 0.053025\n",
      "best loss:  0.05302522304357759\n",
      "Epoch 3 of 500, Train Loss: 0.046321\n",
      "best loss:  0.04632119010356002\n",
      "Epoch 4 of 500, Train Loss: 0.037740\n",
      "best loss:  0.037740225550382894\n",
      "Epoch 5 of 500, Train Loss: 0.031153\n",
      "best loss:  0.031153296672309524\n",
      "Epoch 6 of 500, Train Loss: 0.027142\n",
      "best loss:  0.027141616412604503\n",
      "Epoch 7 of 500, Train Loss: 0.024616\n",
      "best loss:  0.02461623793148414\n",
      "Epoch 8 of 500, Train Loss: 0.022718\n",
      "best loss:  0.022718179700246383\n",
      "Epoch 9 of 500, Train Loss: 0.021177\n",
      "best loss:  0.021176599919554135\n",
      "Epoch 10 of 500, Train Loss: 0.019905\n",
      "best loss:  0.0199051594335492\n",
      "Epoch 11 of 500, Train Loss: 0.018865\n",
      "best loss:  0.018865289428955406\n",
      "Epoch 12 of 500, Train Loss: 0.017965\n",
      "best loss:  0.017964706994194262\n",
      "Epoch 13 of 500, Train Loss: 0.017172\n",
      "best loss:  0.01717154686939325\n",
      "Epoch 14 of 500, Train Loss: 0.016448\n",
      "best loss:  0.016447708394212373\n",
      "Epoch 15 of 500, Train Loss: 0.015780\n",
      "best loss:  0.01577965080870727\n",
      "Epoch 16 of 500, Train Loss: 0.015152\n",
      "best loss:  0.015152131166902926\n",
      "Epoch 17 of 500, Train Loss: 0.014581\n",
      "best loss:  0.014580925106469815\n",
      "Epoch 18 of 500, Train Loss: 0.014074\n",
      "best loss:  0.014074233150319016\n",
      "Epoch 19 of 500, Train Loss: 0.013597\n",
      "best loss:  0.01359718281355174\n",
      "Epoch 20 of 500, Train Loss: 0.013175\n",
      "best loss:  0.013175453763281884\n",
      "Epoch 21 of 500, Train Loss: 0.012809\n",
      "best loss:  0.012809489839241024\n",
      "Epoch 22 of 500, Train Loss: 0.012481\n",
      "best loss:  0.012481481710885645\n",
      "Epoch 23 of 500, Train Loss: 0.012182\n",
      "best loss:  0.01218193079423441\n",
      "Epoch 24 of 500, Train Loss: 0.011914\n",
      "best loss:  0.011913958875789098\n",
      "Epoch 25 of 500, Train Loss: 0.011677\n",
      "best loss:  0.011676705732279233\n",
      "Epoch 26 of 500, Train Loss: 0.011456\n",
      "best loss:  0.011456483554739376\n",
      "Epoch 27 of 500, Train Loss: 0.011253\n",
      "best loss:  0.011253161385633745\n",
      "Epoch 28 of 500, Train Loss: 0.011062\n",
      "best loss:  0.01106166819226928\n",
      "Epoch 29 of 500, Train Loss: 0.010889\n",
      "best loss:  0.010888793007804572\n",
      "Epoch 30 of 500, Train Loss: 0.010727\n",
      "best loss:  0.01072665540724379\n",
      "Epoch 31 of 500, Train Loss: 0.010588\n",
      "best loss:  0.010587680665234666\n",
      "Epoch 32 of 500, Train Loss: 0.010441\n",
      "best loss:  0.010441094337869616\n",
      "Epoch 33 of 500, Train Loss: 0.010323\n",
      "best loss:  0.010323405579520325\n",
      "Epoch 34 of 500, Train Loss: 0.010189\n",
      "best loss:  0.010189144125356259\n",
      "Epoch 35 of 500, Train Loss: 0.010076\n",
      "best loss:  0.01007640138803169\n",
      "Epoch 36 of 500, Train Loss: 0.009978\n",
      "best loss:  0.009977670685029878\n",
      "Epoch 37 of 500, Train Loss: 0.009890\n",
      "best loss:  0.009890194944387604\n",
      "Epoch 38 of 500, Train Loss: 0.009793\n",
      "best loss:  0.009792746753565418\n",
      "Epoch 39 of 500, Train Loss: 0.009729\n",
      "best loss:  0.00972866477483573\n",
      "Epoch 40 of 500, Train Loss: 0.009644\n",
      "best loss:  0.009644052354427588\n",
      "Epoch 41 of 500, Train Loss: 0.009578\n",
      "best loss:  0.009577841116110939\n",
      "Epoch 42 of 500, Train Loss: 0.009524\n",
      "best loss:  0.009523833040468388\n",
      "Epoch 43 of 500, Train Loss: 0.009457\n",
      "best loss:  0.00945701825007428\n",
      "Epoch 44 of 500, Train Loss: 0.009404\n",
      "best loss:  0.009403577709052525\n",
      "Epoch 45 of 500, Train Loss: 0.009355\n",
      "best loss:  0.009354531527866876\n",
      "Epoch 46 of 500, Train Loss: 0.009300\n",
      "best loss:  0.009300274918823279\n",
      "Epoch 47 of 500, Train Loss: 0.009251\n",
      "best loss:  0.009251405043904705\n",
      "Epoch 48 of 500, Train Loss: 0.009219\n",
      "best loss:  0.009218598041383823\n",
      "Epoch 49 of 500, Train Loss: 0.009188\n",
      "best loss:  0.009187807788004803\n",
      "Epoch 50 of 500, Train Loss: 0.009151\n",
      "best loss:  0.009150636457416676\n",
      "Epoch 51 of 500, Train Loss: 0.009133\n",
      "best loss:  0.00913282205976091\n",
      "Epoch 52 of 500, Train Loss: 0.009093\n",
      "best loss:  0.009092753505570698\n",
      "Epoch 53 of 500, Train Loss: 0.009064\n",
      "best loss:  0.009064353371779939\n",
      "Epoch 54 of 500, Train Loss: 0.009049\n",
      "best loss:  0.009049335653016473\n",
      "Epoch 55 of 500, Train Loss: 0.009010\n",
      "best loss:  0.009009557646088295\n",
      "Epoch 56 of 500, Train Loss: 0.008973\n",
      "best loss:  0.008972737000300369\n",
      "Epoch 57 of 500, Train Loss: 0.008946\n",
      "best loss:  0.008945502988635851\n",
      "Epoch 58 of 500, Train Loss: 0.008921\n",
      "best loss:  0.008920908462127144\n",
      "Epoch 59 of 500, Train Loss: 0.008898\n",
      "best loss:  0.008897580023310796\n",
      "Epoch 60 of 500, Train Loss: 0.008871\n",
      "best loss:  0.008870925284586413\n",
      "Epoch 61 of 500, Train Loss: 0.008850\n",
      "best loss:  0.008850034275214014\n",
      "Epoch 62 of 500, Train Loss: 0.008830\n",
      "best loss:  0.008830438813717147\n",
      "Epoch 63 of 500, Train Loss: 0.008818\n",
      "best loss:  0.008818149513180563\n",
      "Epoch 64 of 500, Train Loss: 0.008802\n",
      "best loss:  0.008802412365889899\n",
      "Epoch 65 of 500, Train Loss: 0.008787\n",
      "best loss:  0.008786691694413023\n",
      "Epoch 66 of 500, Train Loss: 0.008770\n",
      "best loss:  0.008769767118848918\n",
      "Epoch 67 of 500, Train Loss: 0.008768\n",
      "best loss:  0.008767514497825927\n",
      "Epoch 68 of 500, Train Loss: 0.008761\n",
      "best loss:  0.008760500247608292\n",
      "Epoch 69 of 500, Train Loss: 0.008772\n",
      "Epoch 70 of 500, Train Loss: 0.008777\n",
      "Epoch 71 of 500, Train Loss: 0.008780\n",
      "Epoch 72 of 500, Train Loss: 0.008774\n",
      "Epoch 73 of 500, Train Loss: 0.008764\n",
      "Epoch 74 of 500, Train Loss: 0.008743\n",
      "best loss:  0.008743282815652662\n",
      "Epoch 75 of 500, Train Loss: 0.008729\n",
      "best loss:  0.008729172847278834\n",
      "Epoch 76 of 500, Train Loss: 0.008713\n",
      "best loss:  0.008712904673076935\n",
      "Epoch 77 of 500, Train Loss: 0.008707\n",
      "best loss:  0.008706518541841023\n",
      "Epoch 78 of 500, Train Loss: 0.008696\n",
      "best loss:  0.00869627316676842\n",
      "Epoch 79 of 500, Train Loss: 0.008691\n",
      "best loss:  0.008690624237701283\n",
      "Epoch 80 of 500, Train Loss: 0.008683\n",
      "best loss:  0.008683301700534508\n",
      "Epoch 81 of 500, Train Loss: 0.008678\n",
      "best loss:  0.008677973778144088\n",
      "Epoch 82 of 500, Train Loss: 0.008669\n",
      "best loss:  0.00866925845506092\n",
      "Epoch 83 of 500, Train Loss: 0.008664\n",
      "best loss:  0.008663933485130015\n",
      "Epoch 84 of 500, Train Loss: 0.008654\n",
      "best loss:  0.008653768573124178\n",
      "Epoch 85 of 500, Train Loss: 0.008652\n",
      "best loss:  0.008652037356135458\n",
      "Epoch 86 of 500, Train Loss: 0.008643\n",
      "best loss:  0.008643219658862186\n",
      "Epoch 87 of 500, Train Loss: 0.008643\n",
      "best loss:  0.008642749060414616\n",
      "Epoch 88 of 500, Train Loss: 0.008638\n",
      "best loss:  0.008638132706486671\n",
      "Epoch 89 of 500, Train Loss: 0.008643\n",
      "Epoch 90 of 500, Train Loss: 0.008641\n",
      "Epoch 91 of 500, Train Loss: 0.008650\n",
      "Epoch 92 of 500, Train Loss: 0.008650\n",
      "Epoch 93 of 500, Train Loss: 0.008660\n",
      "Epoch 94 of 500, Train Loss: 0.008660\n",
      "Epoch 95 of 500, Train Loss: 0.008667\n",
      "Epoch 96 of 500, Train Loss: 0.008665\n",
      "Epoch 97 of 500, Train Loss: 0.008669\n",
      "Epoch 98 of 500, Train Loss: 0.008667\n",
      "Epoch 99 of 500, Train Loss: 0.008669\n",
      "Epoch 100 of 500, Train Loss: 0.008667\n",
      "Epoch 101 of 500, Train Loss: 0.008668\n",
      "Epoch 102 of 500, Train Loss: 0.008664\n",
      "Epoch 103 of 500, Train Loss: 0.008664\n",
      "Epoch 104 of 500, Train Loss: 0.008660\n",
      "Epoch 105 of 500, Train Loss: 0.008661\n",
      "Epoch 106 of 500, Train Loss: 0.008661\n",
      "Epoch 107 of 500, Train Loss: 0.008664\n",
      "Epoch 108 of 500, Train Loss: 0.008662\n",
      "Epoch 109 of 500, Train Loss: 0.008664\n",
      "Epoch 110 of 500, Train Loss: 0.008660\n",
      "Epoch 111 of 500, Train Loss: 0.008665\n",
      "Epoch 112 of 500, Train Loss: 0.008657\n",
      "Epoch 113 of 500, Train Loss: 0.008657\n",
      "Epoch 114 of 500, Train Loss: 0.008647\n",
      "Epoch 115 of 500, Train Loss: 0.008656\n",
      "Epoch 116 of 500, Train Loss: 0.008649\n",
      "Epoch 117 of 500, Train Loss: 0.008661\n",
      "Epoch 118 of 500, Train Loss: 0.008652\n",
      "Epoch 119 of 500, Train Loss: 0.008672\n",
      "Epoch 120 of 500, Train Loss: 0.008657\n",
      "Epoch 121 of 500, Train Loss: 0.008672\n",
      "Epoch 122 of 500, Train Loss: 0.008645\n",
      "Epoch 123 of 500, Train Loss: 0.008664\n",
      "Epoch 124 of 500, Train Loss: 0.008634\n",
      "best loss:  0.008633950677368241\n",
      "Epoch 125 of 500, Train Loss: 0.008677\n",
      "Epoch 126 of 500, Train Loss: 0.008650\n",
      "Epoch 127 of 500, Train Loss: 0.008722\n",
      "Epoch 128 of 500, Train Loss: 0.008683\n",
      "Epoch 129 of 500, Train Loss: 0.008747\n",
      "Epoch 130 of 500, Train Loss: 0.008686\n",
      "Epoch 131 of 500, Train Loss: 0.008726\n",
      "Epoch 132 of 500, Train Loss: 0.008679\n",
      "Epoch 133 of 500, Train Loss: 0.008709\n",
      "Epoch 134 of 500, Train Loss: 0.008689\n",
      "Epoch 135 of 500, Train Loss: 0.008699\n",
      "Epoch 136 of 500, Train Loss: 0.008676\n",
      "Epoch 137 of 500, Train Loss: 0.008675\n",
      "Epoch 138 of 500, Train Loss: 0.008654\n",
      "Epoch 139 of 500, Train Loss: 0.008674\n",
      "Epoch 140 of 500, Train Loss: 0.008660\n",
      "Epoch 141 of 500, Train Loss: 0.008694\n",
      "Epoch 142 of 500, Train Loss: 0.008678\n",
      "Epoch 143 of 500, Train Loss: 0.008716\n",
      "Epoch 144 of 500, Train Loss: 0.008693\n",
      "Epoch 145 of 500, Train Loss: 0.008726\n",
      "Epoch 146 of 500, Train Loss: 0.008697\n",
      "Epoch 147 of 500, Train Loss: 0.008732\n",
      "Epoch 148 of 500, Train Loss: 0.008698\n",
      "Epoch 149 of 500, Train Loss: 0.008732\n",
      "Epoch 150 of 500, Train Loss: 0.008699\n",
      "Epoch 151 of 500, Train Loss: 0.008731\n",
      "Epoch 152 of 500, Train Loss: 0.008700\n",
      "Epoch 153 of 500, Train Loss: 0.008726\n",
      "Epoch 154 of 500, Train Loss: 0.008700\n",
      "Epoch 155 of 500, Train Loss: 0.008727\n",
      "Epoch 156 of 500, Train Loss: 0.008709\n",
      "Epoch 157 of 500, Train Loss: 0.008735\n",
      "Epoch 158 of 500, Train Loss: 0.008726\n",
      "Epoch 159 of 500, Train Loss: 0.008750\n",
      "Epoch 160 of 500, Train Loss: 0.008751\n",
      "Epoch 161 of 500, Train Loss: 0.008773\n",
      "Epoch 162 of 500, Train Loss: 0.008789\n",
      "Epoch 163 of 500, Train Loss: 0.008805\n",
      "Epoch 164 of 500, Train Loss: 0.008841\n",
      "Epoch 165 of 500, Train Loss: 0.008856\n",
      "Epoch 166 of 500, Train Loss: 0.008934\n",
      "Epoch 167 of 500, Train Loss: 0.008958\n",
      "Epoch 168 of 500, Train Loss: 0.009080\n",
      "Epoch 169 of 500, Train Loss: 0.009076\n",
      "Epoch 170 of 500, Train Loss: 0.009171\n",
      "Epoch 171 of 500, Train Loss: 0.009156\n",
      "Epoch 172 of 500, Train Loss: 0.009159\n",
      "Epoch 173 of 500, Train Loss: 0.009152\n",
      "Epoch 174 of 500, Train Loss: 0.009090\n",
      "Epoch 175 of 500, Train Loss: 0.009079\n",
      "Epoch 176 of 500, Train Loss: 0.009026\n",
      "Epoch 177 of 500, Train Loss: 0.009009\n",
      "Epoch 178 of 500, Train Loss: 0.008945\n",
      "Epoch 179 of 500, Train Loss: 0.008931\n",
      "Epoch 180 of 500, Train Loss: 0.008897\n",
      "Epoch 181 of 500, Train Loss: 0.008888\n",
      "Epoch 182 of 500, Train Loss: 0.008844\n",
      "Epoch 183 of 500, Train Loss: 0.008824\n",
      "Epoch 184 of 500, Train Loss: 0.008780\n",
      "Epoch 185 of 500, Train Loss: 0.008780\n",
      "Epoch 186 of 500, Train Loss: 0.008759\n",
      "Epoch 187 of 500, Train Loss: 0.008785\n",
      "Epoch 188 of 500, Train Loss: 0.008776\n",
      "Epoch 189 of 500, Train Loss: 0.008802\n",
      "Epoch 190 of 500, Train Loss: 0.008793\n",
      "Epoch 191 of 500, Train Loss: 0.008797\n",
      "Epoch 192 of 500, Train Loss: 0.008799\n",
      "Epoch 193 of 500, Train Loss: 0.008792\n",
      "Epoch 194 of 500, Train Loss: 0.008818\n",
      "Epoch 195 of 500, Train Loss: 0.008814\n",
      "Epoch 196 of 500, Train Loss: 0.008849\n",
      "Epoch 197 of 500, Train Loss: 0.008836\n",
      "Epoch 198 of 500, Train Loss: 0.008870\n",
      "Epoch 199 of 500, Train Loss: 0.008841\n",
      "Epoch 200 of 500, Train Loss: 0.008864\n",
      "Epoch 201 of 500, Train Loss: 0.008829\n",
      "Epoch 202 of 500, Train Loss: 0.008842\n",
      "Epoch 203 of 500, Train Loss: 0.008811\n",
      "Epoch 204 of 500, Train Loss: 0.008810\n",
      "Epoch 205 of 500, Train Loss: 0.008776\n",
      "Epoch 206 of 500, Train Loss: 0.008769\n",
      "Epoch 207 of 500, Train Loss: 0.008747\n",
      "Epoch 208 of 500, Train Loss: 0.008732\n",
      "Epoch 209 of 500, Train Loss: 0.008735\n",
      "Epoch 210 of 500, Train Loss: 0.008726\n",
      "Epoch 211 of 500, Train Loss: 0.008748\n",
      "Epoch 212 of 500, Train Loss: 0.008738\n",
      "Epoch 213 of 500, Train Loss: 0.008776\n",
      "Epoch 214 of 500, Train Loss: 0.008777\n",
      "Epoch 215 of 500, Train Loss: 0.008831\n",
      "Epoch 216 of 500, Train Loss: 0.008835\n",
      "Epoch 217 of 500, Train Loss: 0.008876\n",
      "Epoch 218 of 500, Train Loss: 0.008838\n",
      "Epoch 219 of 500, Train Loss: 0.008849\n",
      "Epoch 220 of 500, Train Loss: 0.008812\n",
      "Epoch 221 of 500, Train Loss: 0.008821\n",
      "Epoch 222 of 500, Train Loss: 0.008802\n",
      "Epoch 223 of 500, Train Loss: 0.008807\n",
      "Epoch 224 of 500, Train Loss: 0.008798\n",
      "Epoch 225 of 500, Train Loss: 0.008788\n",
      "Epoch 226 of 500, Train Loss: 0.008764\n",
      "Epoch 227 of 500, Train Loss: 0.008742\n",
      "Epoch 228 of 500, Train Loss: 0.008735\n",
      "Epoch 229 of 500, Train Loss: 0.008713\n",
      "Epoch 230 of 500, Train Loss: 0.008727\n",
      "Epoch 231 of 500, Train Loss: 0.008709\n",
      "Epoch 232 of 500, Train Loss: 0.008730\n",
      "Epoch 233 of 500, Train Loss: 0.008707\n",
      "Epoch 234 of 500, Train Loss: 0.008740\n",
      "Epoch 235 of 500, Train Loss: 0.008748\n",
      "Epoch 236 of 500, Train Loss: 0.008789\n",
      "Epoch 237 of 500, Train Loss: 0.008815\n",
      "Epoch 238 of 500, Train Loss: 0.008846\n",
      "Epoch 239 of 500, Train Loss: 0.008891\n",
      "Epoch 240 of 500, Train Loss: 0.008908\n",
      "Epoch 241 of 500, Train Loss: 0.008923\n",
      "Epoch 242 of 500, Train Loss: 0.008937\n",
      "Epoch 243 of 500, Train Loss: 0.008938\n",
      "Epoch 244 of 500, Train Loss: 0.008924\n",
      "Epoch 245 of 500, Train Loss: 0.008917\n",
      "Epoch 246 of 500, Train Loss: 0.008876\n",
      "Epoch 247 of 500, Train Loss: 0.008841\n",
      "Epoch 248 of 500, Train Loss: 0.008828\n",
      "Epoch 249 of 500, Train Loss: 0.008763\n",
      "Epoch 250 of 500, Train Loss: 0.008776\n",
      "Epoch 251 of 500, Train Loss: 0.008724\n",
      "Epoch 252 of 500, Train Loss: 0.008727\n",
      "Epoch 253 of 500, Train Loss: 0.008678\n",
      "Epoch 254 of 500, Train Loss: 0.008671\n",
      "Epoch 255 of 500, Train Loss: 0.008660\n",
      "Epoch 256 of 500, Train Loss: 0.008696\n",
      "Epoch 257 of 500, Train Loss: 0.008707\n",
      "Epoch 258 of 500, Train Loss: 0.008739\n",
      "Epoch 259 of 500, Train Loss: 0.008713\n",
      "Epoch 260 of 500, Train Loss: 0.008714\n",
      "Epoch 261 of 500, Train Loss: 0.008669\n",
      "Epoch 262 of 500, Train Loss: 0.008680\n",
      "Epoch 263 of 500, Train Loss: 0.008639\n",
      "Epoch 264 of 500, Train Loss: 0.008656\n",
      "Epoch 265 of 500, Train Loss: 0.008637\n",
      "Epoch 266 of 500, Train Loss: 0.008669\n",
      "Epoch 267 of 500, Train Loss: 0.008695\n",
      "Epoch 268 of 500, Train Loss: 0.008750\n",
      "Epoch 269 of 500, Train Loss: 0.008816\n",
      "Epoch 270 of 500, Train Loss: 0.008858\n",
      "Epoch 271 of 500, Train Loss: 0.008838\n",
      "Epoch 272 of 500, Train Loss: 0.008799\n",
      "Epoch 273 of 500, Train Loss: 0.008724\n",
      "Epoch 274 of 500, Train Loss: 0.008719\n",
      "Epoch 275 of 500, Train Loss: 0.008698\n",
      "Epoch 276 of 500, Train Loss: 0.008724\n",
      "Epoch 277 of 500, Train Loss: 0.008673\n",
      "Epoch 278 of 500, Train Loss: 0.008682\n",
      "Epoch 279 of 500, Train Loss: 0.008635\n",
      "Epoch 280 of 500, Train Loss: 0.008646\n",
      "Epoch 281 of 500, Train Loss: 0.008630\n",
      "best loss:  0.008629795432459859\n",
      "Epoch 282 of 500, Train Loss: 0.008642\n",
      "Epoch 283 of 500, Train Loss: 0.008625\n",
      "best loss:  0.008624919901814947\n",
      "Epoch 284 of 500, Train Loss: 0.008639\n",
      "Epoch 285 of 500, Train Loss: 0.008642\n",
      "Epoch 286 of 500, Train Loss: 0.008659\n",
      "Epoch 287 of 500, Train Loss: 0.008666\n",
      "Epoch 288 of 500, Train Loss: 0.008661\n",
      "Epoch 289 of 500, Train Loss: 0.008664\n",
      "Epoch 290 of 500, Train Loss: 0.008639\n",
      "Epoch 291 of 500, Train Loss: 0.008645\n",
      "Epoch 292 of 500, Train Loss: 0.008642\n",
      "Epoch 293 of 500, Train Loss: 0.008640\n",
      "Epoch 294 of 500, Train Loss: 0.008640\n",
      "Epoch 295 of 500, Train Loss: 0.008622\n",
      "best loss:  0.008621918431901944\n",
      "Epoch 296 of 500, Train Loss: 0.008631\n",
      "Epoch 297 of 500, Train Loss: 0.008639\n",
      "Epoch 298 of 500, Train Loss: 0.008660\n",
      "Epoch 299 of 500, Train Loss: 0.008668\n",
      "Epoch 300 of 500, Train Loss: 0.008698\n",
      "Epoch 301 of 500, Train Loss: 0.008692\n",
      "Epoch 302 of 500, Train Loss: 0.008714\n",
      "Epoch 303 of 500, Train Loss: 0.008667\n",
      "Epoch 304 of 500, Train Loss: 0.008643\n",
      "Epoch 305 of 500, Train Loss: 0.008594\n",
      "best loss:  0.008593868049445504\n",
      "Epoch 306 of 500, Train Loss: 0.008603\n",
      "Epoch 307 of 500, Train Loss: 0.008599\n",
      "Epoch 308 of 500, Train Loss: 0.008634\n",
      "Epoch 309 of 500, Train Loss: 0.008643\n",
      "Epoch 310 of 500, Train Loss: 0.008671\n",
      "Epoch 311 of 500, Train Loss: 0.008696\n",
      "Epoch 312 of 500, Train Loss: 0.008716\n",
      "Epoch 313 of 500, Train Loss: 0.008732\n",
      "Epoch 314 of 500, Train Loss: 0.008744\n",
      "Epoch 315 of 500, Train Loss: 0.008760\n",
      "Epoch 316 of 500, Train Loss: 0.008761\n",
      "Epoch 317 of 500, Train Loss: 0.008769\n",
      "Epoch 318 of 500, Train Loss: 0.008754\n",
      "Epoch 319 of 500, Train Loss: 0.008762\n",
      "Epoch 320 of 500, Train Loss: 0.008746\n",
      "Epoch 321 of 500, Train Loss: 0.008751\n",
      "Epoch 322 of 500, Train Loss: 0.008724\n",
      "Epoch 323 of 500, Train Loss: 0.008708\n",
      "Epoch 324 of 500, Train Loss: 0.008679\n",
      "Epoch 325 of 500, Train Loss: 0.008661\n",
      "Epoch 326 of 500, Train Loss: 0.008653\n",
      "Epoch 327 of 500, Train Loss: 0.008656\n",
      "Epoch 328 of 500, Train Loss: 0.008660\n",
      "Epoch 329 of 500, Train Loss: 0.008653\n",
      "Epoch 330 of 500, Train Loss: 0.008667\n",
      "Epoch 331 of 500, Train Loss: 0.008672\n",
      "Epoch 332 of 500, Train Loss: 0.008683\n",
      "Epoch 333 of 500, Train Loss: 0.008666\n",
      "Epoch 334 of 500, Train Loss: 0.008669\n",
      "Epoch 335 of 500, Train Loss: 0.008651\n",
      "Epoch 336 of 500, Train Loss: 0.008657\n",
      "Epoch 337 of 500, Train Loss: 0.008644\n",
      "Epoch 338 of 500, Train Loss: 0.008660\n",
      "Epoch 339 of 500, Train Loss: 0.008648\n",
      "Epoch 340 of 500, Train Loss: 0.008658\n",
      "Epoch 341 of 500, Train Loss: 0.008640\n",
      "Epoch 342 of 500, Train Loss: 0.008633\n",
      "Epoch 343 of 500, Train Loss: 0.008609\n",
      "Epoch 344 of 500, Train Loss: 0.008611\n",
      "Epoch 345 of 500, Train Loss: 0.008603\n",
      "Epoch 346 of 500, Train Loss: 0.008610\n",
      "Epoch 347 of 500, Train Loss: 0.008606\n",
      "Epoch 348 of 500, Train Loss: 0.008605\n",
      "Epoch 349 of 500, Train Loss: 0.008609\n",
      "Epoch 350 of 500, Train Loss: 0.008611\n",
      "Epoch 351 of 500, Train Loss: 0.008622\n",
      "Epoch 352 of 500, Train Loss: 0.008635\n",
      "Epoch 353 of 500, Train Loss: 0.008663\n",
      "Epoch 354 of 500, Train Loss: 0.008703\n",
      "Epoch 355 of 500, Train Loss: 0.008738\n",
      "Epoch 356 of 500, Train Loss: 0.008798\n",
      "Epoch 357 of 500, Train Loss: 0.008826\n",
      "Epoch 358 of 500, Train Loss: 0.008836\n",
      "Epoch 359 of 500, Train Loss: 0.008832\n",
      "Epoch 360 of 500, Train Loss: 0.008830\n",
      "Epoch 361 of 500, Train Loss: 0.008833\n",
      "Epoch 362 of 500, Train Loss: 0.008832\n",
      "Epoch 363 of 500, Train Loss: 0.008819\n",
      "Epoch 364 of 500, Train Loss: 0.008786\n",
      "Epoch 365 of 500, Train Loss: 0.008770\n",
      "Epoch 366 of 500, Train Loss: 0.008738\n",
      "Epoch 367 of 500, Train Loss: 0.008744\n",
      "Epoch 368 of 500, Train Loss: 0.008737\n",
      "Epoch 369 of 500, Train Loss: 0.008714\n",
      "Epoch 370 of 500, Train Loss: 0.008688\n",
      "Epoch 371 of 500, Train Loss: 0.008657\n",
      "Epoch 372 of 500, Train Loss: 0.008643\n",
      "Epoch 373 of 500, Train Loss: 0.008633\n",
      "Epoch 374 of 500, Train Loss: 0.008649\n",
      "Epoch 375 of 500, Train Loss: 0.008644\n",
      "Epoch 376 of 500, Train Loss: 0.008647\n",
      "Epoch 377 of 500, Train Loss: 0.008633\n",
      "Epoch 378 of 500, Train Loss: 0.008625\n",
      "Epoch 379 of 500, Train Loss: 0.008619\n",
      "Epoch 380 of 500, Train Loss: 0.008626\n",
      "Epoch 381 of 500, Train Loss: 0.008635\n",
      "Epoch 382 of 500, Train Loss: 0.008661\n",
      "Epoch 383 of 500, Train Loss: 0.008693\n",
      "Epoch 384 of 500, Train Loss: 0.008720\n",
      "Epoch 385 of 500, Train Loss: 0.008792\n",
      "Epoch 386 of 500, Train Loss: 0.008830\n",
      "Epoch 387 of 500, Train Loss: 0.008912\n",
      "Epoch 388 of 500, Train Loss: 0.008945\n",
      "Epoch 389 of 500, Train Loss: 0.008960\n",
      "Epoch 390 of 500, Train Loss: 0.008990\n",
      "Epoch 391 of 500, Train Loss: 0.008954\n",
      "Epoch 392 of 500, Train Loss: 0.008989\n",
      "Epoch 393 of 500, Train Loss: 0.008940\n",
      "Epoch 394 of 500, Train Loss: 0.008954\n",
      "Epoch 395 of 500, Train Loss: 0.008897\n",
      "Epoch 396 of 500, Train Loss: 0.008905\n",
      "Epoch 397 of 500, Train Loss: 0.008863\n",
      "Epoch 398 of 500, Train Loss: 0.008832\n",
      "Epoch 399 of 500, Train Loss: 0.008759\n",
      "Epoch 400 of 500, Train Loss: 0.008721\n",
      "Epoch 401 of 500, Train Loss: 0.008685\n",
      "Epoch 402 of 500, Train Loss: 0.008675\n",
      "Epoch 403 of 500, Train Loss: 0.008666\n",
      "Epoch 404 of 500, Train Loss: 0.008673\n",
      "Epoch 405 of 500, Train Loss: 0.008673\n",
      "Epoch 406 of 500, Train Loss: 0.008691\n",
      "Epoch 407 of 500, Train Loss: 0.008677\n",
      "Epoch 408 of 500, Train Loss: 0.008697\n",
      "Epoch 409 of 500, Train Loss: 0.008667\n",
      "Epoch 410 of 500, Train Loss: 0.008695\n",
      "Epoch 411 of 500, Train Loss: 0.008674\n",
      "Epoch 412 of 500, Train Loss: 0.008714\n",
      "Epoch 413 of 500, Train Loss: 0.008686\n",
      "Epoch 414 of 500, Train Loss: 0.008732\n",
      "Epoch 415 of 500, Train Loss: 0.008693\n",
      "Epoch 416 of 500, Train Loss: 0.008768\n",
      "Epoch 417 of 500, Train Loss: 0.008730\n",
      "Epoch 418 of 500, Train Loss: 0.008839\n",
      "Epoch 419 of 500, Train Loss: 0.008810\n",
      "Epoch 420 of 500, Train Loss: 0.008857\n",
      "Epoch 421 of 500, Train Loss: 0.008828\n",
      "Epoch 422 of 500, Train Loss: 0.008844\n",
      "Epoch 423 of 500, Train Loss: 0.008815\n",
      "Epoch 424 of 500, Train Loss: 0.008814\n",
      "Epoch 425 of 500, Train Loss: 0.008768\n",
      "Epoch 426 of 500, Train Loss: 0.008773\n",
      "Epoch 427 of 500, Train Loss: 0.008734\n",
      "Epoch 428 of 500, Train Loss: 0.008730\n",
      "Epoch 429 of 500, Train Loss: 0.008696\n",
      "Epoch 430 of 500, Train Loss: 0.008700\n",
      "Epoch 431 of 500, Train Loss: 0.008668\n",
      "Epoch 432 of 500, Train Loss: 0.008681\n",
      "Epoch 433 of 500, Train Loss: 0.008657\n",
      "Epoch 434 of 500, Train Loss: 0.008675\n",
      "Epoch 435 of 500, Train Loss: 0.008649\n",
      "Epoch 436 of 500, Train Loss: 0.008662\n",
      "Epoch 437 of 500, Train Loss: 0.008636\n",
      "Epoch 438 of 500, Train Loss: 0.008646\n",
      "Epoch 439 of 500, Train Loss: 0.008627\n",
      "Epoch 440 of 500, Train Loss: 0.008631\n",
      "Epoch 441 of 500, Train Loss: 0.008620\n",
      "Epoch 442 of 500, Train Loss: 0.008635\n",
      "Epoch 443 of 500, Train Loss: 0.008633\n",
      "Epoch 444 of 500, Train Loss: 0.008648\n",
      "Epoch 445 of 500, Train Loss: 0.008651\n",
      "Epoch 446 of 500, Train Loss: 0.008654\n",
      "Epoch 447 of 500, Train Loss: 0.008657\n",
      "Epoch 448 of 500, Train Loss: 0.008661\n",
      "Epoch 449 of 500, Train Loss: 0.008658\n",
      "Epoch 450 of 500, Train Loss: 0.008659\n",
      "Epoch 451 of 500, Train Loss: 0.008648\n",
      "Epoch 452 of 500, Train Loss: 0.008655\n",
      "Epoch 453 of 500, Train Loss: 0.008646\n",
      "Epoch 454 of 500, Train Loss: 0.008656\n",
      "Epoch 455 of 500, Train Loss: 0.008637\n",
      "Epoch 456 of 500, Train Loss: 0.008637\n",
      "Epoch 457 of 500, Train Loss: 0.008613\n",
      "Epoch 458 of 500, Train Loss: 0.008626\n",
      "Epoch 459 of 500, Train Loss: 0.008606\n",
      "Epoch 460 of 500, Train Loss: 0.008616\n",
      "Epoch 461 of 500, Train Loss: 0.008588\n",
      "best loss:  0.008588446222869282\n",
      "Epoch 462 of 500, Train Loss: 0.008602\n",
      "Epoch 463 of 500, Train Loss: 0.008585\n",
      "best loss:  0.008585076976762435\n",
      "Epoch 464 of 500, Train Loss: 0.008627\n",
      "Epoch 465 of 500, Train Loss: 0.008620\n",
      "Epoch 466 of 500, Train Loss: 0.008649\n",
      "Epoch 467 of 500, Train Loss: 0.008631\n",
      "Epoch 468 of 500, Train Loss: 0.008653\n",
      "Epoch 469 of 500, Train Loss: 0.008652\n",
      "Epoch 470 of 500, Train Loss: 0.008693\n",
      "Epoch 471 of 500, Train Loss: 0.008704\n",
      "Epoch 472 of 500, Train Loss: 0.008707\n",
      "Epoch 473 of 500, Train Loss: 0.008707\n",
      "Epoch 474 of 500, Train Loss: 0.008716\n",
      "Epoch 475 of 500, Train Loss: 0.008742\n",
      "Epoch 476 of 500, Train Loss: 0.008714\n",
      "Epoch 477 of 500, Train Loss: 0.008703\n",
      "Epoch 478 of 500, Train Loss: 0.008670\n",
      "Epoch 479 of 500, Train Loss: 0.008661\n",
      "Epoch 480 of 500, Train Loss: 0.008654\n",
      "Epoch 481 of 500, Train Loss: 0.008642\n",
      "Epoch 482 of 500, Train Loss: 0.008608\n",
      "Epoch 483 of 500, Train Loss: 0.008588\n",
      "Epoch 484 of 500, Train Loss: 0.008591\n",
      "Epoch 485 of 500, Train Loss: 0.008583\n",
      "best loss:  0.00858340281309549\n",
      "Epoch 486 of 500, Train Loss: 0.008570\n",
      "best loss:  0.008569721189709271\n",
      "Epoch 487 of 500, Train Loss: 0.008556\n",
      "best loss:  0.008556070902800845\n",
      "Epoch 488 of 500, Train Loss: 0.008559\n",
      "Epoch 489 of 500, Train Loss: 0.008558\n",
      "Epoch 490 of 500, Train Loss: 0.008569\n",
      "Epoch 491 of 500, Train Loss: 0.008560\n",
      "Epoch 492 of 500, Train Loss: 0.008571\n",
      "Epoch 493 of 500, Train Loss: 0.008579\n",
      "Epoch 494 of 500, Train Loss: 0.008626\n",
      "Epoch 495 of 500, Train Loss: 0.008650\n",
      "Epoch 496 of 500, Train Loss: 0.008684\n",
      "Epoch 497 of 500, Train Loss: 0.008673\n",
      "Epoch 498 of 500, Train Loss: 0.008692\n",
      "Epoch 499 of 500, Train Loss: 0.008686\n",
      "Epoch 500 of 500, Train Loss: 0.008695\n",
      "latent train shape:  (16395, 90)\n",
      "M: 90, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 11\n",
      "Training the subspace: 0 / 90\n",
      "Training the subspace: 1 / 90\n",
      "Training the subspace: 2 / 90\n",
      "Training the subspace: 3 / 90\n",
      "Training the subspace: 4 / 90\n",
      "Training the subspace: 5 / 90\n",
      "Training the subspace: 6 / 90\n",
      "Training the subspace: 7 / 90\n",
      "Training the subspace: 8 / 90\n",
      "Training the subspace: 9 / 90\n",
      "Training the subspace: 10 / 90\n",
      "Training the subspace: 11 / 90\n",
      "Training the subspace: 12 / 90\n",
      "Training the subspace: 13 / 90\n",
      "Training the subspace: 14 / 90\n",
      "Training the subspace: 15 / 90\n",
      "Training the subspace: 16 / 90\n",
      "Training the subspace: 17 / 90\n",
      "Training the subspace: 18 / 90\n",
      "Training the subspace: 19 / 90\n",
      "Training the subspace: 20 / 90\n",
      "Training the subspace: 21 / 90\n",
      "Training the subspace: 22 / 90\n",
      "Training the subspace: 23 / 90\n",
      "Training the subspace: 24 / 90\n",
      "Training the subspace: 25 / 90\n",
      "Training the subspace: 26 / 90\n",
      "Training the subspace: 27 / 90\n",
      "Training the subspace: 28 / 90\n",
      "Training the subspace: 29 / 90\n",
      "Training the subspace: 30 / 90\n",
      "Training the subspace: 31 / 90\n",
      "Training the subspace: 32 / 90\n",
      "Training the subspace: 33 / 90\n",
      "Training the subspace: 34 / 90\n",
      "Training the subspace: 35 / 90\n",
      "Training the subspace: 36 / 90\n",
      "Training the subspace: 37 / 90\n",
      "Training the subspace: 38 / 90\n",
      "Training the subspace: 39 / 90\n",
      "Training the subspace: 40 / 90\n",
      "Training the subspace: 41 / 90\n",
      "Training the subspace: 42 / 90\n",
      "Training the subspace: 43 / 90\n",
      "Training the subspace: 44 / 90\n",
      "Training the subspace: 45 / 90\n",
      "Training the subspace: 46 / 90\n",
      "Training the subspace: 47 / 90\n",
      "Training the subspace: 48 / 90\n",
      "Training the subspace: 49 / 90\n",
      "Training the subspace: 50 / 90\n",
      "Training the subspace: 51 / 90\n",
      "Training the subspace: 52 / 90\n",
      "Training the subspace: 53 / 90\n",
      "Training the subspace: 54 / 90\n",
      "Training the subspace: 55 / 90\n",
      "Training the subspace: 56 / 90\n",
      "Training the subspace: 57 / 90\n",
      "Training the subspace: 58 / 90\n",
      "Training the subspace: 59 / 90\n",
      "Training the subspace: 60 / 90\n",
      "Training the subspace: 61 / 90\n",
      "Training the subspace: 62 / 90\n",
      "Training the subspace: 63 / 90\n",
      "Training the subspace: 64 / 90\n",
      "Training the subspace: 65 / 90\n",
      "Training the subspace: 66 / 90\n",
      "Training the subspace: 67 / 90\n",
      "Training the subspace: 68 / 90\n",
      "Training the subspace: 69 / 90\n",
      "Training the subspace: 70 / 90\n",
      "Training the subspace: 71 / 90\n",
      "Training the subspace: 72 / 90\n",
      "Training the subspace: 73 / 90\n",
      "Training the subspace: 74 / 90\n",
      "Training the subspace: 75 / 90\n",
      "Training the subspace: 76 / 90\n",
      "Training the subspace: 77 / 90\n",
      "Training the subspace: 78 / 90\n",
      "Training the subspace: 79 / 90\n",
      "Training the subspace: 80 / 90\n",
      "Training the subspace: 81 / 90\n",
      "Training the subspace: 82 / 90\n",
      "Training the subspace: 83 / 90\n",
      "Training the subspace: 84 / 90\n",
      "Training the subspace: 85 / 90\n",
      "Training the subspace: 86 / 90\n",
      "Training the subspace: 87 / 90\n",
      "Training the subspace: 88 / 90\n",
      "Training the subspace: 89 / 90\n",
      "Encoding the subspace: 0 / 90\n",
      "Encoding the subspace: 1 / 90\n",
      "Encoding the subspace: 2 / 90\n",
      "Encoding the subspace: 3 / 90\n",
      "Encoding the subspace: 4 / 90\n",
      "Encoding the subspace: 5 / 90\n",
      "Encoding the subspace: 6 / 90\n",
      "Encoding the subspace: 7 / 90\n",
      "Encoding the subspace: 8 / 90\n",
      "Encoding the subspace: 9 / 90\n",
      "Encoding the subspace: 10 / 90\n",
      "Encoding the subspace: 11 / 90\n",
      "Encoding the subspace: 12 / 90\n",
      "Encoding the subspace: 13 / 90\n",
      "Encoding the subspace: 14 / 90\n",
      "Encoding the subspace: 15 / 90\n",
      "Encoding the subspace: 16 / 90\n",
      "Encoding the subspace: 17 / 90\n",
      "Encoding the subspace: 18 / 90\n",
      "Encoding the subspace: 19 / 90\n",
      "Encoding the subspace: 20 / 90\n",
      "Encoding the subspace: 21 / 90\n",
      "Encoding the subspace: 22 / 90\n",
      "Encoding the subspace: 23 / 90\n",
      "Encoding the subspace: 24 / 90\n",
      "Encoding the subspace: 25 / 90\n",
      "Encoding the subspace: 26 / 90\n",
      "Encoding the subspace: 27 / 90\n",
      "Encoding the subspace: 28 / 90\n",
      "Encoding the subspace: 29 / 90\n",
      "Encoding the subspace: 30 / 90\n",
      "Encoding the subspace: 31 / 90\n",
      "Encoding the subspace: 32 / 90\n",
      "Encoding the subspace: 33 / 90\n",
      "Encoding the subspace: 34 / 90\n",
      "Encoding the subspace: 35 / 90\n",
      "Encoding the subspace: 36 / 90\n",
      "Encoding the subspace: 37 / 90\n",
      "Encoding the subspace: 38 / 90\n",
      "Encoding the subspace: 39 / 90\n",
      "Encoding the subspace: 40 / 90\n",
      "Encoding the subspace: 41 / 90\n",
      "Encoding the subspace: 42 / 90\n",
      "Encoding the subspace: 43 / 90\n",
      "Encoding the subspace: 44 / 90\n",
      "Encoding the subspace: 45 / 90\n",
      "Encoding the subspace: 46 / 90\n",
      "Encoding the subspace: 47 / 90\n",
      "Encoding the subspace: 48 / 90\n",
      "Encoding the subspace: 49 / 90\n",
      "Encoding the subspace: 50 / 90\n",
      "Encoding the subspace: 51 / 90\n",
      "Encoding the subspace: 52 / 90\n",
      "Encoding the subspace: 53 / 90\n",
      "Encoding the subspace: 54 / 90\n",
      "Encoding the subspace: 55 / 90\n",
      "Encoding the subspace: 56 / 90\n",
      "Encoding the subspace: 57 / 90\n",
      "Encoding the subspace: 58 / 90\n",
      "Encoding the subspace: 59 / 90\n",
      "Encoding the subspace: 60 / 90\n",
      "Encoding the subspace: 61 / 90\n",
      "Encoding the subspace: 62 / 90\n",
      "Encoding the subspace: 63 / 90\n",
      "Encoding the subspace: 64 / 90\n",
      "Encoding the subspace: 65 / 90\n",
      "Encoding the subspace: 66 / 90\n",
      "Encoding the subspace: 67 / 90\n",
      "Encoding the subspace: 68 / 90\n",
      "Encoding the subspace: 69 / 90\n",
      "Encoding the subspace: 70 / 90\n",
      "Encoding the subspace: 71 / 90\n",
      "Encoding the subspace: 72 / 90\n",
      "Encoding the subspace: 73 / 90\n",
      "Encoding the subspace: 74 / 90\n",
      "Encoding the subspace: 75 / 90\n",
      "Encoding the subspace: 76 / 90\n",
      "Encoding the subspace: 77 / 90\n",
      "Encoding the subspace: 78 / 90\n",
      "Encoding the subspace: 79 / 90\n",
      "Encoding the subspace: 80 / 90\n",
      "Encoding the subspace: 81 / 90\n",
      "Encoding the subspace: 82 / 90\n",
      "Encoding the subspace: 83 / 90\n",
      "Encoding the subspace: 84 / 90\n",
      "Encoding the subspace: 85 / 90\n",
      "Encoding the subspace: 86 / 90\n",
      "Encoding the subspace: 87 / 90\n",
      "Encoding the subspace: 88 / 90\n",
      "Encoding the subspace: 89 / 90\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=120, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.271161\n",
      "best loss:  0.27116103507867156\n",
      "Epoch 2 of 500, Train Loss: 0.049643\n",
      "best loss:  0.049642813995267564\n",
      "Epoch 3 of 500, Train Loss: 0.039712\n",
      "best loss:  0.03971247019087391\n",
      "Epoch 4 of 500, Train Loss: 0.031471\n",
      "best loss:  0.031470616246111575\n",
      "Epoch 5 of 500, Train Loss: 0.026861\n",
      "best loss:  0.026860531086513643\n",
      "Epoch 6 of 500, Train Loss: 0.024032\n",
      "best loss:  0.024031617793763532\n",
      "Epoch 7 of 500, Train Loss: 0.021881\n",
      "best loss:  0.021880570106486277\n",
      "Epoch 8 of 500, Train Loss: 0.020180\n",
      "best loss:  0.020180068306837867\n",
      "Epoch 9 of 500, Train Loss: 0.018834\n",
      "best loss:  0.018833694173891257\n",
      "Epoch 10 of 500, Train Loss: 0.017730\n",
      "best loss:  0.017729699273924017\n",
      "Epoch 11 of 500, Train Loss: 0.016715\n",
      "best loss:  0.016714785631805614\n",
      "Epoch 12 of 500, Train Loss: 0.015830\n",
      "best loss:  0.01582999259879625\n",
      "Epoch 13 of 500, Train Loss: 0.015004\n",
      "best loss:  0.015003751272104623\n",
      "Epoch 14 of 500, Train Loss: 0.014286\n",
      "best loss:  0.014286039702823501\n",
      "Epoch 15 of 500, Train Loss: 0.013629\n",
      "best loss:  0.013628936672760102\n",
      "Epoch 16 of 500, Train Loss: 0.013026\n",
      "best loss:  0.013025725377469086\n",
      "Epoch 17 of 500, Train Loss: 0.012523\n",
      "best loss:  0.012523453726203997\n",
      "Epoch 18 of 500, Train Loss: 0.012052\n",
      "best loss:  0.012052012236550624\n",
      "Epoch 19 of 500, Train Loss: 0.011643\n",
      "best loss:  0.011643308068196065\n",
      "Epoch 20 of 500, Train Loss: 0.011277\n",
      "best loss:  0.01127683948869853\n",
      "Epoch 21 of 500, Train Loss: 0.010961\n",
      "best loss:  0.010961074278962459\n",
      "Epoch 22 of 500, Train Loss: 0.010667\n",
      "best loss:  0.010667343543167406\n",
      "Epoch 23 of 500, Train Loss: 0.010405\n",
      "best loss:  0.01040548788019918\n",
      "Epoch 24 of 500, Train Loss: 0.010144\n",
      "best loss:  0.010143909654683838\n",
      "Epoch 25 of 500, Train Loss: 0.009915\n",
      "best loss:  0.00991537339819321\n",
      "Epoch 26 of 500, Train Loss: 0.009714\n",
      "best loss:  0.009713971250520861\n",
      "Epoch 27 of 500, Train Loss: 0.009506\n",
      "best loss:  0.00950595687489592\n",
      "Epoch 28 of 500, Train Loss: 0.009328\n",
      "best loss:  0.00932824949132885\n",
      "Epoch 29 of 500, Train Loss: 0.009190\n",
      "best loss:  0.009190376790000357\n",
      "Epoch 30 of 500, Train Loss: 0.009026\n",
      "best loss:  0.009026301574627978\n",
      "Epoch 31 of 500, Train Loss: 0.008907\n",
      "best loss:  0.008906661942077787\n",
      "Epoch 32 of 500, Train Loss: 0.008785\n",
      "best loss:  0.008784701420168406\n",
      "Epoch 33 of 500, Train Loss: 0.008712\n",
      "best loss:  0.008711584463427674\n",
      "Epoch 34 of 500, Train Loss: 0.008589\n",
      "best loss:  0.008589020119271559\n",
      "Epoch 35 of 500, Train Loss: 0.008496\n",
      "best loss:  0.008495938374666324\n",
      "Epoch 36 of 500, Train Loss: 0.008439\n",
      "best loss:  0.00843860058009249\n",
      "Epoch 37 of 500, Train Loss: 0.008343\n",
      "best loss:  0.008342565355985786\n",
      "Epoch 38 of 500, Train Loss: 0.008234\n",
      "best loss:  0.00823437454155917\n",
      "Epoch 39 of 500, Train Loss: 0.008195\n",
      "best loss:  0.00819496208855561\n",
      "Epoch 40 of 500, Train Loss: 0.008133\n",
      "best loss:  0.008133436267330969\n",
      "Epoch 41 of 500, Train Loss: 0.008075\n",
      "best loss:  0.00807475031192447\n",
      "Epoch 42 of 500, Train Loss: 0.008033\n",
      "best loss:  0.008032849817238964\n",
      "Epoch 43 of 500, Train Loss: 0.007977\n",
      "best loss:  0.00797714280098128\n",
      "Epoch 44 of 500, Train Loss: 0.007918\n",
      "best loss:  0.00791758374634105\n",
      "Epoch 45 of 500, Train Loss: 0.007878\n",
      "best loss:  0.007878402233924248\n",
      "Epoch 46 of 500, Train Loss: 0.007833\n",
      "best loss:  0.007833259482619844\n",
      "Epoch 47 of 500, Train Loss: 0.007808\n",
      "best loss:  0.007808421678771626\n",
      "Epoch 48 of 500, Train Loss: 0.007752\n",
      "best loss:  0.007752191558048895\n",
      "Epoch 49 of 500, Train Loss: 0.007715\n",
      "best loss:  0.0077147992906953\n",
      "Epoch 50 of 500, Train Loss: 0.007694\n",
      "best loss:  0.007694201628982573\n",
      "Epoch 51 of 500, Train Loss: 0.007643\n",
      "best loss:  0.0076430758561527486\n",
      "Epoch 52 of 500, Train Loss: 0.007621\n",
      "best loss:  0.00762124604948475\n",
      "Epoch 53 of 500, Train Loss: 0.007588\n",
      "best loss:  0.007588128326235108\n",
      "Epoch 54 of 500, Train Loss: 0.007553\n",
      "best loss:  0.0075530499497847275\n",
      "Epoch 55 of 500, Train Loss: 0.007534\n",
      "best loss:  0.007534440228595514\n",
      "Epoch 56 of 500, Train Loss: 0.007518\n",
      "best loss:  0.007518284005887243\n",
      "Epoch 57 of 500, Train Loss: 0.007485\n",
      "best loss:  0.007485439351244555\n",
      "Epoch 58 of 500, Train Loss: 0.007483\n",
      "best loss:  0.007483319464331861\n",
      "Epoch 59 of 500, Train Loss: 0.007460\n",
      "best loss:  0.0074601454622845\n",
      "Epoch 60 of 500, Train Loss: 0.007450\n",
      "best loss:  0.007449965066476833\n",
      "Epoch 61 of 500, Train Loss: 0.007445\n",
      "best loss:  0.007445200286397725\n",
      "Epoch 62 of 500, Train Loss: 0.007442\n",
      "best loss:  0.007441837621704754\n",
      "Epoch 63 of 500, Train Loss: 0.007432\n",
      "best loss:  0.007432304896786989\n",
      "Epoch 64 of 500, Train Loss: 0.007433\n",
      "Epoch 65 of 500, Train Loss: 0.007420\n",
      "best loss:  0.007419749120225076\n",
      "Epoch 66 of 500, Train Loss: 0.007451\n",
      "Epoch 67 of 500, Train Loss: 0.007467\n",
      "Epoch 68 of 500, Train Loss: 0.007471\n",
      "Epoch 69 of 500, Train Loss: 0.007439\n",
      "Epoch 70 of 500, Train Loss: 0.007415\n",
      "best loss:  0.0074148754954298365\n",
      "Epoch 71 of 500, Train Loss: 0.007373\n",
      "best loss:  0.007373362722424898\n",
      "Epoch 72 of 500, Train Loss: 0.007356\n",
      "best loss:  0.007356204795845163\n",
      "Epoch 73 of 500, Train Loss: 0.007311\n",
      "best loss:  0.0073110503436728605\n",
      "Epoch 74 of 500, Train Loss: 0.007298\n",
      "best loss:  0.007298024422367277\n",
      "Epoch 75 of 500, Train Loss: 0.007276\n",
      "best loss:  0.007276264812671868\n",
      "Epoch 76 of 500, Train Loss: 0.007290\n",
      "Epoch 77 of 500, Train Loss: 0.007295\n",
      "Epoch 78 of 500, Train Loss: 0.007331\n",
      "Epoch 79 of 500, Train Loss: 0.007334\n",
      "Epoch 80 of 500, Train Loss: 0.007343\n",
      "Epoch 81 of 500, Train Loss: 0.007322\n",
      "Epoch 82 of 500, Train Loss: 0.007305\n",
      "Epoch 83 of 500, Train Loss: 0.007269\n",
      "best loss:  0.007269473068570437\n",
      "Epoch 84 of 500, Train Loss: 0.007254\n",
      "best loss:  0.007253941219474203\n",
      "Epoch 85 of 500, Train Loss: 0.007232\n",
      "best loss:  0.007232175744567319\n",
      "Epoch 86 of 500, Train Loss: 0.007237\n",
      "Epoch 87 of 500, Train Loss: 0.007234\n",
      "Epoch 88 of 500, Train Loss: 0.007258\n",
      "Epoch 89 of 500, Train Loss: 0.007277\n",
      "Epoch 90 of 500, Train Loss: 0.007341\n",
      "Epoch 91 of 500, Train Loss: 0.007389\n",
      "Epoch 92 of 500, Train Loss: 0.007480\n",
      "Epoch 93 of 500, Train Loss: 0.007494\n",
      "Epoch 94 of 500, Train Loss: 0.007496\n",
      "Epoch 95 of 500, Train Loss: 0.007442\n",
      "Epoch 96 of 500, Train Loss: 0.007448\n",
      "Epoch 97 of 500, Train Loss: 0.007437\n",
      "Epoch 98 of 500, Train Loss: 0.007448\n",
      "Epoch 99 of 500, Train Loss: 0.007438\n",
      "Epoch 100 of 500, Train Loss: 0.007427\n",
      "Epoch 101 of 500, Train Loss: 0.007397\n",
      "Epoch 102 of 500, Train Loss: 0.007384\n",
      "Epoch 103 of 500, Train Loss: 0.007365\n",
      "Epoch 104 of 500, Train Loss: 0.007385\n",
      "Epoch 105 of 500, Train Loss: 0.007404\n",
      "Epoch 106 of 500, Train Loss: 0.007470\n",
      "Epoch 107 of 500, Train Loss: 0.007519\n",
      "Epoch 108 of 500, Train Loss: 0.007598\n",
      "Epoch 109 of 500, Train Loss: 0.007692\n",
      "Epoch 110 of 500, Train Loss: 0.007727\n",
      "Epoch 111 of 500, Train Loss: 0.007891\n",
      "Epoch 112 of 500, Train Loss: 0.007928\n",
      "Epoch 113 of 500, Train Loss: 0.008142\n",
      "Epoch 114 of 500, Train Loss: 0.008118\n",
      "Epoch 115 of 500, Train Loss: 0.008205\n",
      "Epoch 116 of 500, Train Loss: 0.008126\n",
      "Epoch 117 of 500, Train Loss: 0.007961\n",
      "Epoch 118 of 500, Train Loss: 0.007898\n",
      "Epoch 119 of 500, Train Loss: 0.007719\n",
      "Epoch 120 of 500, Train Loss: 0.007689\n",
      "Epoch 121 of 500, Train Loss: 0.007633\n",
      "Epoch 122 of 500, Train Loss: 0.007625\n",
      "Epoch 123 of 500, Train Loss: 0.007583\n",
      "Epoch 124 of 500, Train Loss: 0.007603\n",
      "Epoch 125 of 500, Train Loss: 0.007562\n",
      "Epoch 126 of 500, Train Loss: 0.007590\n",
      "Epoch 127 of 500, Train Loss: 0.007568\n",
      "Epoch 128 of 500, Train Loss: 0.007559\n",
      "Epoch 129 of 500, Train Loss: 0.007558\n",
      "Epoch 130 of 500, Train Loss: 0.007554\n",
      "Epoch 131 of 500, Train Loss: 0.007574\n",
      "Epoch 132 of 500, Train Loss: 0.007579\n",
      "Epoch 133 of 500, Train Loss: 0.007565\n",
      "Epoch 134 of 500, Train Loss: 0.007561\n",
      "Epoch 135 of 500, Train Loss: 0.007525\n",
      "Epoch 136 of 500, Train Loss: 0.007516\n",
      "Epoch 137 of 500, Train Loss: 0.007495\n",
      "Epoch 138 of 500, Train Loss: 0.007484\n",
      "Epoch 139 of 500, Train Loss: 0.007488\n",
      "Epoch 140 of 500, Train Loss: 0.007480\n",
      "Epoch 141 of 500, Train Loss: 0.007494\n",
      "Epoch 142 of 500, Train Loss: 0.007480\n",
      "Epoch 143 of 500, Train Loss: 0.007504\n",
      "Epoch 144 of 500, Train Loss: 0.007525\n",
      "Epoch 145 of 500, Train Loss: 0.007527\n",
      "Epoch 146 of 500, Train Loss: 0.007534\n",
      "Epoch 147 of 500, Train Loss: 0.007551\n",
      "Epoch 148 of 500, Train Loss: 0.007566\n",
      "Epoch 149 of 500, Train Loss: 0.007584\n",
      "Epoch 150 of 500, Train Loss: 0.007607\n",
      "Epoch 151 of 500, Train Loss: 0.007602\n",
      "Epoch 152 of 500, Train Loss: 0.007617\n",
      "Epoch 153 of 500, Train Loss: 0.007569\n",
      "Epoch 154 of 500, Train Loss: 0.007564\n",
      "Epoch 155 of 500, Train Loss: 0.007510\n",
      "Epoch 156 of 500, Train Loss: 0.007506\n",
      "Epoch 157 of 500, Train Loss: 0.007473\n",
      "Epoch 158 of 500, Train Loss: 0.007474\n",
      "Epoch 159 of 500, Train Loss: 0.007470\n",
      "Epoch 160 of 500, Train Loss: 0.007466\n",
      "Epoch 161 of 500, Train Loss: 0.007488\n",
      "Epoch 162 of 500, Train Loss: 0.007484\n",
      "Epoch 163 of 500, Train Loss: 0.007523\n",
      "Epoch 164 of 500, Train Loss: 0.007502\n",
      "Epoch 165 of 500, Train Loss: 0.007523\n",
      "Epoch 166 of 500, Train Loss: 0.007479\n",
      "Epoch 167 of 500, Train Loss: 0.007485\n",
      "Epoch 168 of 500, Train Loss: 0.007456\n",
      "Epoch 169 of 500, Train Loss: 0.007453\n",
      "Epoch 170 of 500, Train Loss: 0.007424\n",
      "Epoch 171 of 500, Train Loss: 0.007425\n",
      "Epoch 172 of 500, Train Loss: 0.007407\n",
      "Epoch 173 of 500, Train Loss: 0.007419\n",
      "Epoch 174 of 500, Train Loss: 0.007429\n",
      "Epoch 175 of 500, Train Loss: 0.007424\n",
      "Epoch 176 of 500, Train Loss: 0.007436\n",
      "Epoch 177 of 500, Train Loss: 0.007419\n",
      "Epoch 178 of 500, Train Loss: 0.007424\n",
      "Epoch 179 of 500, Train Loss: 0.007421\n",
      "Epoch 180 of 500, Train Loss: 0.007401\n",
      "Epoch 181 of 500, Train Loss: 0.007410\n",
      "Epoch 182 of 500, Train Loss: 0.007389\n",
      "Epoch 183 of 500, Train Loss: 0.007410\n",
      "Epoch 184 of 500, Train Loss: 0.007387\n",
      "Epoch 185 of 500, Train Loss: 0.007398\n",
      "Epoch 186 of 500, Train Loss: 0.007376\n",
      "Epoch 187 of 500, Train Loss: 0.007392\n",
      "Epoch 188 of 500, Train Loss: 0.007372\n",
      "Epoch 189 of 500, Train Loss: 0.007385\n",
      "Epoch 190 of 500, Train Loss: 0.007364\n",
      "Epoch 191 of 500, Train Loss: 0.007371\n",
      "Epoch 192 of 500, Train Loss: 0.007347\n",
      "Epoch 193 of 500, Train Loss: 0.007359\n",
      "Epoch 194 of 500, Train Loss: 0.007341\n",
      "Epoch 195 of 500, Train Loss: 0.007344\n",
      "Epoch 196 of 500, Train Loss: 0.007330\n",
      "Epoch 197 of 500, Train Loss: 0.007345\n",
      "Epoch 198 of 500, Train Loss: 0.007353\n",
      "Epoch 199 of 500, Train Loss: 0.007384\n",
      "Epoch 200 of 500, Train Loss: 0.007423\n",
      "Epoch 201 of 500, Train Loss: 0.007462\n",
      "Epoch 202 of 500, Train Loss: 0.007512\n",
      "Epoch 203 of 500, Train Loss: 0.007548\n",
      "Epoch 204 of 500, Train Loss: 0.007591\n",
      "Epoch 205 of 500, Train Loss: 0.007608\n",
      "Epoch 206 of 500, Train Loss: 0.007609\n",
      "Epoch 207 of 500, Train Loss: 0.007614\n",
      "Epoch 208 of 500, Train Loss: 0.007610\n",
      "Epoch 209 of 500, Train Loss: 0.007608\n",
      "Epoch 210 of 500, Train Loss: 0.007601\n",
      "Epoch 211 of 500, Train Loss: 0.007594\n",
      "Epoch 212 of 500, Train Loss: 0.007590\n",
      "Epoch 213 of 500, Train Loss: 0.007589\n",
      "Epoch 214 of 500, Train Loss: 0.007524\n",
      "Epoch 215 of 500, Train Loss: 0.007513\n",
      "Epoch 216 of 500, Train Loss: 0.007469\n",
      "Epoch 217 of 500, Train Loss: 0.007454\n",
      "Epoch 218 of 500, Train Loss: 0.007414\n",
      "Epoch 219 of 500, Train Loss: 0.007413\n",
      "Epoch 220 of 500, Train Loss: 0.007398\n",
      "Epoch 221 of 500, Train Loss: 0.007408\n",
      "Epoch 222 of 500, Train Loss: 0.007403\n",
      "Epoch 223 of 500, Train Loss: 0.007401\n",
      "Epoch 224 of 500, Train Loss: 0.007408\n",
      "Epoch 225 of 500, Train Loss: 0.007416\n",
      "Epoch 226 of 500, Train Loss: 0.007411\n",
      "Epoch 227 of 500, Train Loss: 0.007414\n",
      "Epoch 228 of 500, Train Loss: 0.007406\n",
      "Epoch 229 of 500, Train Loss: 0.007421\n",
      "Epoch 230 of 500, Train Loss: 0.007411\n",
      "Epoch 231 of 500, Train Loss: 0.007444\n",
      "Epoch 232 of 500, Train Loss: 0.007435\n",
      "Epoch 233 of 500, Train Loss: 0.007464\n",
      "Epoch 234 of 500, Train Loss: 0.007459\n",
      "Epoch 235 of 500, Train Loss: 0.007530\n",
      "Epoch 236 of 500, Train Loss: 0.007534\n",
      "Epoch 237 of 500, Train Loss: 0.007650\n",
      "Epoch 238 of 500, Train Loss: 0.007651\n",
      "Epoch 239 of 500, Train Loss: 0.007821\n",
      "Epoch 240 of 500, Train Loss: 0.007787\n",
      "Epoch 241 of 500, Train Loss: 0.007711\n",
      "Epoch 242 of 500, Train Loss: 0.007586\n",
      "Epoch 243 of 500, Train Loss: 0.007482\n",
      "Epoch 244 of 500, Train Loss: 0.007429\n",
      "Epoch 245 of 500, Train Loss: 0.007446\n",
      "Epoch 246 of 500, Train Loss: 0.007463\n",
      "Epoch 247 of 500, Train Loss: 0.007501\n",
      "Epoch 248 of 500, Train Loss: 0.007527\n",
      "Epoch 249 of 500, Train Loss: 0.007570\n",
      "Epoch 250 of 500, Train Loss: 0.007587\n",
      "Epoch 251 of 500, Train Loss: 0.007554\n",
      "Epoch 252 of 500, Train Loss: 0.007533\n",
      "Epoch 253 of 500, Train Loss: 0.007515\n",
      "Epoch 254 of 500, Train Loss: 0.007533\n",
      "Epoch 255 of 500, Train Loss: 0.007457\n",
      "Epoch 256 of 500, Train Loss: 0.007455\n",
      "Epoch 257 of 500, Train Loss: 0.007443\n",
      "Epoch 258 of 500, Train Loss: 0.007462\n",
      "Epoch 259 of 500, Train Loss: 0.007422\n",
      "Epoch 260 of 500, Train Loss: 0.007401\n",
      "Epoch 261 of 500, Train Loss: 0.007379\n",
      "Epoch 262 of 500, Train Loss: 0.007375\n",
      "Epoch 263 of 500, Train Loss: 0.007380\n",
      "Epoch 264 of 500, Train Loss: 0.007378\n",
      "Epoch 265 of 500, Train Loss: 0.007396\n",
      "Epoch 266 of 500, Train Loss: 0.007392\n",
      "Epoch 267 of 500, Train Loss: 0.007417\n",
      "Epoch 268 of 500, Train Loss: 0.007402\n",
      "Epoch 269 of 500, Train Loss: 0.007425\n",
      "Epoch 270 of 500, Train Loss: 0.007394\n",
      "Epoch 271 of 500, Train Loss: 0.007399\n",
      "Epoch 272 of 500, Train Loss: 0.007378\n",
      "Epoch 273 of 500, Train Loss: 0.007401\n",
      "Epoch 274 of 500, Train Loss: 0.007381\n",
      "Epoch 275 of 500, Train Loss: 0.007404\n",
      "Epoch 276 of 500, Train Loss: 0.007367\n",
      "Epoch 277 of 500, Train Loss: 0.007362\n",
      "Epoch 278 of 500, Train Loss: 0.007360\n",
      "Epoch 279 of 500, Train Loss: 0.007368\n",
      "Epoch 280 of 500, Train Loss: 0.007371\n",
      "Epoch 281 of 500, Train Loss: 0.007379\n",
      "Epoch 282 of 500, Train Loss: 0.007384\n",
      "Epoch 283 of 500, Train Loss: 0.007389\n",
      "Epoch 284 of 500, Train Loss: 0.007402\n",
      "Epoch 285 of 500, Train Loss: 0.007417\n",
      "Epoch 286 of 500, Train Loss: 0.007419\n",
      "Epoch 287 of 500, Train Loss: 0.007412\n",
      "Epoch 288 of 500, Train Loss: 0.007412\n",
      "Epoch 289 of 500, Train Loss: 0.007398\n",
      "Epoch 290 of 500, Train Loss: 0.007402\n",
      "Epoch 291 of 500, Train Loss: 0.007402\n",
      "Epoch 292 of 500, Train Loss: 0.007403\n",
      "Epoch 293 of 500, Train Loss: 0.007400\n",
      "Epoch 294 of 500, Train Loss: 0.007400\n",
      "Epoch 295 of 500, Train Loss: 0.007391\n",
      "Epoch 296 of 500, Train Loss: 0.007389\n",
      "Epoch 297 of 500, Train Loss: 0.007382\n",
      "Epoch 298 of 500, Train Loss: 0.007381\n",
      "Epoch 299 of 500, Train Loss: 0.007373\n",
      "Epoch 300 of 500, Train Loss: 0.007370\n",
      "Epoch 301 of 500, Train Loss: 0.007356\n",
      "Epoch 302 of 500, Train Loss: 0.007355\n",
      "Epoch 303 of 500, Train Loss: 0.007338\n",
      "Epoch 304 of 500, Train Loss: 0.007346\n",
      "Epoch 305 of 500, Train Loss: 0.007328\n",
      "Epoch 306 of 500, Train Loss: 0.007339\n",
      "Epoch 307 of 500, Train Loss: 0.007321\n",
      "Epoch 308 of 500, Train Loss: 0.007334\n",
      "Epoch 309 of 500, Train Loss: 0.007319\n",
      "Epoch 310 of 500, Train Loss: 0.007347\n",
      "Epoch 311 of 500, Train Loss: 0.007336\n",
      "Epoch 312 of 500, Train Loss: 0.007378\n",
      "Epoch 313 of 500, Train Loss: 0.007360\n",
      "Epoch 314 of 500, Train Loss: 0.007405\n",
      "Epoch 315 of 500, Train Loss: 0.007376\n",
      "Epoch 316 of 500, Train Loss: 0.007426\n",
      "Epoch 317 of 500, Train Loss: 0.007418\n",
      "Epoch 318 of 500, Train Loss: 0.007476\n",
      "Epoch 319 of 500, Train Loss: 0.007489\n",
      "Epoch 320 of 500, Train Loss: 0.007550\n",
      "Epoch 321 of 500, Train Loss: 0.007573\n",
      "Epoch 322 of 500, Train Loss: 0.007582\n",
      "Epoch 323 of 500, Train Loss: 0.007590\n",
      "Epoch 324 of 500, Train Loss: 0.007531\n",
      "Epoch 325 of 500, Train Loss: 0.007536\n",
      "Epoch 326 of 500, Train Loss: 0.007477\n",
      "Epoch 327 of 500, Train Loss: 0.007483\n",
      "Epoch 328 of 500, Train Loss: 0.007433\n",
      "Epoch 329 of 500, Train Loss: 0.007407\n",
      "Epoch 330 of 500, Train Loss: 0.007400\n",
      "Epoch 331 of 500, Train Loss: 0.007345\n",
      "Epoch 332 of 500, Train Loss: 0.007361\n",
      "Epoch 333 of 500, Train Loss: 0.007332\n",
      "Epoch 334 of 500, Train Loss: 0.007356\n",
      "Epoch 335 of 500, Train Loss: 0.007345\n",
      "Epoch 336 of 500, Train Loss: 0.007386\n",
      "Epoch 337 of 500, Train Loss: 0.007376\n",
      "Epoch 338 of 500, Train Loss: 0.007428\n",
      "Epoch 339 of 500, Train Loss: 0.007414\n",
      "Epoch 340 of 500, Train Loss: 0.007454\n",
      "Epoch 341 of 500, Train Loss: 0.007439\n",
      "Epoch 342 of 500, Train Loss: 0.007464\n",
      "Epoch 343 of 500, Train Loss: 0.007448\n",
      "Epoch 344 of 500, Train Loss: 0.007448\n",
      "Epoch 345 of 500, Train Loss: 0.007420\n",
      "Epoch 346 of 500, Train Loss: 0.007418\n",
      "Epoch 347 of 500, Train Loss: 0.007404\n",
      "Epoch 348 of 500, Train Loss: 0.007416\n",
      "Epoch 349 of 500, Train Loss: 0.007408\n",
      "Epoch 350 of 500, Train Loss: 0.007414\n",
      "Epoch 351 of 500, Train Loss: 0.007399\n",
      "Epoch 352 of 500, Train Loss: 0.007377\n",
      "Epoch 353 of 500, Train Loss: 0.007347\n",
      "Epoch 354 of 500, Train Loss: 0.007338\n",
      "Epoch 355 of 500, Train Loss: 0.007316\n",
      "Epoch 356 of 500, Train Loss: 0.007326\n",
      "Epoch 357 of 500, Train Loss: 0.007311\n",
      "Epoch 358 of 500, Train Loss: 0.007332\n",
      "Epoch 359 of 500, Train Loss: 0.007316\n",
      "Epoch 360 of 500, Train Loss: 0.007347\n",
      "Epoch 361 of 500, Train Loss: 0.007324\n",
      "Epoch 362 of 500, Train Loss: 0.007361\n",
      "Epoch 363 of 500, Train Loss: 0.007336\n",
      "Epoch 364 of 500, Train Loss: 0.007368\n",
      "Epoch 365 of 500, Train Loss: 0.007350\n",
      "Epoch 366 of 500, Train Loss: 0.007374\n",
      "Epoch 367 of 500, Train Loss: 0.007371\n",
      "Epoch 368 of 500, Train Loss: 0.007392\n",
      "Epoch 369 of 500, Train Loss: 0.007398\n",
      "Epoch 370 of 500, Train Loss: 0.007405\n",
      "Epoch 371 of 500, Train Loss: 0.007415\n",
      "Epoch 372 of 500, Train Loss: 0.007380\n",
      "Epoch 373 of 500, Train Loss: 0.007351\n",
      "Epoch 374 of 500, Train Loss: 0.007319\n",
      "Epoch 375 of 500, Train Loss: 0.007295\n",
      "Epoch 376 of 500, Train Loss: 0.007317\n",
      "Epoch 377 of 500, Train Loss: 0.007305\n",
      "Epoch 378 of 500, Train Loss: 0.007344\n",
      "Epoch 379 of 500, Train Loss: 0.007333\n",
      "Epoch 380 of 500, Train Loss: 0.007369\n",
      "Epoch 381 of 500, Train Loss: 0.007354\n",
      "Epoch 382 of 500, Train Loss: 0.007390\n",
      "Epoch 383 of 500, Train Loss: 0.007368\n",
      "Epoch 384 of 500, Train Loss: 0.007382\n",
      "Epoch 385 of 500, Train Loss: 0.007359\n",
      "Epoch 386 of 500, Train Loss: 0.007385\n",
      "Epoch 387 of 500, Train Loss: 0.007379\n",
      "Epoch 388 of 500, Train Loss: 0.007385\n",
      "Epoch 389 of 500, Train Loss: 0.007386\n",
      "Epoch 390 of 500, Train Loss: 0.007383\n",
      "Epoch 391 of 500, Train Loss: 0.007385\n",
      "Epoch 392 of 500, Train Loss: 0.007367\n",
      "Epoch 393 of 500, Train Loss: 0.007371\n",
      "Epoch 394 of 500, Train Loss: 0.007358\n",
      "Epoch 395 of 500, Train Loss: 0.007368\n",
      "Epoch 396 of 500, Train Loss: 0.007359\n",
      "Epoch 397 of 500, Train Loss: 0.007368\n",
      "Epoch 398 of 500, Train Loss: 0.007372\n",
      "Epoch 399 of 500, Train Loss: 0.007380\n",
      "Epoch 400 of 500, Train Loss: 0.007406\n",
      "Epoch 401 of 500, Train Loss: 0.007410\n",
      "Epoch 402 of 500, Train Loss: 0.007465\n",
      "Epoch 403 of 500, Train Loss: 0.007439\n",
      "Epoch 404 of 500, Train Loss: 0.007481\n",
      "Epoch 405 of 500, Train Loss: 0.007430\n",
      "Epoch 406 of 500, Train Loss: 0.007450\n",
      "Epoch 407 of 500, Train Loss: 0.007379\n",
      "Epoch 408 of 500, Train Loss: 0.007385\n",
      "Epoch 409 of 500, Train Loss: 0.007347\n",
      "Epoch 410 of 500, Train Loss: 0.007360\n",
      "Epoch 411 of 500, Train Loss: 0.007353\n",
      "Epoch 412 of 500, Train Loss: 0.007328\n",
      "Epoch 413 of 500, Train Loss: 0.007328\n",
      "Epoch 414 of 500, Train Loss: 0.007315\n",
      "Epoch 415 of 500, Train Loss: 0.007334\n",
      "Epoch 416 of 500, Train Loss: 0.007305\n",
      "Epoch 417 of 500, Train Loss: 0.007326\n",
      "Epoch 418 of 500, Train Loss: 0.007320\n",
      "Epoch 419 of 500, Train Loss: 0.007330\n",
      "Epoch 420 of 500, Train Loss: 0.007326\n",
      "Epoch 421 of 500, Train Loss: 0.007339\n",
      "Epoch 422 of 500, Train Loss: 0.007329\n",
      "Epoch 423 of 500, Train Loss: 0.007341\n",
      "Epoch 424 of 500, Train Loss: 0.007337\n",
      "Epoch 425 of 500, Train Loss: 0.007345\n",
      "Epoch 426 of 500, Train Loss: 0.007352\n",
      "Epoch 427 of 500, Train Loss: 0.007355\n",
      "Epoch 428 of 500, Train Loss: 0.007370\n",
      "Epoch 429 of 500, Train Loss: 0.007365\n",
      "Epoch 430 of 500, Train Loss: 0.007383\n",
      "Epoch 431 of 500, Train Loss: 0.007377\n",
      "Epoch 432 of 500, Train Loss: 0.007398\n",
      "Epoch 433 of 500, Train Loss: 0.007388\n",
      "Epoch 434 of 500, Train Loss: 0.007403\n",
      "Epoch 435 of 500, Train Loss: 0.007393\n",
      "Epoch 436 of 500, Train Loss: 0.007409\n",
      "Epoch 437 of 500, Train Loss: 0.007395\n",
      "Epoch 438 of 500, Train Loss: 0.007388\n",
      "Epoch 439 of 500, Train Loss: 0.007390\n",
      "Epoch 440 of 500, Train Loss: 0.007386\n",
      "Epoch 441 of 500, Train Loss: 0.007402\n",
      "Epoch 442 of 500, Train Loss: 0.007354\n",
      "Epoch 443 of 500, Train Loss: 0.007362\n",
      "Epoch 444 of 500, Train Loss: 0.007319\n",
      "Epoch 445 of 500, Train Loss: 0.007337\n",
      "Epoch 446 of 500, Train Loss: 0.007291\n",
      "Epoch 447 of 500, Train Loss: 0.007308\n",
      "Epoch 448 of 500, Train Loss: 0.007275\n",
      "Epoch 449 of 500, Train Loss: 0.007289\n",
      "Epoch 450 of 500, Train Loss: 0.007265\n",
      "Epoch 451 of 500, Train Loss: 0.007278\n",
      "Epoch 452 of 500, Train Loss: 0.007280\n",
      "Epoch 453 of 500, Train Loss: 0.007295\n",
      "Epoch 454 of 500, Train Loss: 0.007312\n",
      "Epoch 455 of 500, Train Loss: 0.007322\n",
      "Epoch 456 of 500, Train Loss: 0.007357\n",
      "Epoch 457 of 500, Train Loss: 0.007359\n",
      "Epoch 458 of 500, Train Loss: 0.007371\n",
      "Epoch 459 of 500, Train Loss: 0.007373\n",
      "Epoch 460 of 500, Train Loss: 0.007372\n",
      "Epoch 461 of 500, Train Loss: 0.007373\n",
      "Epoch 462 of 500, Train Loss: 0.007361\n",
      "Epoch 463 of 500, Train Loss: 0.007361\n",
      "Epoch 464 of 500, Train Loss: 0.007350\n",
      "Epoch 465 of 500, Train Loss: 0.007349\n",
      "Epoch 466 of 500, Train Loss: 0.007344\n",
      "Epoch 467 of 500, Train Loss: 0.007353\n",
      "Epoch 468 of 500, Train Loss: 0.007365\n",
      "Epoch 469 of 500, Train Loss: 0.007382\n",
      "Epoch 470 of 500, Train Loss: 0.007370\n",
      "Epoch 471 of 500, Train Loss: 0.007361\n",
      "Epoch 472 of 500, Train Loss: 0.007352\n",
      "Epoch 473 of 500, Train Loss: 0.007358\n",
      "Epoch 474 of 500, Train Loss: 0.007350\n",
      "Epoch 475 of 500, Train Loss: 0.007353\n",
      "Epoch 476 of 500, Train Loss: 0.007324\n",
      "Epoch 477 of 500, Train Loss: 0.007310\n",
      "Epoch 478 of 500, Train Loss: 0.007306\n",
      "Epoch 479 of 500, Train Loss: 0.007319\n",
      "Epoch 480 of 500, Train Loss: 0.007308\n",
      "Epoch 481 of 500, Train Loss: 0.007308\n",
      "Epoch 482 of 500, Train Loss: 0.007296\n",
      "Epoch 483 of 500, Train Loss: 0.007313\n",
      "Epoch 484 of 500, Train Loss: 0.007322\n",
      "Epoch 485 of 500, Train Loss: 0.007346\n",
      "Epoch 486 of 500, Train Loss: 0.007322\n",
      "Epoch 487 of 500, Train Loss: 0.007318\n",
      "Epoch 488 of 500, Train Loss: 0.007305\n",
      "Epoch 489 of 500, Train Loss: 0.007333\n",
      "Epoch 490 of 500, Train Loss: 0.007348\n",
      "Epoch 491 of 500, Train Loss: 0.007370\n",
      "Epoch 492 of 500, Train Loss: 0.007365\n",
      "Epoch 493 of 500, Train Loss: 0.007390\n",
      "Epoch 494 of 500, Train Loss: 0.007411\n",
      "Epoch 495 of 500, Train Loss: 0.007432\n",
      "Epoch 496 of 500, Train Loss: 0.007441\n",
      "Epoch 497 of 500, Train Loss: 0.007421\n",
      "Epoch 498 of 500, Train Loss: 0.007411\n",
      "Epoch 499 of 500, Train Loss: 0.007393\n",
      "Epoch 500 of 500, Train Loss: 0.007422\n",
      "latent train shape:  (16395, 120)\n",
      "M: 120, Ks: 16, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 66\n",
      "Training the subspace: 0 / 120\n",
      "Training the subspace: 1 / 120\n",
      "Training the subspace: 2 / 120\n",
      "Training the subspace: 3 / 120\n",
      "Training the subspace: 4 / 120\n",
      "Training the subspace: 5 / 120\n",
      "Training the subspace: 6 / 120\n",
      "Training the subspace: 7 / 120\n",
      "Training the subspace: 8 / 120\n",
      "Training the subspace: 9 / 120\n",
      "Training the subspace: 10 / 120\n",
      "Training the subspace: 11 / 120\n",
      "Training the subspace: 12 / 120\n",
      "Training the subspace: 13 / 120\n",
      "Training the subspace: 14 / 120\n",
      "Training the subspace: 15 / 120\n",
      "Training the subspace: 16 / 120\n",
      "Training the subspace: 17 / 120\n",
      "Training the subspace: 18 / 120\n",
      "Training the subspace: 19 / 120\n",
      "Training the subspace: 20 / 120\n",
      "Training the subspace: 21 / 120\n",
      "Training the subspace: 22 / 120\n",
      "Training the subspace: 23 / 120\n",
      "Training the subspace: 24 / 120\n",
      "Training the subspace: 25 / 120\n",
      "Training the subspace: 26 / 120\n",
      "Training the subspace: 27 / 120\n",
      "Training the subspace: 28 / 120\n",
      "Training the subspace: 29 / 120\n",
      "Training the subspace: 30 / 120\n",
      "Training the subspace: 31 / 120\n",
      "Training the subspace: 32 / 120\n",
      "Training the subspace: 33 / 120\n",
      "Training the subspace: 34 / 120\n",
      "Training the subspace: 35 / 120\n",
      "Training the subspace: 36 / 120\n",
      "Training the subspace: 37 / 120\n",
      "Training the subspace: 38 / 120\n",
      "Training the subspace: 39 / 120\n",
      "Training the subspace: 40 / 120\n",
      "Training the subspace: 41 / 120\n",
      "Training the subspace: 42 / 120\n",
      "Training the subspace: 43 / 120\n",
      "Training the subspace: 44 / 120\n",
      "Training the subspace: 45 / 120\n",
      "Training the subspace: 46 / 120\n",
      "Training the subspace: 47 / 120\n",
      "Training the subspace: 48 / 120\n",
      "Training the subspace: 49 / 120\n",
      "Training the subspace: 50 / 120\n",
      "Training the subspace: 51 / 120\n",
      "Training the subspace: 52 / 120\n",
      "Training the subspace: 53 / 120\n",
      "Training the subspace: 54 / 120\n",
      "Training the subspace: 55 / 120\n",
      "Training the subspace: 56 / 120\n",
      "Training the subspace: 57 / 120\n",
      "Training the subspace: 58 / 120\n",
      "Training the subspace: 59 / 120\n",
      "Training the subspace: 60 / 120\n",
      "Training the subspace: 61 / 120\n",
      "Training the subspace: 62 / 120\n",
      "Training the subspace: 63 / 120\n",
      "Training the subspace: 64 / 120\n",
      "Training the subspace: 65 / 120\n",
      "Training the subspace: 66 / 120\n",
      "Training the subspace: 67 / 120\n",
      "Training the subspace: 68 / 120\n",
      "Training the subspace: 69 / 120\n",
      "Training the subspace: 70 / 120\n",
      "Training the subspace: 71 / 120\n",
      "Training the subspace: 72 / 120\n",
      "Training the subspace: 73 / 120\n",
      "Training the subspace: 74 / 120\n",
      "Training the subspace: 75 / 120\n",
      "Training the subspace: 76 / 120\n",
      "Training the subspace: 77 / 120\n",
      "Training the subspace: 78 / 120\n",
      "Training the subspace: 79 / 120\n",
      "Training the subspace: 80 / 120\n",
      "Training the subspace: 81 / 120\n",
      "Training the subspace: 82 / 120\n",
      "Training the subspace: 83 / 120\n",
      "Training the subspace: 84 / 120\n",
      "Training the subspace: 85 / 120\n",
      "Training the subspace: 86 / 120\n",
      "Training the subspace: 87 / 120\n",
      "Training the subspace: 88 / 120\n",
      "Training the subspace: 89 / 120\n",
      "Training the subspace: 90 / 120\n",
      "Training the subspace: 91 / 120\n",
      "Training the subspace: 92 / 120\n",
      "Training the subspace: 93 / 120\n",
      "Training the subspace: 94 / 120\n",
      "Training the subspace: 95 / 120\n",
      "Training the subspace: 96 / 120\n",
      "Training the subspace: 97 / 120\n",
      "Training the subspace: 98 / 120\n",
      "Training the subspace: 99 / 120\n",
      "Training the subspace: 100 / 120\n",
      "Training the subspace: 101 / 120\n",
      "Training the subspace: 102 / 120\n",
      "Training the subspace: 103 / 120\n",
      "Training the subspace: 104 / 120\n",
      "Training the subspace: 105 / 120\n",
      "Training the subspace: 106 / 120\n",
      "Training the subspace: 107 / 120\n",
      "Training the subspace: 108 / 120\n",
      "Training the subspace: 109 / 120\n",
      "Training the subspace: 110 / 120\n",
      "Training the subspace: 111 / 120\n",
      "Training the subspace: 112 / 120\n",
      "Training the subspace: 113 / 120\n",
      "Training the subspace: 114 / 120\n",
      "Training the subspace: 115 / 120\n",
      "Training the subspace: 116 / 120\n",
      "Training the subspace: 117 / 120\n",
      "Training the subspace: 118 / 120\n",
      "Training the subspace: 119 / 120\n",
      "Encoding the subspace: 0 / 120\n",
      "Encoding the subspace: 1 / 120\n",
      "Encoding the subspace: 2 / 120\n",
      "Encoding the subspace: 3 / 120\n",
      "Encoding the subspace: 4 / 120\n",
      "Encoding the subspace: 5 / 120\n",
      "Encoding the subspace: 6 / 120\n",
      "Encoding the subspace: 7 / 120\n",
      "Encoding the subspace: 8 / 120\n",
      "Encoding the subspace: 9 / 120\n",
      "Encoding the subspace: 10 / 120\n",
      "Encoding the subspace: 11 / 120\n",
      "Encoding the subspace: 12 / 120\n",
      "Encoding the subspace: 13 / 120\n",
      "Encoding the subspace: 14 / 120\n",
      "Encoding the subspace: 15 / 120\n",
      "Encoding the subspace: 16 / 120\n",
      "Encoding the subspace: 17 / 120\n",
      "Encoding the subspace: 18 / 120\n",
      "Encoding the subspace: 19 / 120\n",
      "Encoding the subspace: 20 / 120\n",
      "Encoding the subspace: 21 / 120\n",
      "Encoding the subspace: 22 / 120\n",
      "Encoding the subspace: 23 / 120\n",
      "Encoding the subspace: 24 / 120\n",
      "Encoding the subspace: 25 / 120\n",
      "Encoding the subspace: 26 / 120\n",
      "Encoding the subspace: 27 / 120\n",
      "Encoding the subspace: 28 / 120\n",
      "Encoding the subspace: 29 / 120\n",
      "Encoding the subspace: 30 / 120\n",
      "Encoding the subspace: 31 / 120\n",
      "Encoding the subspace: 32 / 120\n",
      "Encoding the subspace: 33 / 120\n",
      "Encoding the subspace: 34 / 120\n",
      "Encoding the subspace: 35 / 120\n",
      "Encoding the subspace: 36 / 120\n",
      "Encoding the subspace: 37 / 120\n",
      "Encoding the subspace: 38 / 120\n",
      "Encoding the subspace: 39 / 120\n",
      "Encoding the subspace: 40 / 120\n",
      "Encoding the subspace: 41 / 120\n",
      "Encoding the subspace: 42 / 120\n",
      "Encoding the subspace: 43 / 120\n",
      "Encoding the subspace: 44 / 120\n",
      "Encoding the subspace: 45 / 120\n",
      "Encoding the subspace: 46 / 120\n",
      "Encoding the subspace: 47 / 120\n",
      "Encoding the subspace: 48 / 120\n",
      "Encoding the subspace: 49 / 120\n",
      "Encoding the subspace: 50 / 120\n",
      "Encoding the subspace: 51 / 120\n",
      "Encoding the subspace: 52 / 120\n",
      "Encoding the subspace: 53 / 120\n",
      "Encoding the subspace: 54 / 120\n",
      "Encoding the subspace: 55 / 120\n",
      "Encoding the subspace: 56 / 120\n",
      "Encoding the subspace: 57 / 120\n",
      "Encoding the subspace: 58 / 120\n",
      "Encoding the subspace: 59 / 120\n",
      "Encoding the subspace: 60 / 120\n",
      "Encoding the subspace: 61 / 120\n",
      "Encoding the subspace: 62 / 120\n",
      "Encoding the subspace: 63 / 120\n",
      "Encoding the subspace: 64 / 120\n",
      "Encoding the subspace: 65 / 120\n",
      "Encoding the subspace: 66 / 120\n",
      "Encoding the subspace: 67 / 120\n",
      "Encoding the subspace: 68 / 120\n",
      "Encoding the subspace: 69 / 120\n",
      "Encoding the subspace: 70 / 120\n",
      "Encoding the subspace: 71 / 120\n",
      "Encoding the subspace: 72 / 120\n",
      "Encoding the subspace: 73 / 120\n",
      "Encoding the subspace: 74 / 120\n",
      "Encoding the subspace: 75 / 120\n",
      "Encoding the subspace: 76 / 120\n",
      "Encoding the subspace: 77 / 120\n",
      "Encoding the subspace: 78 / 120\n",
      "Encoding the subspace: 79 / 120\n",
      "Encoding the subspace: 80 / 120\n",
      "Encoding the subspace: 81 / 120\n",
      "Encoding the subspace: 82 / 120\n",
      "Encoding the subspace: 83 / 120\n",
      "Encoding the subspace: 84 / 120\n",
      "Encoding the subspace: 85 / 120\n",
      "Encoding the subspace: 86 / 120\n",
      "Encoding the subspace: 87 / 120\n",
      "Encoding the subspace: 88 / 120\n",
      "Encoding the subspace: 89 / 120\n",
      "Encoding the subspace: 90 / 120\n",
      "Encoding the subspace: 91 / 120\n",
      "Encoding the subspace: 92 / 120\n",
      "Encoding the subspace: 93 / 120\n",
      "Encoding the subspace: 94 / 120\n",
      "Encoding the subspace: 95 / 120\n",
      "Encoding the subspace: 96 / 120\n",
      "Encoding the subspace: 97 / 120\n",
      "Encoding the subspace: 98 / 120\n",
      "Encoding the subspace: 99 / 120\n",
      "Encoding the subspace: 100 / 120\n",
      "Encoding the subspace: 101 / 120\n",
      "Encoding the subspace: 102 / 120\n",
      "Encoding the subspace: 103 / 120\n",
      "Encoding the subspace: 104 / 120\n",
      "Encoding the subspace: 105 / 120\n",
      "Encoding the subspace: 106 / 120\n",
      "Encoding the subspace: 107 / 120\n",
      "Encoding the subspace: 108 / 120\n",
      "Encoding the subspace: 109 / 120\n",
      "Encoding the subspace: 110 / 120\n",
      "Encoding the subspace: 111 / 120\n",
      "Encoding the subspace: 112 / 120\n",
      "Encoding the subspace: 113 / 120\n",
      "Encoding the subspace: 114 / 120\n",
      "Encoding the subspace: 115 / 120\n",
      "Encoding the subspace: 116 / 120\n",
      "Encoding the subspace: 117 / 120\n",
      "Encoding the subspace: 118 / 120\n",
      "Encoding the subspace: 119 / 120\n",
      "recon train shape:  (16395, 1521)\n",
      "train shape after inverse tucker:  (16395, 1521)\n",
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=1521, out_features=150, bias=True)\n",
      ")\n",
      "Epoch 1 of 500, Train Loss: 0.237547\n",
      "best loss:  0.2375473324606527\n",
      "Epoch 2 of 500, Train Loss: 0.047305\n",
      "best loss:  0.04730477994948611\n",
      "Epoch 3 of 500, Train Loss: 0.036185\n",
      "best loss:  0.03618536313071616\n",
      "Epoch 4 of 500, Train Loss: 0.028420\n",
      "best loss:  0.028419789993077833\n",
      "Epoch 5 of 500, Train Loss: 0.024236\n",
      "best loss:  0.024235734803143658\n",
      "Epoch 6 of 500, Train Loss: 0.021489\n",
      "best loss:  0.021489107555370462\n",
      "Epoch 7 of 500, Train Loss: 0.019461\n",
      "best loss:  0.019460901070671343\n",
      "Epoch 8 of 500, Train Loss: 0.017940\n",
      "best loss:  0.01794036265594394\n",
      "Epoch 9 of 500, Train Loss: 0.016727\n",
      "best loss:  0.016727315151410595\n",
      "Epoch 10 of 500, Train Loss: 0.015635\n",
      "best loss:  0.01563499010864703\n",
      "Epoch 11 of 500, Train Loss: 0.014672\n",
      "best loss:  0.014671764375557112\n",
      "Epoch 12 of 500, Train Loss: 0.013801\n",
      "best loss:  0.013801007712749013\n",
      "Epoch 13 of 500, Train Loss: 0.013084\n",
      "best loss:  0.01308358243694565\n",
      "Epoch 14 of 500, Train Loss: 0.012431\n",
      "best loss:  0.012431338039369416\n",
      "Epoch 15 of 500, Train Loss: 0.011858\n",
      "best loss:  0.011858023174265904\n",
      "Epoch 16 of 500, Train Loss: 0.011364\n",
      "best loss:  0.011363622194378052\n",
      "Epoch 17 of 500, Train Loss: 0.010921\n",
      "best loss:  0.010921237450372788\n",
      "Epoch 18 of 500, Train Loss: 0.010508\n",
      "best loss:  0.01050814459111195\n",
      "Epoch 19 of 500, Train Loss: 0.010158\n",
      "best loss:  0.010158474263130478\n",
      "Epoch 20 of 500, Train Loss: 0.009820\n",
      "best loss:  0.009819792800509847\n",
      "Epoch 21 of 500, Train Loss: 0.009530\n",
      "best loss:  0.009529583857420925\n",
      "Epoch 22 of 500, Train Loss: 0.009247\n",
      "best loss:  0.00924695177825417\n",
      "Epoch 23 of 500, Train Loss: 0.008994\n",
      "best loss:  0.008994263578783865\n",
      "Epoch 24 of 500, Train Loss: 0.008767\n",
      "best loss:  0.00876688737897816\n",
      "Epoch 25 of 500, Train Loss: 0.008562\n",
      "best loss:  0.008562258771072907\n",
      "Epoch 26 of 500, Train Loss: 0.008363\n",
      "best loss:  0.008363199935907447\n",
      "Epoch 27 of 500, Train Loss: 0.008213\n",
      "best loss:  0.008212600022580568\n",
      "Epoch 28 of 500, Train Loss: 0.008019\n",
      "best loss:  0.008018675059113514\n",
      "Epoch 29 of 500, Train Loss: 0.007878\n",
      "best loss:  0.00787782610983808\n",
      "Epoch 30 of 500, Train Loss: 0.007752\n",
      "best loss:  0.007752311864710781\n",
      "Epoch 31 of 500, Train Loss: 0.007650\n",
      "best loss:  0.007649569581091303\n",
      "Epoch 32 of 500, Train Loss: 0.007532\n",
      "best loss:  0.007531828274196844\n",
      "Epoch 33 of 500, Train Loss: 0.007451\n",
      "best loss:  0.007450966267461181\n",
      "Epoch 34 of 500, Train Loss: 0.007389\n",
      "best loss:  0.0073888809754072814\n",
      "Epoch 35 of 500, Train Loss: 0.007269\n",
      "best loss:  0.007268748873783377\n",
      "Epoch 36 of 500, Train Loss: 0.007201\n",
      "best loss:  0.007201359143084485\n",
      "Epoch 37 of 500, Train Loss: 0.007123\n",
      "best loss:  0.007123466322781489\n",
      "Epoch 38 of 500, Train Loss: 0.007079\n",
      "best loss:  0.007078670475148242\n",
      "Epoch 39 of 500, Train Loss: 0.006986\n",
      "best loss:  0.006985801474016174\n",
      "Epoch 40 of 500, Train Loss: 0.006963\n",
      "best loss:  0.006962524429744275\n",
      "Epoch 41 of 500, Train Loss: 0.006915\n",
      "best loss:  0.006915226040065989\n",
      "Epoch 42 of 500, Train Loss: 0.006884\n",
      "best loss:  0.006883628285682197\n",
      "Epoch 43 of 500, Train Loss: 0.006842\n",
      "best loss:  0.006842294987088048\n",
      "Epoch 44 of 500, Train Loss: 0.006825\n",
      "best loss:  0.0068246327517030745\n",
      "Epoch 45 of 500, Train Loss: 0.006738\n",
      "best loss:  0.006738399726685223\n",
      "Epoch 46 of 500, Train Loss: 0.006761\n",
      "Epoch 47 of 500, Train Loss: 0.006684\n",
      "best loss:  0.006684268863674059\n",
      "Epoch 48 of 500, Train Loss: 0.006717\n",
      "Epoch 49 of 500, Train Loss: 0.006684\n",
      "Epoch 50 of 500, Train Loss: 0.006714\n",
      "Epoch 51 of 500, Train Loss: 0.006682\n",
      "best loss:  0.0066816489595302574\n",
      "Epoch 52 of 500, Train Loss: 0.006765\n",
      "Epoch 53 of 500, Train Loss: 0.006718\n",
      "Epoch 54 of 500, Train Loss: 0.006843\n",
      "Epoch 55 of 500, Train Loss: 0.006854\n",
      "Epoch 56 of 500, Train Loss: 0.006944\n",
      "Epoch 57 of 500, Train Loss: 0.007062\n",
      "Epoch 58 of 500, Train Loss: 0.007130\n",
      "Epoch 59 of 500, Train Loss: 0.007347\n",
      "Epoch 60 of 500, Train Loss: 0.007283\n",
      "Epoch 61 of 500, Train Loss: 0.007510\n",
      "Epoch 62 of 500, Train Loss: 0.007354\n",
      "Epoch 63 of 500, Train Loss: 0.007341\n",
      "Epoch 64 of 500, Train Loss: 0.007203\n",
      "Epoch 65 of 500, Train Loss: 0.007159\n",
      "Epoch 66 of 500, Train Loss: 0.007039\n",
      "Epoch 67 of 500, Train Loss: 0.006908\n",
      "Epoch 68 of 500, Train Loss: 0.006819\n",
      "Epoch 69 of 500, Train Loss: 0.006789\n",
      "Epoch 70 of 500, Train Loss: 0.006717\n",
      "Epoch 71 of 500, Train Loss: 0.006712\n",
      "Epoch 72 of 500, Train Loss: 0.006707\n",
      "Epoch 73 of 500, Train Loss: 0.006731\n",
      "Epoch 74 of 500, Train Loss: 0.006729\n",
      "Epoch 75 of 500, Train Loss: 0.006733\n",
      "Epoch 76 of 500, Train Loss: 0.006713\n",
      "Epoch 77 of 500, Train Loss: 0.006694\n",
      "Epoch 78 of 500, Train Loss: 0.006654\n",
      "best loss:  0.006653884994637676\n",
      "Epoch 79 of 500, Train Loss: 0.006638\n",
      "best loss:  0.006638168265940188\n",
      "Epoch 80 of 500, Train Loss: 0.006601\n",
      "best loss:  0.006600589203885612\n",
      "Epoch 81 of 500, Train Loss: 0.006589\n",
      "best loss:  0.006588578513415545\n",
      "Epoch 82 of 500, Train Loss: 0.006576\n",
      "best loss:  0.00657553456995777\n",
      "Epoch 83 of 500, Train Loss: 0.006565\n",
      "best loss:  0.006565229383355021\n",
      "Epoch 84 of 500, Train Loss: 0.006573\n",
      "Epoch 85 of 500, Train Loss: 0.006549\n",
      "best loss:  0.006549494425732779\n",
      "Epoch 86 of 500, Train Loss: 0.006556\n",
      "Epoch 87 of 500, Train Loss: 0.006537\n",
      "best loss:  0.0065370734278816555\n",
      "Epoch 88 of 500, Train Loss: 0.006512\n",
      "best loss:  0.006512460886665923\n",
      "Epoch 89 of 500, Train Loss: 0.006502\n",
      "best loss:  0.0065015470174767526\n",
      "Epoch 90 of 500, Train Loss: 0.006500\n",
      "best loss:  0.006499597219779163\n",
      "Epoch 91 of 500, Train Loss: 0.006508\n",
      "Epoch 92 of 500, Train Loss: 0.006501\n",
      "Epoch 93 of 500, Train Loss: 0.006523\n",
      "Epoch 94 of 500, Train Loss: 0.006558\n",
      "Epoch 95 of 500, Train Loss: 0.006589\n",
      "Epoch 96 of 500, Train Loss: 0.006583\n",
      "Epoch 97 of 500, Train Loss: 0.006647\n",
      "Epoch 98 of 500, Train Loss: 0.006663\n",
      "Epoch 99 of 500, Train Loss: 0.006694\n",
      "Epoch 100 of 500, Train Loss: 0.006699\n",
      "Epoch 101 of 500, Train Loss: 0.006706\n",
      "Epoch 102 of 500, Train Loss: 0.006649\n",
      "Epoch 103 of 500, Train Loss: 0.006612\n",
      "Epoch 104 of 500, Train Loss: 0.006553\n",
      "Epoch 105 of 500, Train Loss: 0.006529\n",
      "Epoch 106 of 500, Train Loss: 0.006541\n",
      "Epoch 107 of 500, Train Loss: 0.006538\n",
      "Epoch 108 of 500, Train Loss: 0.006542\n",
      "Epoch 109 of 500, Train Loss: 0.006570\n",
      "Epoch 110 of 500, Train Loss: 0.006576\n",
      "Epoch 111 of 500, Train Loss: 0.006586\n",
      "Epoch 112 of 500, Train Loss: 0.006585\n",
      "Epoch 113 of 500, Train Loss: 0.006581\n",
      "Epoch 114 of 500, Train Loss: 0.006573\n",
      "Epoch 115 of 500, Train Loss: 0.006577\n",
      "Epoch 116 of 500, Train Loss: 0.006586\n",
      "Epoch 117 of 500, Train Loss: 0.006587\n",
      "Epoch 118 of 500, Train Loss: 0.006611\n",
      "Epoch 119 of 500, Train Loss: 0.006629\n",
      "Epoch 120 of 500, Train Loss: 0.006649\n",
      "Epoch 121 of 500, Train Loss: 0.006631\n",
      "Epoch 122 of 500, Train Loss: 0.006599\n",
      "Epoch 123 of 500, Train Loss: 0.006559\n",
      "Epoch 124 of 500, Train Loss: 0.006542\n",
      "Epoch 125 of 500, Train Loss: 0.006508\n",
      "Epoch 126 of 500, Train Loss: 0.006502\n",
      "Epoch 127 of 500, Train Loss: 0.006516\n",
      "Epoch 128 of 500, Train Loss: 0.006514\n",
      "Epoch 129 of 500, Train Loss: 0.006527\n",
      "Epoch 130 of 500, Train Loss: 0.006518\n",
      "Epoch 131 of 500, Train Loss: 0.006584\n",
      "Epoch 132 of 500, Train Loss: 0.006548\n",
      "Epoch 133 of 500, Train Loss: 0.006599\n",
      "Epoch 134 of 500, Train Loss: 0.006517\n",
      "Epoch 135 of 500, Train Loss: 0.006561\n",
      "Epoch 136 of 500, Train Loss: 0.006467\n",
      "best loss:  0.006466767023874324\n",
      "Epoch 137 of 500, Train Loss: 0.006493\n",
      "Epoch 138 of 500, Train Loss: 0.006449\n",
      "best loss:  0.006448933149100455\n",
      "Epoch 139 of 500, Train Loss: 0.006499\n",
      "Epoch 140 of 500, Train Loss: 0.006506\n",
      "Epoch 141 of 500, Train Loss: 0.006528\n",
      "Epoch 142 of 500, Train Loss: 0.006545\n",
      "Epoch 143 of 500, Train Loss: 0.006549\n",
      "Epoch 144 of 500, Train Loss: 0.006518\n",
      "Epoch 145 of 500, Train Loss: 0.006494\n",
      "Epoch 146 of 500, Train Loss: 0.006491\n",
      "Epoch 147 of 500, Train Loss: 0.006451\n",
      "Epoch 148 of 500, Train Loss: 0.006438\n",
      "best loss:  0.006437998754040035\n",
      "Epoch 149 of 500, Train Loss: 0.006439\n",
      "Epoch 150 of 500, Train Loss: 0.006417\n",
      "best loss:  0.006416837155864964\n",
      "Epoch 151 of 500, Train Loss: 0.006434\n",
      "Epoch 152 of 500, Train Loss: 0.006457\n",
      "Epoch 153 of 500, Train Loss: 0.006508\n",
      "Epoch 154 of 500, Train Loss: 0.006552\n",
      "Epoch 155 of 500, Train Loss: 0.006562\n",
      "Epoch 156 of 500, Train Loss: 0.006599\n",
      "Epoch 157 of 500, Train Loss: 0.006590\n",
      "Epoch 158 of 500, Train Loss: 0.006519\n",
      "Epoch 159 of 500, Train Loss: 0.006506\n",
      "Epoch 160 of 500, Train Loss: 0.006466\n",
      "Epoch 161 of 500, Train Loss: 0.006409\n",
      "best loss:  0.006408717067706463\n",
      "Epoch 162 of 500, Train Loss: 0.006400\n",
      "best loss:  0.006399986606683264\n",
      "Epoch 163 of 500, Train Loss: 0.006377\n",
      "best loss:  0.006377115673313365\n",
      "Epoch 164 of 500, Train Loss: 0.006394\n",
      "Epoch 165 of 500, Train Loss: 0.006429\n",
      "Epoch 166 of 500, Train Loss: 0.006432\n",
      "Epoch 167 of 500, Train Loss: 0.006458\n",
      "Epoch 168 of 500, Train Loss: 0.006448\n",
      "Epoch 169 of 500, Train Loss: 0.006459\n",
      "Epoch 170 of 500, Train Loss: 0.006456\n",
      "Epoch 171 of 500, Train Loss: 0.006443\n",
      "Epoch 172 of 500, Train Loss: 0.006426\n",
      "Epoch 173 of 500, Train Loss: 0.006413\n",
      "Epoch 174 of 500, Train Loss: 0.006417\n",
      "Epoch 175 of 500, Train Loss: 0.006399\n",
      "Epoch 176 of 500, Train Loss: 0.006408\n",
      "Epoch 177 of 500, Train Loss: 0.006414\n",
      "Epoch 178 of 500, Train Loss: 0.006411\n",
      "Epoch 179 of 500, Train Loss: 0.006403\n",
      "Epoch 180 of 500, Train Loss: 0.006407\n",
      "Epoch 181 of 500, Train Loss: 0.006422\n",
      "Epoch 182 of 500, Train Loss: 0.006444\n",
      "Epoch 183 of 500, Train Loss: 0.006440\n",
      "Epoch 184 of 500, Train Loss: 0.006438\n",
      "Epoch 185 of 500, Train Loss: 0.006408\n",
      "Epoch 186 of 500, Train Loss: 0.006404\n",
      "Epoch 187 of 500, Train Loss: 0.006386\n",
      "Epoch 188 of 500, Train Loss: 0.006376\n",
      "best loss:  0.006376224643976767\n",
      "Epoch 189 of 500, Train Loss: 0.006359\n",
      "best loss:  0.006359395012665583\n",
      "Epoch 190 of 500, Train Loss: 0.006352\n",
      "best loss:  0.006352183897257602\n",
      "Epoch 191 of 500, Train Loss: 0.006353\n",
      "Epoch 192 of 500, Train Loss: 0.006362\n",
      "Epoch 193 of 500, Train Loss: 0.006366\n",
      "Epoch 194 of 500, Train Loss: 0.006397\n",
      "Epoch 195 of 500, Train Loss: 0.006410\n",
      "Epoch 196 of 500, Train Loss: 0.006492\n",
      "Epoch 197 of 500, Train Loss: 0.006453\n",
      "Epoch 198 of 500, Train Loss: 0.006445\n",
      "Epoch 199 of 500, Train Loss: 0.006436\n",
      "Epoch 200 of 500, Train Loss: 0.006384\n",
      "Epoch 201 of 500, Train Loss: 0.006294\n",
      "best loss:  0.006294286565073629\n",
      "Epoch 202 of 500, Train Loss: 0.006294\n",
      "best loss:  0.0062936399538790115\n",
      "Epoch 203 of 500, Train Loss: 0.006283\n",
      "best loss:  0.006282544173904904\n",
      "Epoch 204 of 500, Train Loss: 0.006299\n",
      "Epoch 205 of 500, Train Loss: 0.006311\n",
      "Epoch 206 of 500, Train Loss: 0.006341\n",
      "Epoch 207 of 500, Train Loss: 0.006348\n",
      "Epoch 208 of 500, Train Loss: 0.006383\n",
      "Epoch 209 of 500, Train Loss: 0.006415\n",
      "Epoch 210 of 500, Train Loss: 0.006475\n",
      "Epoch 211 of 500, Train Loss: 0.006495\n",
      "Epoch 212 of 500, Train Loss: 0.006500\n",
      "Epoch 213 of 500, Train Loss: 0.006437\n",
      "Epoch 214 of 500, Train Loss: 0.006404\n",
      "Epoch 215 of 500, Train Loss: 0.006408\n",
      "Epoch 216 of 500, Train Loss: 0.006437\n",
      "Epoch 217 of 500, Train Loss: 0.006440\n",
      "Epoch 218 of 500, Train Loss: 0.006490\n",
      "Epoch 219 of 500, Train Loss: 0.006466\n",
      "Epoch 220 of 500, Train Loss: 0.006494\n",
      "Epoch 221 of 500, Train Loss: 0.006441\n",
      "Epoch 222 of 500, Train Loss: 0.006365\n",
      "Epoch 223 of 500, Train Loss: 0.006329\n",
      "Epoch 224 of 500, Train Loss: 0.006290\n",
      "Epoch 225 of 500, Train Loss: 0.006258\n",
      "best loss:  0.006258064023870614\n",
      "Epoch 226 of 500, Train Loss: 0.006265\n",
      "Epoch 227 of 500, Train Loss: 0.006265\n",
      "Epoch 228 of 500, Train Loss: 0.006293\n",
      "Epoch 229 of 500, Train Loss: 0.006296\n",
      "Epoch 230 of 500, Train Loss: 0.006318\n",
      "Epoch 231 of 500, Train Loss: 0.006317\n",
      "Epoch 232 of 500, Train Loss: 0.006329\n",
      "Epoch 233 of 500, Train Loss: 0.006329\n",
      "Epoch 234 of 500, Train Loss: 0.006347\n",
      "Epoch 235 of 500, Train Loss: 0.006356\n",
      "Epoch 236 of 500, Train Loss: 0.006337\n",
      "Epoch 237 of 500, Train Loss: 0.006328\n",
      "Epoch 238 of 500, Train Loss: 0.006348\n",
      "Epoch 239 of 500, Train Loss: 0.006399\n",
      "Epoch 240 of 500, Train Loss: 0.006405\n",
      "Epoch 241 of 500, Train Loss: 0.006400\n",
      "Epoch 242 of 500, Train Loss: 0.006384\n",
      "Epoch 243 of 500, Train Loss: 0.006393\n",
      "Epoch 244 of 500, Train Loss: 0.006433\n",
      "Epoch 245 of 500, Train Loss: 0.006472\n",
      "Epoch 246 of 500, Train Loss: 0.006484\n",
      "Epoch 247 of 500, Train Loss: 0.006447\n",
      "Epoch 248 of 500, Train Loss: 0.006469\n",
      "Epoch 249 of 500, Train Loss: 0.006436\n",
      "Epoch 250 of 500, Train Loss: 0.006421\n",
      "Epoch 251 of 500, Train Loss: 0.006420\n",
      "Epoch 252 of 500, Train Loss: 0.006431\n",
      "Epoch 253 of 500, Train Loss: 0.006388\n",
      "Epoch 254 of 500, Train Loss: 0.006415\n",
      "Epoch 255 of 500, Train Loss: 0.006426\n",
      "Epoch 256 of 500, Train Loss: 0.006419\n",
      "Epoch 257 of 500, Train Loss: 0.006367\n",
      "Epoch 258 of 500, Train Loss: 0.006353\n",
      "Epoch 259 of 500, Train Loss: 0.006337\n",
      "Epoch 260 of 500, Train Loss: 0.006356\n",
      "Epoch 261 of 500, Train Loss: 0.006362\n",
      "Epoch 262 of 500, Train Loss: 0.006356\n",
      "Epoch 263 of 500, Train Loss: 0.006321\n",
      "Epoch 264 of 500, Train Loss: 0.006307\n",
      "Epoch 265 of 500, Train Loss: 0.006299\n",
      "Epoch 266 of 500, Train Loss: 0.006333\n",
      "Epoch 267 of 500, Train Loss: 0.006355\n",
      "Epoch 268 of 500, Train Loss: 0.006358\n",
      "Epoch 269 of 500, Train Loss: 0.006332\n",
      "Epoch 270 of 500, Train Loss: 0.006309\n",
      "Epoch 271 of 500, Train Loss: 0.006340\n",
      "Epoch 272 of 500, Train Loss: 0.006373\n",
      "Epoch 273 of 500, Train Loss: 0.006417\n",
      "Epoch 274 of 500, Train Loss: 0.006409\n",
      "Epoch 275 of 500, Train Loss: 0.006433\n",
      "Epoch 276 of 500, Train Loss: 0.006500\n",
      "Epoch 277 of 500, Train Loss: 0.006582\n",
      "Epoch 278 of 500, Train Loss: 0.006597\n",
      "Epoch 279 of 500, Train Loss: 0.006555\n",
      "Epoch 280 of 500, Train Loss: 0.006549\n",
      "Epoch 281 of 500, Train Loss: 0.006601\n",
      "Epoch 282 of 500, Train Loss: 0.006633\n",
      "Epoch 283 of 500, Train Loss: 0.006549\n",
      "Epoch 284 of 500, Train Loss: 0.006542\n",
      "Epoch 285 of 500, Train Loss: 0.006493\n",
      "Epoch 286 of 500, Train Loss: 0.006483\n",
      "Epoch 287 of 500, Train Loss: 0.006436\n",
      "Epoch 288 of 500, Train Loss: 0.006448\n",
      "Epoch 289 of 500, Train Loss: 0.006381\n",
      "Epoch 290 of 500, Train Loss: 0.006352\n",
      "Epoch 291 of 500, Train Loss: 0.006316\n",
      "Epoch 292 of 500, Train Loss: 0.006302\n",
      "Epoch 293 of 500, Train Loss: 0.006264\n",
      "Epoch 294 of 500, Train Loss: 0.006255\n",
      "best loss:  0.006255093530585715\n",
      "Epoch 295 of 500, Train Loss: 0.006247\n",
      "best loss:  0.0062465171957778376\n",
      "Epoch 296 of 500, Train Loss: 0.006310\n",
      "Epoch 297 of 500, Train Loss: 0.006318\n",
      "Epoch 298 of 500, Train Loss: 0.006347\n",
      "Epoch 299 of 500, Train Loss: 0.006347\n",
      "Epoch 300 of 500, Train Loss: 0.006410\n",
      "Epoch 301 of 500, Train Loss: 0.006451\n",
      "Epoch 302 of 500, Train Loss: 0.006542\n",
      "Epoch 303 of 500, Train Loss: 0.006482\n",
      "Epoch 304 of 500, Train Loss: 0.006448\n",
      "Epoch 305 of 500, Train Loss: 0.006426\n",
      "Epoch 306 of 500, Train Loss: 0.006490\n",
      "Epoch 307 of 500, Train Loss: 0.006470\n",
      "Epoch 308 of 500, Train Loss: 0.006447\n",
      "Epoch 309 of 500, Train Loss: 0.006439\n",
      "Epoch 310 of 500, Train Loss: 0.006406\n",
      "Epoch 311 of 500, Train Loss: 0.006402\n",
      "Epoch 312 of 500, Train Loss: 0.006368\n",
      "Epoch 313 of 500, Train Loss: 0.006364\n",
      "Epoch 314 of 500, Train Loss: 0.006311\n",
      "Epoch 315 of 500, Train Loss: 0.006310\n",
      "Epoch 316 of 500, Train Loss: 0.006321\n",
      "Epoch 317 of 500, Train Loss: 0.006347\n",
      "Epoch 318 of 500, Train Loss: 0.006314\n",
      "Epoch 319 of 500, Train Loss: 0.006299\n",
      "Epoch 320 of 500, Train Loss: 0.006301\n",
      "Epoch 321 of 500, Train Loss: 0.006331\n",
      "Epoch 322 of 500, Train Loss: 0.006354\n",
      "Epoch 323 of 500, Train Loss: 0.006362\n",
      "Epoch 324 of 500, Train Loss: 0.006313\n",
      "Epoch 325 of 500, Train Loss: 0.006289\n",
      "Epoch 326 of 500, Train Loss: 0.006273\n",
      "Epoch 327 of 500, Train Loss: 0.006297\n",
      "Epoch 328 of 500, Train Loss: 0.006310\n",
      "Epoch 329 of 500, Train Loss: 0.006312\n",
      "Epoch 330 of 500, Train Loss: 0.006291\n",
      "Epoch 331 of 500, Train Loss: 0.006328\n",
      "Epoch 332 of 500, Train Loss: 0.006377\n",
      "Epoch 333 of 500, Train Loss: 0.006393\n",
      "Epoch 334 of 500, Train Loss: 0.006352\n",
      "Epoch 335 of 500, Train Loss: 0.006339\n",
      "Epoch 336 of 500, Train Loss: 0.006334\n",
      "Epoch 337 of 500, Train Loss: 0.006349\n",
      "Epoch 338 of 500, Train Loss: 0.006358\n",
      "Epoch 339 of 500, Train Loss: 0.006362\n",
      "Epoch 340 of 500, Train Loss: 0.006340\n",
      "Epoch 341 of 500, Train Loss: 0.006343\n",
      "Epoch 342 of 500, Train Loss: 0.006364\n",
      "Epoch 343 of 500, Train Loss: 0.006379\n",
      "Epoch 344 of 500, Train Loss: 0.006352\n",
      "Epoch 345 of 500, Train Loss: 0.006329\n",
      "Epoch 346 of 500, Train Loss: 0.006292\n",
      "Epoch 347 of 500, Train Loss: 0.006299\n",
      "Epoch 348 of 500, Train Loss: 0.006317\n",
      "Epoch 349 of 500, Train Loss: 0.006328\n",
      "Epoch 350 of 500, Train Loss: 0.006339\n",
      "Epoch 351 of 500, Train Loss: 0.006344\n",
      "Epoch 352 of 500, Train Loss: 0.006391\n",
      "Epoch 353 of 500, Train Loss: 0.006406\n",
      "Epoch 354 of 500, Train Loss: 0.006408\n",
      "Epoch 355 of 500, Train Loss: 0.006380\n",
      "Epoch 356 of 500, Train Loss: 0.006392\n",
      "Epoch 357 of 500, Train Loss: 0.006409\n",
      "Epoch 358 of 500, Train Loss: 0.006444\n",
      "Epoch 359 of 500, Train Loss: 0.006458\n",
      "Epoch 360 of 500, Train Loss: 0.006452\n",
      "Epoch 361 of 500, Train Loss: 0.006478\n",
      "Epoch 362 of 500, Train Loss: 0.006514\n",
      "Epoch 363 of 500, Train Loss: 0.006537\n",
      "Epoch 364 of 500, Train Loss: 0.006522\n",
      "Epoch 365 of 500, Train Loss: 0.006483\n",
      "Epoch 366 of 500, Train Loss: 0.006523\n",
      "Epoch 367 of 500, Train Loss: 0.006512\n",
      "Epoch 368 of 500, Train Loss: 0.006521\n",
      "Epoch 369 of 500, Train Loss: 0.006434\n",
      "Epoch 370 of 500, Train Loss: 0.006425\n",
      "Epoch 371 of 500, Train Loss: 0.006378\n",
      "Epoch 372 of 500, Train Loss: 0.006364\n",
      "Epoch 373 of 500, Train Loss: 0.006292\n",
      "Epoch 374 of 500, Train Loss: 0.006281\n",
      "Epoch 375 of 500, Train Loss: 0.006260\n",
      "Epoch 376 of 500, Train Loss: 0.006313\n",
      "Epoch 377 of 500, Train Loss: 0.006305\n",
      "Epoch 378 of 500, Train Loss: 0.006322\n",
      "Epoch 379 of 500, Train Loss: 0.006309\n",
      "Epoch 380 of 500, Train Loss: 0.006336\n",
      "Epoch 381 of 500, Train Loss: 0.006346\n",
      "Epoch 382 of 500, Train Loss: 0.006355\n",
      "Epoch 383 of 500, Train Loss: 0.006332\n",
      "Epoch 384 of 500, Train Loss: 0.006366\n",
      "Epoch 385 of 500, Train Loss: 0.006369\n",
      "Epoch 386 of 500, Train Loss: 0.006356\n",
      "Epoch 387 of 500, Train Loss: 0.006315\n",
      "Epoch 388 of 500, Train Loss: 0.006313\n",
      "Epoch 389 of 500, Train Loss: 0.006312\n",
      "Epoch 390 of 500, Train Loss: 0.006316\n",
      "Epoch 391 of 500, Train Loss: 0.006308\n",
      "Epoch 392 of 500, Train Loss: 0.006296\n",
      "Epoch 393 of 500, Train Loss: 0.006308\n",
      "Epoch 394 of 500, Train Loss: 0.006347\n",
      "Epoch 395 of 500, Train Loss: 0.006397\n",
      "Epoch 396 of 500, Train Loss: 0.006430\n",
      "Epoch 397 of 500, Train Loss: 0.006433\n",
      "Epoch 398 of 500, Train Loss: 0.006467\n",
      "Epoch 399 of 500, Train Loss: 0.006513\n",
      "Epoch 400 of 500, Train Loss: 0.006524\n",
      "Epoch 401 of 500, Train Loss: 0.006473\n",
      "Epoch 402 of 500, Train Loss: 0.006464\n",
      "Epoch 403 of 500, Train Loss: 0.006440\n",
      "Epoch 404 of 500, Train Loss: 0.006472\n",
      "Epoch 405 of 500, Train Loss: 0.006410\n",
      "Epoch 406 of 500, Train Loss: 0.006402\n",
      "Epoch 407 of 500, Train Loss: 0.006332\n",
      "Epoch 408 of 500, Train Loss: 0.006351\n",
      "Epoch 409 of 500, Train Loss: 0.006322\n",
      "Epoch 410 of 500, Train Loss: 0.006311\n",
      "Epoch 411 of 500, Train Loss: 0.006248\n",
      "Epoch 412 of 500, Train Loss: 0.006285\n",
      "Epoch 413 of 500, Train Loss: 0.006310\n",
      "Epoch 414 of 500, Train Loss: 0.006334\n",
      "Epoch 415 of 500, Train Loss: 0.006282\n",
      "Epoch 416 of 500, Train Loss: 0.006315\n",
      "Epoch 417 of 500, Train Loss: 0.006362\n",
      "Epoch 418 of 500, Train Loss: 0.006425\n",
      "Epoch 419 of 500, Train Loss: 0.006375\n",
      "Epoch 420 of 500, Train Loss: 0.006369\n",
      "Epoch 421 of 500, Train Loss: 0.006386\n",
      "Epoch 422 of 500, Train Loss: 0.006409\n",
      "Epoch 423 of 500, Train Loss: 0.006374\n",
      "Epoch 424 of 500, Train Loss: 0.006378\n",
      "Epoch 425 of 500, Train Loss: 0.006379\n",
      "Epoch 426 of 500, Train Loss: 0.006447\n",
      "Epoch 427 of 500, Train Loss: 0.006433\n",
      "Epoch 428 of 500, Train Loss: 0.006460\n",
      "Epoch 429 of 500, Train Loss: 0.006463\n",
      "Epoch 430 of 500, Train Loss: 0.006512\n",
      "Epoch 431 of 500, Train Loss: 0.006520\n",
      "Epoch 432 of 500, Train Loss: 0.006606\n",
      "Epoch 433 of 500, Train Loss: 0.006576\n",
      "Epoch 434 of 500, Train Loss: 0.006597\n",
      "Epoch 435 of 500, Train Loss: 0.006539\n",
      "Epoch 436 of 500, Train Loss: 0.006523\n",
      "Epoch 437 of 500, Train Loss: 0.006447\n",
      "Epoch 438 of 500, Train Loss: 0.006441\n",
      "Epoch 439 of 500, Train Loss: 0.006407\n",
      "Epoch 440 of 500, Train Loss: 0.006443\n",
      "Epoch 441 of 500, Train Loss: 0.006442\n",
      "Epoch 442 of 500, Train Loss: 0.006476\n",
      "Epoch 443 of 500, Train Loss: 0.006448\n",
      "Epoch 444 of 500, Train Loss: 0.006466\n",
      "Epoch 445 of 500, Train Loss: 0.006445\n",
      "Epoch 446 of 500, Train Loss: 0.006436\n",
      "Epoch 447 of 500, Train Loss: 0.006425\n",
      "Epoch 448 of 500, Train Loss: 0.006467\n",
      "Epoch 449 of 500, Train Loss: 0.006459\n",
      "Epoch 450 of 500, Train Loss: 0.006445\n",
      "Epoch 451 of 500, Train Loss: 0.006416\n",
      "Epoch 452 of 500, Train Loss: 0.006442\n",
      "Epoch 453 of 500, Train Loss: 0.006450\n",
      "Epoch 454 of 500, Train Loss: 0.006443\n",
      "Epoch 455 of 500, Train Loss: 0.006424\n",
      "Epoch 456 of 500, Train Loss: 0.006424\n",
      "Epoch 457 of 500, Train Loss: 0.006406\n",
      "Epoch 458 of 500, Train Loss: 0.006417\n",
      "Epoch 459 of 500, Train Loss: 0.006417\n",
      "Epoch 460 of 500, Train Loss: 0.006465\n",
      "Epoch 461 of 500, Train Loss: 0.006456\n",
      "Epoch 462 of 500, Train Loss: 0.006457\n",
      "Epoch 463 of 500, Train Loss: 0.006427\n",
      "Epoch 464 of 500, Train Loss: 0.006407\n",
      "Epoch 465 of 500, Train Loss: 0.006396\n",
      "Epoch 466 of 500, Train Loss: 0.006457\n",
      "Epoch 467 of 500, Train Loss: 0.006439\n",
      "Epoch 468 of 500, Train Loss: 0.006427\n",
      "Epoch 469 of 500, Train Loss: 0.006394\n",
      "Epoch 470 of 500, Train Loss: 0.006433\n",
      "Epoch 471 of 500, Train Loss: 0.006404\n",
      "Epoch 472 of 500, Train Loss: 0.006419\n",
      "Epoch 473 of 500, Train Loss: 0.006380\n",
      "Epoch 474 of 500, Train Loss: 0.006390\n",
      "Epoch 475 of 500, Train Loss: 0.006422\n",
      "Epoch 476 of 500, Train Loss: 0.006403\n",
      "Epoch 477 of 500, Train Loss: 0.006434\n",
      "Epoch 478 of 500, Train Loss: 0.006417\n",
      "Epoch 479 of 500, Train Loss: 0.006515\n",
      "Epoch 480 of 500, Train Loss: 0.006480\n",
      "Epoch 481 of 500, Train Loss: 0.006576\n",
      "Epoch 482 of 500, Train Loss: 0.006509\n",
      "Epoch 483 of 500, Train Loss: 0.006557\n",
      "Epoch 484 of 500, Train Loss: 0.006573\n",
      "Epoch 485 of 500, Train Loss: 0.006579\n",
      "Epoch 486 of 500, Train Loss: 0.006553\n",
      "Epoch 487 of 500, Train Loss: 0.006513\n",
      "Epoch 488 of 500, Train Loss: 0.006486\n",
      "Epoch 489 of 500, Train Loss: 0.006488\n"
     ]
    }
   ],
   "source": [
    "# AET 3d - Per plane, Full Tucker coeffs\n",
    "\n",
    "#latent_dims = [4, 8, 12, 16, 24, 32, 48, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320]\n",
    "#latent_dims = [5, 10, 15, 20, 25, 30, 45, 60, 75, 90, 105, 130, 145, 160, 190, 220, 250, 280, 320]\n",
    "#latent_dims = [1, 2, 3, 5, 10, 20, 30, 45, 60, 90, 120, 150, 180, 210, 240, 300]\n",
    "latent_dims = [5, 10, 20, 30, 45, 60, 90, 120, 150, 180, 210, 240, 300]\n",
    "\n",
    "#latent_dims = []\n",
    "\n",
    "\n",
    "pq_dic = []\n",
    "pq_var = [16]\n",
    "pq_bits = [4]\n",
    "\n",
    "#pq_var = [16]\n",
    "#pq_bits = [4]\n",
    "\n",
    "#AET3d_all = np.zeros((8,len(pq_bits),len(latent_dims),16395,39,39))\n",
    "#print('AET3d shape: ', AET3d_all.shape)\n",
    "\n",
    "for j in range(len(pq_var)):\n",
    "    pq_bit = pq_bits[j]\n",
    "    print('pqbit: ', pq_bit)\n",
    "    \n",
    "    for p in range(8):\n",
    "        print('data timestep: ', timestep)\n",
    "        #X_train = all_planes[p]\n",
    "        #X_train = i_f[p]\n",
    "        plane_idx = p\n",
    "        \n",
    "        print('plane idx: ', plane_idx)\n",
    "        \n",
    "        Num_coeffs = [1521]\n",
    "        \n",
    "        for num_coeffs in Num_coeffs:\n",
    "            \n",
    "            tqdm.write('tucker_coeffs: '+str(num_coeffs))\n",
    "            \n",
    "            basis_product = np.load('./results/AET3d/v2_{}/basis_product_{}_plane{}.npy'.format(timestep, num_coeffs, p))            \n",
    "            print(\"basis product shape: \", basis_product.shape)\n",
    "            \n",
    "            X_train_tucker = np.load('./results/AET3d/v2_{}/X_train_tucker_{}_plane{}.npy'.format(timestep, num_coeffs, p))\n",
    "            print('X_train_tucker shape: ', X_train_tucker.shape)\n",
    "            print('X_train max avg: ', np.max(X_train_tucker), np.average(X_train_tucker))\n",
    "            train_num = X_train_tucker.shape[0]\n",
    "            \n",
    "            training_data_tucker = torch.utils.data.TensorDataset(torch.Tensor(X_train_tucker))\n",
    "            \n",
    "            training_loader_tucker = DataLoader(training_data_tucker,\n",
    "                                                batch_size=Batch_Size,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True)\n",
    "            \n",
    "            AET3d_recon = np.zeros((len(latent_dims),train_num,39,39))\n",
    "            #AET3d_encode = []\n",
    "            \n",
    "            for i in range(len(latent_dims)):            \n",
    "                latent_dim = latent_dims[i]\n",
    "                model = Autoencoder(X_train_tucker.shape[1], latent_dim)\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=Lr_Rate)\n",
    "                device = get_device()\n",
    "                model.to(device)\n",
    "                print(model)\n",
    "                \n",
    "                train_loss = training(model, training_loader_tucker, Epochs, latent_dim, timestep, plane_idx)\n",
    "                model.load_state_dict(torch.load('./results/AET3d/v2_{}/model/AET3d_plane{}_latent{}_best_parameters.pt'.format(timestep,plane_idx,latent_dim)))\n",
    "                #train_loss = training_rmse_without_norm(model, training_loader_tucker, Epochs, latent_dim, timestep, plane_idx, pq_bit)\n",
    "                #model.load_state_dict(torch.load('./results/AET3d/v2_{}/model/AET3d_pq{}bits_plane{}_latent{}_without_norm_best_parameters.pt'.format(timestep, pq_bit, plane_idx,latent_dim)))\n",
    "                model.eval()\n",
    "                \n",
    "                latent_train = encoding(model, training_loader_tucker, X_train_tucker.shape[0], latent_dim)\n",
    "                print('latent train shape: ', latent_train.shape)\n",
    "                \n",
    "                def train_quantizer():\n",
    "                    seed = np.random.randint(100)\n",
    "                    pq = nanopq.PQ(M=latent_dim, Ks=pq_var[j], verbose=True)          \n",
    "                    pq.fit(vecs=latent_train, iter=20, seed=seed)\n",
    "                    return pq\n",
    "\n",
    "                pq = train_quantizer()\n",
    "                if p == 0:\n",
    "                    pq_dic.append(pq.codewords)\n",
    "            \n",
    "                latent_train_quan = pq.encode(vecs=latent_train)\n",
    "                #np.save('./model/TuckerAE/compress/pq_dict_{}bits_latent{}.npy'.format(pq_bits[k],latent_dim), pq.codewords)\n",
    "        \n",
    "                latent_train_dequan = pq.decode(codes=latent_train_quan)\n",
    "            \n",
    "                training_latent = torch.utils.data.TensorDataset(torch.Tensor(latent_train_dequan))\n",
    "        \n",
    "                training_loader_latent = DataLoader(training_latent,\n",
    "                                                    batch_size=Batch_Size,\n",
    "                                                    shuffle=False,\n",
    "                                                    pin_memory=True)\n",
    "        \n",
    "                recon_train_tucker = decoding(model, training_loader_latent, X_train_tucker.shape[0], X_train_tucker.shape[1])\n",
    "                \n",
    "                print('recon train shape: ', recon_train_tucker.shape)\n",
    "                recon_train_tucker = recon_train_tucker\n",
    "                \n",
    "                train_recon = recon_train_tucker@basis_product.T\n",
    "                print('train shape after inverse tucker: ', train_recon.shape)\n",
    "                \n",
    "                train_recon = train_recon.reshape(train_num,39,39)*sig[p,:,np.newaxis, np.newaxis] + mu[p,:,np.newaxis, np.newaxis]\n",
    "                AET3d_recon[i] = train_recon\n",
    "                \n",
    "            #np.save('./results/AET3d/v2_{}/output/AET3d_BatchSize{}_pq{}bits_pqM1_plane{}_13latents_denorm.npy'.format(timestep, Batch_Size, pq_bit, plane_idx), AET3d_recon)\n",
    "            #np.save('./results/AET3d/v2_{}/output/AET3d_BatchSize{}_pq{}bits_plane{}_13latents_denorm.npy'.format(timestep, Batch_Size, pq_bit, plane_idx), AET3d_recon)\n",
    "            #np.save('./results/AET3d/v2_{}/output/AET3d_pq{}bits_plane{}_16latents_denorm.npy'.format(timestep, pq_bit, plane_idx), AET3d_recon)\n",
    "            #np.save('./results/AET3d/v2_{}/output/AET3d_pq{}bits_plane{}_without_norm.npy'.format(timestep, pq_bit, plane_idx), AET3d_recon)\n",
    "            np.save('./results/AET3d/v2_{}/output/AET3d_pq{}bits_plane{}_denorm.npy'.format(timestep, pq_bit, plane_idx), AET3d_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf758547-ec36-45f1-b61b-f7575cbd72e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 16, 1) (16395, 5)\n",
      "(10, 16, 1) (16395, 10)\n",
      "(20, 16, 1) (16395, 20)\n",
      "(30, 16, 1) (16395, 30)\n",
      "(45, 16, 1) (16395, 45)\n",
      "(60, 16, 1) (16395, 60)\n",
      "(90, 16, 1) (16395, 90)\n",
      "(120, 16, 1) (16395, 120)\n",
      "(150, 16, 1) (16395, 150)\n",
      "(180, 16, 1) (16395, 180)\n",
      "(210, 16, 1) (16395, 210)\n",
      "(240, 16, 1) (16395, 240)\n",
      "(300, 16, 1) (16395, 300)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(latent_dims)):\n",
    "    print(pq_dic[i].shape, AET3d_encode[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58720431-bc73-48d1-98f9-91244ee04d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 5) (16395, 1)\n",
      "(1, 16, 10) (16395, 1)\n",
      "(1, 16, 20) (16395, 1)\n",
      "(1, 16, 30) (16395, 1)\n",
      "(1, 16, 45) (16395, 1)\n",
      "(1, 16, 60) (16395, 1)\n",
      "(1, 16, 90) (16395, 1)\n",
      "(1, 16, 120) (16395, 1)\n",
      "(1, 16, 150) (16395, 1)\n",
      "(1, 16, 180) (16395, 1)\n",
      "(1, 16, 210) (16395, 1)\n",
      "(1, 16, 240) (16395, 1)\n",
      "(1, 16, 300) (16395, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(latent_dims)):\n",
    "    print(pq_dic[i].shape, AET3d_encode[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455160ce-4782-4c83-8cf9-9cea84889b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:  1595954880.0\n",
      "Compression ratio 4 bits with fixed cost:  [2076.26033606 1188.66216611  640.78926913  438.62169953  297.72455894\n",
      "  225.33944196  151.61553773  114.23987961   91.64734208   76.51535758\n",
      "   65.67216123   57.52074265   46.08126331]\n"
     ]
    }
   ],
   "source": [
    "# Compression ratio with fixed cost\n",
    "# sizes are in bits\n",
    "data_size = 16395.*39.*39.*8.*8.\n",
    "Tucker_basis_size = 24336.*8.\n",
    "print('data size: ', data_size)\n",
    "\n",
    "latent_dims = [5, 10, 20, 30, 45, 60, 90, 120, 150, 180, 210, 240, 300]\n",
    "pq_var = [16]\n",
    "pq_bits = [4]\n",
    "\n",
    "compression_ratio_fixed_cost_AET = np.zeros((len(latent_dims)))\n",
    "pq_dics = np.zeros((len(latent_dims)))\n",
    "\n",
    "AE_numpara = [7610, 15220, 30440, 45660, 68490, 91320, 136980, 182640, 228300, 273960, 319620, 365280, 456600]\n",
    "\n",
    "for i in range(len(latent_dims)):\n",
    "    pq_dics[i] = latent_dims[i]*pq_var[0]*4.*8.\n",
    "\n",
    "for i in range(len(latent_dims)):\n",
    "    compression_ratio_fixed_cost_AET[i] = data_size/((16395*latent_dims[i]*pq_bits[0])+32.*AE_numpara[i]+pq_dics[i]+Tucker_basis_size)\n",
    "\n",
    "print('Compression ratio %d bits with fixed cost: '%pq_bits[0], compression_ratio_fixed_cost_AET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02583135-07d8-4cb8-bcf0-b26e9355e5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2076.26033606, 1188.66216611,  640.78926913,  438.62169953,\n",
       "        297.72455894,  225.33944196,  151.61553773,  114.23987961,\n",
       "         91.64734208,   76.51535758,   65.67216123,   57.52074265,\n",
       "         46.08126331])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio_fixed_cost_AET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfee2df4-8f6d-4323-87d1-3e0f6324fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep = 1000\n",
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467a9fd8-c2f6-48f1-91a6-9a29f7b23ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_bits = 4\n",
    "tailname = 'AET3d_{}bits'.format(pq_bits)\n",
    "np.save('./results/AET3d/v2_{}/output/evaluations/compression_ratio_fixed_cost_{}.npy'.format(timestep, tailname), compression_ratio_fixed_cost_AET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ebebdaf-a56d-4d1c-bf4b-94ab963f2def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_bits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4707e136-2ee4-46c6-9118-8f183c703ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 420\n",
    "tailname = 'AET3d_BatchSize{}_{}bits_M1'.format(Batch_Size, pq_bits)\n",
    "np.save('./results/AET3d/v2_{}/output/evaluations/compression_ratio_fixed_cost_{}.npy'.format(timestep, tailname), compression_ratio_fixed_cost_AET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4499af-f6a9-4e9c-9f3a-03ef5e5c3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AET3d 4bits\n",
    "timestep = 420\n",
    "pq_bits = 4\n",
    "\n",
    "tailname = 'AET3d_{}bits'.format(pq_bits)\n",
    "f0f_rmse_AET3d_4bits = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rmse_{}.npy'.format(timestep,tailname))\n",
    "f0f_rel_rmse_ornl_AET3d_4bits = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "QoI_rmse_AET3d_4bits = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rmse_{}.npy'.format(timestep,tailname))\n",
    "QoI_rel_rmse_ornl_AET3d_4bits = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "\n",
    "compression_ratio_fixed_cost_AET3d_4bits = np.load('./results/AET3d/v2_{}/output/evaluations/compression_ratio_fixed_cost_AET3d_{}bits.npy'.format(timestep, pq_bits))\n",
    "\n",
    "f0f_rmse_AET3d_4bits_nodedist = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rmse_{}_nodedist.npy'.format(timestep,tailname))\n",
    "f0f_rel_rmse_ornl_AET3d_4bits_nodedist = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}_nodedist.npy'.format(timestep,tailname))\n",
    "QoI_rmse_AET3d_4bits_nodedist = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rmse_{}_nodedist.npy'.format(timestep,tailname))\n",
    "QoI_rel_rmse_ornl_AET3d_4bits_nodedist = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rel_rmse_ornl_{}_nodedist.npy'.format(timestep,tailname))\n",
    "\n",
    "f0f_rel_rmse_ornl_AET3d_plane0 = np.load('./results_temp/f0f_rel_rmse_ornl_AET_plane0.npy')\n",
    "QoI_rel_rmse_ornl_AET3d_plane0 = np.load('./results_temp/QoI_rel_rmse_ornl_AET_plane0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd5957a-2151-48fd-ac3e-6122f3dfe17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2076.26033606, 1188.66216611,  640.78926913,  438.62169953,\n",
       "        297.72455894,  225.33944196,  151.61553773,  114.23987961,\n",
       "         91.64734208,   76.51535758,   65.67216123,   57.52074265,\n",
       "         46.08126331])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio_fixed_cost_AET3d_4bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be74b59c-4476-4154-95bb-a985834c6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AET3d 6, 8bits\n",
    "timestep = 420\n",
    "pq_bits = 6\n",
    "\n",
    "tailname = 'AET3d_{}bits'.format(pq_bits)\n",
    "f0f_rel_rmse_ornl_AET3d_6bits = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "QoI_rel_rmse_ornl_AET3d_6bits = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "f0f_rel_rmse_ornl_AET3d_6bits_nodedist = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}_nodedist.npy'.format(timestep,tailname))\n",
    "\n",
    "compression_ratio_fixed_cost_AET3d_6bits = np.load('./results/AET3d/v2_{}/output/evaluations/compression_ratio_fixed_cost_AET3d_{}bits.npy'.format(timestep, pq_bits))\n",
    "\n",
    "pq_bits = 8\n",
    "\n",
    "tailname = 'AET3d_{}bits'.format(pq_bits)\n",
    "f0f_rel_rmse_ornl_AET3d_8bits = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "QoI_rel_rmse_ornl_AET3d_8bits = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "f0f_rel_rmse_ornl_AET3d_8bits_nodedist = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}_nodedist.npy'.format(timestep,tailname))\n",
    "\n",
    "compression_ratio_fixed_cost_AET3d_8bits = np.load('./results/AET3d/v2_{}/output/evaluations/compression_ratio_fixed_cost_AET3d_{}bits.npy'.format(timestep, pq_bits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7086bd42-c819-4004-b5d8-ce213cd45ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_bits = 4\n",
    "tailname = 'AET3d_BatchSize{}_{}bits'.format(Batch_Size, pq_bits)\n",
    "#f0f_rmse_AET3d_4bits_M1 = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rmse_{}.npy'.format(timestep,tailname))\n",
    "f0f_rel_rmse_ornl_AET3d_4bits_M1 = np.load('./results/AET3d/v2_{}/output/evaluations/f0f_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "#QoI_rmse_AET3d_4bits_M1 = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rmse_{}.npy'.format(timestep,tailname))\n",
    "#QoI_rel_rmse_ornl_AET3d_4bits_M1 = np.load('./results/AET3d/v2_{}/output/evaluations/QoI_rel_rmse_ornl_{}.npy'.format(timestep,tailname))\n",
    "\n",
    "tailname = 'AET3d_BatchSize{}_{}bits_M1'.format(Batch_Size, pq_bits)\n",
    "compression_ratio_fixed_cost_AET3d_4bits_M1 = np.load('./results/AET3d/v2_{}/output/evaluations/compression_ratio_fixed_cost_{}.npy'.format(timestep, tailname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64d536ca-f732-4fd4-a925-ef5dc4eb248b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'PD')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 489.6x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# New results\n",
    "\n",
    "# PD\n",
    "\n",
    "fig, axis = plt.subplots(1,1, figsize=(6.8*1,6))\n",
    "axis.plot(compression_ratio_fixed_cost_AET3d_4bits[:], f0f_rel_rmse_ornl_AET3d_4bits[:], '-^r', label='AET 4 bits')\n",
    "#axis.plot(compression_ratio_fixed_cost_AET3d_4bits_M1[:], f0f_rel_rmse_ornl_AET3d_4bits_M1[:], '-^b', label='AET 1 subquantizer')\n",
    "axis.plot(compression_ratio_fixed_cost_AET3d_6bits[:], f0f_rel_rmse_ornl_AET3d_6bits[:], '-^', label='AET 6 bits')\n",
    "axis.plot(compression_ratio_fixed_cost_AET3d_8bits[:], f0f_rel_rmse_ornl_AET3d_8bits[:], '-^', label='AET 8 bits')\n",
    "\n",
    "axis.set_xlabel('Compression ratio', fontsize=14)\n",
    "axis.set_ylabel('NRMSE', fontsize=15)\n",
    "\n",
    "axis.legend(loc=\"lower right\", bbox_to_anchor=(1.6, 0.3), fontsize=14)\n",
    "#plt.subplots_adjust(wspace=0.25)\n",
    "plt.suptitle('PD',fontsize=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fa56e-e088-433c-9f2b-ace6f6a883a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c764d1-bc87-4552-a6cd-c407fff58ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OLCF-CUDA11 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
